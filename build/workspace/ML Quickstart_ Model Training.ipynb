{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "528c6d2b-3c5a-4236-ac61-fa8cc8d4d323",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Databricks ML Quickstart: Model Training\n",
    "\n",
    "This notebook provides a quick overview of machine learning model training on Databricks. To train models, you can use libraries like scikit-learn that are preinstalled on the Databricks Runtime for Machine Learning. In addition, you can use MLflow to track the trained models, and Hyperopt with SparkTrials to scale hyperparameter tuning.\n",
    "\n",
    "This tutorial covers:\n",
    "- Part 1: Training a simple classification model with MLflow tracking\n",
    "- Part 2: Hyperparameter tuning a better performing model with Hyperopt\n",
    "\n",
    "For more details on productionizing machine learning on Databricks including model lifecycle management and model inference, see the ML End to End Example ([AWS](https://docs.databricks.com/applications/mlflow/end-to-end-example.html)|[Azure](https://docs.microsoft.com/azure/databricks/applications/mlflow/end-to-end-example)|[GCP](https://docs.gcp.databricks.com/applications/mlflow/end-to-end-example.html)).\n",
    "\n",
    "### Requirements\n",
    "- Cluster running Databricks Runtime 7.5 ML or above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "8738a402-24dc-4074-bebb-b51bec8e74db",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Libraries\n",
    "Import the necessary libraries. These libraries are preinstalled on Databricks Runtime for Machine Learning ([AWS](https://docs.databricks.com/runtime/mlruntime.html)|[Azure](https://docs.microsoft.com/azure/databricks/runtime/mlruntime)|[GCP](https://docs.gcp.databricks.com/runtime/mlruntime.html)) clusters and are tuned for compatibility and performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-23T19:39:18.435238Z",
     "iopub.status.busy": "2022-09-23T19:39:18.435115Z",
     "iopub.status.idle": "2022-09-23T19:39:18.442419Z",
     "shell.execute_reply": "2022-09-23T19:39:18.442144Z",
     "shell.execute_reply.started": "2022-09-23T19:39:18.435203Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('spark.driver.extraClassPath', '/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/sparkmonitor/listener_2.12.jar'), ('spark.extraListeners', 'sparkmonitor.listener.JupyterSparkMonitorListener')])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf.getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-23T19:39:18.443169Z",
     "iopub.status.busy": "2022-09-23T19:39:18.443093Z",
     "iopub.status.idle": "2022-09-23T19:39:18.461782Z",
     "shell.execute_reply": "2022-09-23T19:39:18.461242Z",
     "shell.execute_reply.started": "2022-09-23T19:39:18.443159Z"
    }
   },
   "outputs": [],
   "source": [
    "# from pyspark.sql import SparkSession\n",
    "\n",
    "# spark = SparkSession.\\\n",
    "#         builder.\\\n",
    "#         appName(\"pyspark-notebook\").\\\n",
    "#         config('spark.extraListeners', 'sparkmonitor.listener.JupyterSparkMonitorListener').\\\n",
    "#         config('spark.driver.extraClassPath', '/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/sparkmonitor/listener_2.12.jar').\\\n",
    "#         config(\"spark.executor.memory\", \"8278m\").\\\n",
    "#         getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-23T19:39:18.462711Z",
     "iopub.status.busy": "2022-09-23T19:39:18.462529Z",
     "iopub.status.idle": "2022-09-23T19:39:20.507993Z",
     "shell.execute_reply": "2022-09-23T19:39:20.507611Z",
     "shell.execute_reply.started": "2022-09-23T19:39:18.462692Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/09/24 01:09:19 WARN Utils: Your hostname, workstation resolves to a loopback address: 127.0.1.1; using 192.168.1.22 instead (on interface wlp5s0)\n",
      "22/09/24 01:09:19 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/pyspark/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Ivy Default Cache set to: /home/debankurs/.ivy2/cache\n",
      "The jars for the packages stored in: /home/debankurs/.ivy2/jars\n",
      "org.mlflow#mlflow-spark added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-a35f2000-6a86-48be-b1c0-84981d421f07;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.mlflow#mlflow-spark;1.29.0 in central\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/pyspark/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tfound org.slf4j#slf4j-api;1.7.25 in central\n",
      ":: resolution report :: resolve 79ms :: artifacts dl 2ms\n",
      "\t:: modules in use:\n",
      "\torg.mlflow#mlflow-spark;1.29.0 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.25 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   2   |   0   |   0   |   0   ||   2   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-a35f2000-6a86-48be-b1c0-84981d421f07\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 2 already retrieved (0kB/2ms)\n",
      "22/09/24 01:09:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "INFO:SparkMonitorKernel:Client Connected ('127.0.0.1', 53094)\n",
      "Exception in thread Thread-5:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/threading.py\", line 932, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/sparkmonitor/kernelextension.py\", line 126, in run\n",
      "    self.onrecv(msg)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/sparkmonitor/kernelextension.py\", line 143, in onrecv\n",
      "    sendToFrontEnd({\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/sparkmonitor/kernelextension.py\", line 223, in sendToFrontEnd\n",
      "    monitor.send(msg)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/sparkmonitor/kernelextension.py\", line 57, in send\n",
      "    self.comm.send(msg)\n",
      "AttributeError: 'ScalaMonitor' object has no attribute 'comm'\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.\\\n",
    "        builder.\\\n",
    "        appName(\"pyspark-notebook\").\\\n",
    "        master(\"spark://spark-master:7077\").\\\n",
    "        config('spark.extraListeners', dict(conf.getAll())['spark.extraListeners']).\\\n",
    "        config('spark.driver.extraClassPath', dict(conf.getAll())['spark.driver.extraClassPath']).\\\n",
    "        config(\"spark.jars.packages\", \"org.mlflow:mlflow-spark:1.29.0\").\\\n",
    "        config(\"spark.executor.memory\", \"8278m\").\\\n",
    "        getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"OFF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "cec61d6d-ea1a-4d2f-9ee6-625393a24aa5",
     "showTitle": false,
     "title": ""
    },
    "execution": {
     "iopub.execute_input": "2022-09-23T19:39:20.508650Z",
     "iopub.status.busy": "2022-09-23T19:39:20.508562Z",
     "iopub.status.idle": "2022-09-23T19:39:21.110356Z",
     "shell.execute_reply": "2022-09-23T19:39:21.110022Z",
     "shell.execute_reply.started": "2022-09-23T19:39:20.508638Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.datasets\n",
    "import sklearn.metrics\n",
    "import sklearn.model_selection\n",
    "import sklearn.ensemble\n",
    "\n",
    "from hyperopt import fmin, tpe, hp, SparkTrials, Trials, STATUS_OK\n",
    "from hyperopt.pyll import scope"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b2f67ffe-ad1b-49a1-a7cf-603daa8c9890",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Load data\n",
    "The tutorial uses a dataset describing different wine samples. The [dataset](https://archive.ics.uci.edu/ml/datasets/Wine) is from the UCI Machine Learning Repository and is included in DBFS ([AWS](https://docs.databricks.com/data/databricks-file-system.html)|[Azure](https://docs.microsoft.com/azure/databricks/data/databricks-file-system)|[GCP](https://docs.gcp.databricks.com/data/databricks-file-system.html)).\n",
    "The goal is to classify red and white wines by their quality. \n",
    "\n",
    "For more details on uploading and loading from other data sources, see the documentation on working with data ([AWS](https://docs.databricks.com/data/index.html)|[Azure](https://docs.microsoft.com/azure/databricks/data/index)|[GCP](https://docs.gcp.databricks.com/data/index.html))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "14c8d056-74b2-46f0-8c3e-627447f7671e",
     "showTitle": false,
     "title": ""
    },
    "execution": {
     "iopub.execute_input": "2022-09-23T19:39:21.110890Z",
     "iopub.status.busy": "2022-09-23T19:39:21.110812Z",
     "iopub.status.idle": "2022-09-23T19:39:21.112879Z",
     "shell.execute_reply": "2022-09-23T19:39:21.112595Z",
     "shell.execute_reply.started": "2022-09-23T19:39:21.110879Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Enable MLflow autologging for this notebook\n",
    "mlflow.autolog(silent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e7656d3b-6c6e-4f78-b1b9-6d1f9ea5bf4d",
     "showTitle": false,
     "title": ""
    },
    "execution": {
     "iopub.execute_input": "2022-09-23T19:39:21.113468Z",
     "iopub.status.busy": "2022-09-23T19:39:21.113294Z",
     "iopub.status.idle": "2022-09-23T19:39:21.132531Z",
     "shell.execute_reply": "2022-09-23T19:39:21.132110Z",
     "shell.execute_reply.started": "2022-09-23T19:39:21.113456Z"
    }
   },
   "outputs": [],
   "source": [
    "# dbutils.fs.cp(\"dbfs:/databricks-datasets/wine-quality\",\"file:/tmp/databricks-datasets/wine-quality\",recurse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "4c1ccaa2-b643-4313-be1f-ea4cc78a0380",
     "showTitle": false,
     "title": ""
    },
    "execution": {
     "iopub.execute_input": "2022-09-23T19:39:21.134072Z",
     "iopub.status.busy": "2022-09-23T19:39:21.133960Z",
     "iopub.status.idle": "2022-09-23T19:39:21.138409Z",
     "shell.execute_reply": "2022-09-23T19:39:21.137995Z",
     "shell.execute_reply.started": "2022-09-23T19:39:21.134060Z"
    }
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.listdir(\"/tmp/databricks-datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "51fd8a9f-bef3-4fbd-90ce-8531f5f71205",
     "showTitle": false,
     "title": ""
    },
    "execution": {
     "iopub.execute_input": "2022-09-23T19:39:21.139120Z",
     "iopub.status.busy": "2022-09-23T19:39:21.138963Z",
     "iopub.status.idle": "2022-09-23T19:39:21.152731Z",
     "shell.execute_reply": "2022-09-23T19:39:21.152458Z",
     "shell.execute_reply.started": "2022-09-23T19:39:21.139104Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load and preprocess data\n",
    "white_wine = pd.read_csv(\"./winequality-white.csv\", sep=';')\n",
    "red_wine = pd.read_csv(\"./winequality-red.csv\", sep=';')\n",
    "white_wine['is_red'] = 0.0\n",
    "red_wine['is_red'] = 1.0\n",
    "data_df = pd.concat([white_wine, red_wine], axis=0)\n",
    "\n",
    "# Define classification labels based on the wine quality\n",
    "data_labels = data_df['quality'] >= 7\n",
    "data_df = data_df.drop(['quality'], axis=1)\n",
    "\n",
    "# Split 80/20 train-test\n",
    "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(\n",
    "  data_df,\n",
    "  data_labels,\n",
    "  test_size=0.2,\n",
    "  random_state=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "8123df40-67fa-43d0-97c1-6f608c9f7d61",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Part 1. Train a classification model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f3441df0-a7fe-4d6d-bf62-46184cf6ac2c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### MLflow Tracking\n",
    "[MLflow tracking](https://www.mlflow.org/docs/latest/tracking.html) allows you to organize your machine learning training code, parameters, and models. \n",
    "\n",
    "You can enable automatic MLflow tracking by using [*autologging*](https://www.mlflow.org/docs/latest/tracking.html#automatic-logging)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "18e0381e-7c02-4e57-b29b-127a7585992b",
     "showTitle": false,
     "title": ""
    },
    "execution": {
     "iopub.execute_input": "2022-09-23T19:39:21.153175Z",
     "iopub.status.busy": "2022-09-23T19:39:21.153093Z",
     "iopub.status.idle": "2022-09-23T19:39:21.240709Z",
     "shell.execute_reply": "2022-09-23T19:39:21.240419Z",
     "shell.execute_reply.started": "2022-09-23T19:39:21.153164Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022/09/24 01:09:21 INFO mlflow.tracking.fluent: Autologging successfully enabled for sklearn.\n",
      "2022/09/24 01:09:21 INFO mlflow._spark_autologging: Autologging successfully enabled for spark.\n",
      "2022/09/24 01:09:21 INFO mlflow.tracking.fluent: Autologging successfully enabled for pyspark.\n",
      "2022/09/24 01:09:21 INFO mlflow.tracking.fluent: Autologging successfully enabled for pyspark.ml.\n"
     ]
    }
   ],
   "source": [
    "# Enable MLflow autologging for this notebook\n",
    "mlflow.autolog()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "88913160-61c4-4cbf-8f2d-235e86fa8768",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Next, train a classifier within the context of an MLflow run, which automatically logs the trained model and many associated metrics and parameters. \n",
    "\n",
    "You can supplement the logging with additional metrics such as the model's AUC score on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "70fefb47-9af8-49c8-932d-49a0727c1428",
     "showTitle": false,
     "title": ""
    },
    "execution": {
     "iopub.execute_input": "2022-09-23T19:39:21.241222Z",
     "iopub.status.busy": "2022-09-23T19:39:21.241128Z",
     "iopub.status.idle": "2022-09-23T19:39:23.718112Z",
     "shell.execute_reply": "2022-09-23T19:39:23.717787Z",
     "shell.execute_reply.started": "2022-09-23T19:39:21.241211Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022/09/24 01:09:23 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test AUC of: 0.8834365701533531\n"
     ]
    }
   ],
   "source": [
    "with mlflow.start_run(run_name='gradient_boost') as run:\n",
    "  model = sklearn.ensemble.GradientBoostingClassifier(random_state=0)\n",
    "  \n",
    "  # Models, parameters, and training metrics are tracked automatically\n",
    "  model.fit(X_train, y_train)\n",
    "\n",
    "  predicted_probs = model.predict_proba(X_test)\n",
    "  roc_auc = sklearn.metrics.roc_auc_score(y_test, predicted_probs[:,1])\n",
    "  \n",
    "  # The AUC score on test data is not automatically logged, so log it manually\n",
    "  mlflow.log_metric(\"test_auc\", roc_auc)\n",
    "  print(\"Test AUC of: {}\".format(roc_auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "06eadee7-a786-4a6a-be73-c70a7c18c0a0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "If you aren't happy with the performance of this model, train another model with different hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "85fa6e22-56ab-44da-89c0-e77f883c5fcd",
     "showTitle": false,
     "title": ""
    },
    "execution": {
     "iopub.execute_input": "2022-09-23T19:39:23.718696Z",
     "iopub.status.busy": "2022-09-23T19:39:23.718614Z",
     "iopub.status.idle": "2022-09-23T19:39:26.381157Z",
     "shell.execute_reply": "2022-09-23T19:39:26.380789Z",
     "shell.execute_reply.started": "2022-09-23T19:39:23.718685Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test AUC of: 0.8914761673151751\n"
     ]
    }
   ],
   "source": [
    "# Start a new run and assign a run_name for future reference\n",
    "with mlflow.start_run(run_name='gradient_boost') as run:\n",
    "  model_2 = sklearn.ensemble.GradientBoostingClassifier(\n",
    "    random_state=0, \n",
    "    \n",
    "    # Try a new parameter setting for n_estimators\n",
    "    n_estimators=200,\n",
    "  )\n",
    "  model_2.fit(X_train, y_train)\n",
    "\n",
    "  predicted_probs = model_2.predict_proba(X_test)\n",
    "  roc_auc = sklearn.metrics.roc_auc_score(y_test, predicted_probs[:,1])\n",
    "  mlflow.log_metric(\"test_auc\", roc_auc)\n",
    "  print(\"Test AUC of: {}\".format(roc_auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "70e02a64-6878-4b9b-9297-5390c9e19ddc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### View MLflow runs\n",
    "To view the logged training runs, click the **Experiment** icon at the upper right of the notebook to display the experiment sidebar. If necessary, click the refresh icon to fetch and monitor the latest runs. \n",
    "\n",
    "<img width=\"350\" src=\"https://docs.databricks.com/_static/images/mlflow/quickstart/experiment-sidebar-icons.png\"/>\n",
    "\n",
    "You can then click the experiment page icon to display the more detailed MLflow experiment page ([AWS](https://docs.databricks.com/applications/mlflow/tracking.html#notebook-experiments)|[Azure](https://docs.microsoft.com/azure/databricks/applications/mlflow/tracking#notebook-experiments)|[GCP](https://docs.gcp.databricks.com/applications/mlflow/tracking.html#notebook-experiments)). This page allows you to compare runs and view details for specific runs.\n",
    "\n",
    "<img width=\"800\" src=\"https://docs.databricks.com/_static/images/mlflow/quickstart/compare-runs.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "78c381a0-15ef-4b8c-92a3-73261bf7160f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Load models\n",
    "You can also access the results for a specific run using the MLflow API. The code in the following cell illustrates how to load the model trained in a given MLflow run and use it to make predictions. You can also find code snippets for loading specific models on the MLflow run page ([AWS](https://docs.databricks.com/applications/mlflow/tracking.html#view-notebook-experiment)|[Azure](https://docs.microsoft.com/azure/databricks/applications/mlflow/tracking#view-notebook-experiment)|[GCP](https://docs.gcp.databricks.com/applications/mlflow/tracking.html#view-notebook-experiment))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "c2995022-c1c9-4e8c-adac-89a0d78c73f6",
     "showTitle": false,
     "title": ""
    },
    "execution": {
     "iopub.execute_input": "2022-09-23T19:39:26.381865Z",
     "iopub.status.busy": "2022-09-23T19:39:26.381762Z",
     "iopub.status.idle": "2022-09-23T19:39:26.384580Z",
     "shell.execute_reply": "2022-09-23T19:39:26.384272Z",
     "shell.execute_reply.started": "2022-09-23T19:39:26.381851Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<RunInfo: artifact_uri='./mlruns/0/41529af14ffc4d3bbffebeddc3cd9962/artifacts', end_time=None, experiment_id='0', lifecycle_stage='active', run_id='41529af14ffc4d3bbffebeddc3cd9962', run_uuid='41529af14ffc4d3bbffebeddc3cd9962', start_time=1663961963723, status='RUNNING', user_id='debankurs'>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "2d85461a-9f6d-43e4-abb5-0d72e1fe48a7",
     "showTitle": false,
     "title": ""
    },
    "execution": {
     "iopub.execute_input": "2022-09-23T19:39:26.385119Z",
     "iopub.status.busy": "2022-09-23T19:39:26.385026Z",
     "iopub.status.idle": "2022-09-23T19:39:26.424711Z",
     "shell.execute_reply": "2022-09-23T19:39:26.424331Z",
     "shell.execute_reply.started": "2022-09-23T19:39:26.385107Z"
    }
   },
   "outputs": [],
   "source": [
    "# After a model has been logged, you can load it in different notebooks or jobs\n",
    "# mlflow.pyfunc.load_model makes model prediction available under a common API\n",
    "model_loaded = mlflow.pyfunc.load_model(\n",
    "  'runs:/{run_id}/model'.format(\n",
    "    run_id=run.info.run_id\n",
    "  )\n",
    ")\n",
    "\n",
    "predictions_loaded = model_loaded.predict(X_test)\n",
    "predictions_original = model_2.predict(X_test)\n",
    "\n",
    "# The loaded model should match the original\n",
    "assert(np.array_equal(predictions_loaded, predictions_original))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "6eaefdfe-f00e-4e3b-b647-4ed56cb89b15",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Part 2. Hyperparameter Tuning\n",
    "At this point, you have trained a simple model and used the MLflow tracking service to organize your work. This section covers how to perform more sophisticated tuning using Hyperopt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "112f48ac-e1dd-40e5-b40b-56266306130c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Parallel training with Hyperopt and SparkTrials\n",
    "[Hyperopt](http://hyperopt.github.io/hyperopt/) is a Python library for hyperparameter tuning. For more information about using Hyperopt in Databricks, see the documentation ([AWS](https://docs.databricks.com/applications/machine-learning/automl-hyperparam-tuning/index.html#hyperparameter-tuning-with-hyperopt)|[Azure](https://docs.microsoft.com/azure/databricks/applications/machine-learning/automl-hyperparam-tuning/index#hyperparameter-tuning-with-hyperopt)|[GCP](https://docs.gcp.databricks.com/applications/machine-learning/automl-hyperparam-tuning/index.html#hyperparameter-tuning-with-hyperopt)).\n",
    "\n",
    "You can use Hyperopt with SparkTrials to run hyperparameter sweeps and train multiple models in parallel. This reduces the time required to optimize model performance. MLflow tracking is integrated with Hyperopt to automatically log models and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "6d3f22d5-8217-42ad-bd78-fbf30ef5538c",
     "showTitle": false,
     "title": ""
    },
    "execution": {
     "iopub.execute_input": "2022-09-23T19:39:26.425269Z",
     "iopub.status.busy": "2022-09-23T19:39:26.425184Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                    | 0/32 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/09/24 01:09:27 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Connection reset by peer (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implFlushBuffer(StreamEncoder.java:312)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implFlush(StreamEncoder.java:316)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.flush(StreamEncoder.java:153)\n",
      "\tat java.base/java.io.OutputStreamWriter.flush(OutputStreamWriter.java:251)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:54)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onJobStart(CustomListener.scala:236)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:37)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:117)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:101)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:105)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:105)\n",
      "\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:100)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:96)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1381)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:96)\n",
      "22/09/24 01:09:27 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implFlushBuffer(StreamEncoder.java:312)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implFlush(StreamEncoder.java:316)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.flush(StreamEncoder.java:153)\n",
      "\tat java.base/java.io.OutputStreamWriter.flush(OutputStreamWriter.java:251)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:54)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onStageSubmitted(CustomListener.scala:379)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:33)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:117)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:101)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:105)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:105)\n",
      "\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:100)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:96)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1381)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:96)\n",
      "22/09/24 01:09:28 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.$anonfun$onStageStatusActive$2(CustomListener.scala:402)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.$anonfun$onStageStatusActive$2$adapted(CustomListener.scala:385)\n",
      "\tat scala.collection.TraversableLike$WithFilter.$anonfun$foreach$1(TraversableLike.scala:877)\n",
      "\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)\n",
      "\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)\n",
      "\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:149)\n",
      "\tat scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:876)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onStageStatusActive(CustomListener.scala:385)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener$$anon$1.run(CustomListener.scala:71)\n",
      "\tat java.base/java.util.TimerThread.mainLoop(Timer.java:556)\n",
      "\tat java.base/java.util.TimerThread.run(Timer.java:506)\n",
      "22/09/24 01:09:28 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implFlushBuffer(StreamEncoder.java:312)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implFlush(StreamEncoder.java:316)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.flush(StreamEncoder.java:153)\n",
      "\tat java.base/java.io.OutputStreamWriter.flush(OutputStreamWriter.java:251)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:54)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onStageStatusActive(CustomListener.scala:409)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener$$anon$1.run(CustomListener.scala:71)\n",
      "\tat java.base/java.util.TimerThread.mainLoop(Timer.java:556)\n",
      "\tat java.base/java.util.TimerThread.run(Timer.java:506)\n",
      "22/09/24 01:09:28 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onJobStart(CustomListener.scala:236)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:37)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:117)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:101)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:105)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:105)\n",
      "\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:100)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:96)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1381)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:96)\n",
      "22/09/24 01:09:28 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implFlushBuffer(StreamEncoder.java:312)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implFlush(StreamEncoder.java:316)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.flush(StreamEncoder.java:153)\n",
      "\tat java.base/java.io.OutputStreamWriter.flush(OutputStreamWriter.java:251)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:54)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onStageSubmitted(CustomListener.scala:379)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:33)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:117)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:101)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:105)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:105)\n",
      "\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:100)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:96)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1381)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:96)\n",
      "2022/09/24 01:09:28 INFO mlflow.tracking.fluent: Autologging successfully enabled for sklearn.\n",
      "2022/09/24 01:09:28 INFO mlflow.tracking.fluent: Autologging successfully enabled for pyspark.\n",
      "2022/09/24 01:09:28 INFO mlflow.pyspark.ml: No SparkSession detected. Autologging will log pyspark.ml models contained in the default allowlist. To specify a custom allowlist, initialize a SparkSession prior to calling mlflow.pyspark.ml.autolog() and specify the path to your allowlist file via the spark.mlflow.pysparkml.autolog.logModelAllowlistFile conf.\n",
      "2022/09/24 01:09:28 INFO mlflow.tracking.fluent: Autologging successfully enabled for pyspark.ml.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/hyperopt/spark.py\", line 467, in run_task_on_executor\n",
      "    result = domain.evaluate(\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/hyperopt/base.py\", line 892, in evaluate\n",
      "    rval = self.fn(pyll_rval)\n",
      "  File \"/tmp/ipykernel_229044/4099234749.py\", line 11, in train_model\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/fluent.py\", line 353, in start_run\n",
      "    active_run_obj = client.create_run(experiment_id=exp_id_for_run, tags=resolved_tags)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/client.py\", line 265, in create_run\n",
      "    return self._tracking_client.create_run(experiment_id, start_time, tags)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/_tracking_service/client.py\", line 107, in create_run\n",
      "    return self.store.create_run(\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 583, in create_run\n",
      "    experiment = self.get_experiment(experiment_id)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 430, in get_experiment\n",
      "    experiment = self._get_experiment(experiment_id)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 403, in _get_experiment\n",
      "    meta = FileStore._read_yaml(experiment_dir, FileStore.META_DATA_FILE_NAME)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 1075, in _read_yaml\n",
      "    return _read_helper(root, file_name, attempts_remaining=retries)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 1068, in _read_helper\n",
      "    result = read_yaml(root, file_name)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/utils/file_utils.py\", line 181, in read_yaml\n",
      "    raise MissingConfigException(\"Yaml file '%s' does not exist.\" % file_path)\n",
      "mlflow.exceptions.MissingConfigException: Yaml file '/home/debankurs/Desktop/learningspark/spark-standalone-cluster-on-docker/build/workspace/mlruns/0/meta.yaml' does not exist.\n",
      "\n",
      "trial task 0 failed, exception is Yaml file '/home/debankurs/Desktop/learningspark/spark-standalone-cluster-on-docker/build/workspace/mlruns/0/meta.yaml' does not exist..\n",
      " Traceback (most recent call last):\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/hyperopt/spark.py\", line 467, in run_task_on_executor\n",
      "    result = domain.evaluate(\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/hyperopt/base.py\", line 892, in evaluate\n",
      "    rval = self.fn(pyll_rval)\n",
      "  File \"/tmp/ipykernel_229044/4099234749.py\", line 11, in train_model\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/fluent.py\", line 353, in start_run\n",
      "    active_run_obj = client.create_run(experiment_id=exp_id_for_run, tags=resolved_tags)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/client.py\", line 265, in create_run\n",
      "    return self._tracking_client.create_run(experiment_id, start_time, tags)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/_tracking_service/client.py\", line 107, in create_run\n",
      "    return self.store.create_run(\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 583, in create_run\n",
      "    experiment = self.get_experiment(experiment_id)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 430, in get_experiment\n",
      "    experiment = self._get_experiment(experiment_id)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 403, in _get_experiment\n",
      "    meta = FileStore._read_yaml(experiment_dir, FileStore.META_DATA_FILE_NAME)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 1075, in _read_yaml\n",
      "    return _read_helper(root, file_name, attempts_remaining=retries)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 1068, in _read_helper\n",
      "    result = read_yaml(root, file_name)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/utils/file_utils.py\", line 181, in read_yaml\n",
      "    raise MissingConfigException(\"Yaml file '%s' does not exist.\" % file_path)\n",
      "mlflow.exceptions.MissingConfigException: Yaml file '/home/debankurs/Desktop/learningspark/spark-standalone-cluster-on-docker/build/workspace/mlruns/0/meta.yaml' does not exist.\n",
      "\n",
      "22/09/24 01:09:29 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onStageCompleted(CustomListener.scala:343)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:35)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:117)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:101)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:105)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:105)\n",
      "\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:100)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:96)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1381)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:96)\n",
      "22/09/24 01:09:29 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implFlushBuffer(StreamEncoder.java:312)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implFlush(StreamEncoder.java:316)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.flush(StreamEncoder.java:153)\n",
      "\tat java.base/java.io.OutputStreamWriter.flush(OutputStreamWriter.java:251)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:54)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onJobEnd(CustomListener.scala:288)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:39)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:117)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:101)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:105)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:105)\n",
      "\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:100)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:96)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1381)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:96)\n",
      "22/09/24 01:09:29 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.$anonfun$onStageStatusActive$2(CustomListener.scala:402)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.$anonfun$onStageStatusActive$2$adapted(CustomListener.scala:385)\n",
      "\tat scala.collection.TraversableLike$WithFilter.$anonfun$foreach$1(TraversableLike.scala:877)\n",
      "\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)\n",
      "\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)\n",
      "\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:149)\n",
      "\tat scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:876)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onStageStatusActive(CustomListener.scala:385)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener$$anon$1.run(CustomListener.scala:71)\n",
      "\tat java.base/java.util.TimerThread.mainLoop(Timer.java:556)\n",
      "\tat java.base/java.util.TimerThread.run(Timer.java:506)\n",
      "22/09/24 01:09:29 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onStageStatusActive(CustomListener.scala:409)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener$$anon$1.run(CustomListener.scala:71)\n",
      "\tat java.base/java.util.TimerThread.mainLoop(Timer.java:556)\n",
      "\tat java.base/java.util.TimerThread.run(Timer.java:506)\n",
      "22/09/24 01:09:29 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onStageStatusActive(CustomListener.scala:409)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener$$anon$1.run(CustomListener.scala:71)\n",
      "\tat java.base/java.util.TimerThread.mainLoop(Timer.java:556)\n",
      "\tat java.base/java.util.TimerThread.run(Timer.java:506)\n",
      "2022/09/24 01:09:29 INFO mlflow.tracking.fluent: Autologging successfully enabled for sklearn.\n",
      "22/09/24 01:09:29 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onJobStart(CustomListener.scala:236)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:37)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:117)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:101)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:105)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:105)\n",
      "\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:100)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:96)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1381)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:96)\n",
      "22/09/24 01:09:29 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onStageSubmitted(CustomListener.scala:379)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:33)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:117)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:101)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:105)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:105)\n",
      "\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:100)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:96)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1381)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:96)\n",
      "2022/09/24 01:09:29 INFO mlflow.tracking.fluent: Autologging successfully enabled for pyspark.\n",
      "2022/09/24 01:09:29 INFO mlflow.pyspark.ml: No SparkSession detected. Autologging will log pyspark.ml models contained in the default allowlist. To specify a custom allowlist, initialize a SparkSession prior to calling mlflow.pyspark.ml.autolog() and specify the path to your allowlist file via the spark.mlflow.pysparkml.autolog.logModelAllowlistFile conf.\n",
      "2022/09/24 01:09:29 INFO mlflow.tracking.fluent: Autologging successfully enabled for pyspark.ml.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/hyperopt/spark.py\", line 467, in run_task_on_executor\n",
      "    result = domain.evaluate(\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/hyperopt/base.py\", line 892, in evaluate\n",
      "    rval = self.fn(pyll_rval)\n",
      "  File \"/tmp/ipykernel_229044/4099234749.py\", line 11, in train_model\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/fluent.py\", line 353, in start_run\n",
      "    active_run_obj = client.create_run(experiment_id=exp_id_for_run, tags=resolved_tags)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/client.py\", line 265, in create_run\n",
      "    return self._tracking_client.create_run(experiment_id, start_time, tags)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/_tracking_service/client.py\", line 107, in create_run\n",
      "    return self.store.create_run(\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 583, in create_run\n",
      "    experiment = self.get_experiment(experiment_id)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 430, in get_experiment\n",
      "    experiment = self._get_experiment(experiment_id)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 403, in _get_experiment\n",
      "    meta = FileStore._read_yaml(experiment_dir, FileStore.META_DATA_FILE_NAME)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 1075, in _read_yaml\n",
      "    return _read_helper(root, file_name, attempts_remaining=retries)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 1068, in _read_helper\n",
      "    result = read_yaml(root, file_name)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/utils/file_utils.py\", line 181, in read_yaml\n",
      "    raise MissingConfigException(\"Yaml file '%s' does not exist.\" % file_path)\n",
      "mlflow.exceptions.MissingConfigException: Yaml file '/home/debankurs/Desktop/learningspark/spark-standalone-cluster-on-docker/build/workspace/mlruns/0/meta.yaml' does not exist.\n",
      "\n",
      "22/09/24 01:09:29 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onStageCompleted(CustomListener.scala:343)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:35)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:117)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:101)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:105)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:105)\n",
      "\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:100)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:96)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1381)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:96)\n",
      "22/09/24 01:09:29 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implFlushBuffer(StreamEncoder.java:312)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implFlush(StreamEncoder.java:316)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.flush(StreamEncoder.java:153)\n",
      "\tat java.base/java.io.OutputStreamWriter.flush(OutputStreamWriter.java:251)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:54)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onJobEnd(CustomListener.scala:288)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:39trial task 1 failed, exception is Yaml file '/home/debankurs/Desktop/learningspark/spark-standalone-cluster-on-docker/build/workspace/mlruns/0/meta.yaml' does not exist..\n",
      " Traceback (most recent call last):\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/hyperopt/spark.py\", line 467, in run_task_on_executor\n",
      "    result = domain.evaluate(\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/hyperopt/base.py\", line 892, in evaluate\n",
      "    rval = self.fn(pyll_rval)\n",
      "  File \"/tmp/ipykernel_229044/4099234749.py\", line 11, in train_model\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/fluent.py\", line 353, in start_run\n",
      "    active_run_obj = client.create_run(experiment_id=exp_id_for_run, tags=resolved_tags)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/client.py\", line 265, in create_run\n",
      "    return self._tracking_client.create_run(experiment_id, start_time, tags)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/_tracking_service/client.py\", line 107, in create_run\n",
      "    return self.store.create_run(\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 583, in create_run\n",
      "    experiment = self.get_experiment(experiment_id)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 430, in get_experiment\n",
      "    experiment = self._get_experiment(experiment_id)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 403, in _get_experiment\n",
      "    meta = FileStore._read_yaml(experiment_dir, FileStore.META_DATA_FILE_NAME)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 1075, in _read_yaml\n",
      "    return _read_helper(root, file_name, attempts_remaining=retries)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 1068, in _read_helper\n",
      "    result = read_yaml(root, file_name)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/utils/file_utils.py\", line 181, in read_yaml\n",
      "    raise MissingConfigException(\"Yaml file '%s' does not exist.\" % file_path)\n",
      "mlflow.exceptions.MissingConfigException: Yaml file '/home/debankurs/Desktop/learningspark/spark-standalone-cluster-on-docker/build/workspace/mlruns/0/meta.yaml' does not exist.\n",
      "\n",
      ")\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:117)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:101)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:105)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:105)\n",
      "\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:100)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:96)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1381)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:96)\n",
      "22/09/24 01:09:30 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.$anonfun$onStageStatusActive$2(CustomListener.scala:402)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.$anonfun$onStageStatusActive$2$adapted(CustomListener.scala:385)\n",
      "\tat scala.collection.TraversableLike$WithFilter.$anonfun$foreach$1(TraversableLike.scala:877)\n",
      "\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)\n",
      "\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)\n",
      "\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:149)\n",
      "\tat scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:876)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onStageStatusActive(CustomListener.scala:385)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener$$anon$1.run(CustomListener.scala:71)\n",
      "\tat java.base/java.util.TimerThread.mainLoop(Timer.java:556)\n",
      "\tat java.base/java.util.TimerThread.run(Timer.java:506)\n",
      "22/09/24 01:09:30 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onStageStatusActive(CustomListener.scala:409)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener$$anon$1.run(CustomListener.scala:71)\n",
      "\tat java.base/java.util.TimerThread.mainLoop(Timer.java:556)\n",
      "\tat java.base/java.util.TimerThread.run(Timer.java:506)\n",
      "22/09/24 01:09:30 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onStageStatusActive(CustomListener.scala:409)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener$$anon$1.run(CustomListener.scala:71)\n",
      "\tat java.base/java.util.TimerThread.mainLoop(Timer.java:556)\n",
      "\tat java.base/java.util.TimerThread.run(Timer.java:506)\n",
      "2022/09/24 01:09:30 INFO mlflow.tracking.fluent: Autologging successfully enabled for sklearn.\n",
      "2022/09/24 01:09:30 INFO mlflow.tracking.fluent: Autologging successfully enabled for pyspark.\n",
      "2022/09/24 01:09:30 INFO mlflow.pyspark.ml: No SparkSession detected. Autologging will log pyspark.ml models contained in the default allowlist. To specify a custom allowlist, initialize a SparkSession prior to calling mlflow.pyspark.ml.autolog() and specify the path to your allowlist file via the spark.mlflow.pysparkml.autolog.logModelAllowlistFile conf.\n",
      "2022/09/24 01:09:30 INFO mlflow.tracking.fluent: Autologging successfully enabled for pyspark.ml.\n",
      "22/09/24 01:09:30 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onJobStart(CustomListener.scala:236)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:37)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:117)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:101)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:105)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:105)\n",
      "\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:100)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:96)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1381)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:96)\n",
      "22/09/24 01:09:30 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onStageSubmitted(CustomListener.scala:379)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:33)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:117)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:101)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:105)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:105)\n",
      "\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:100)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:96)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1381)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:96)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/hyperopt/spark.py\", line 467, in run_task_on_executor\n",
      "    result = domain.evaluate(\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/hyperopt/base.py\", line 892, in evaluate\n",
      "    rval = self.fn(pyll_rval)\n",
      "  File \"/tmp/ipykernel_229044/4099234749.py\", line 11, in train_model\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/fluent.py\", line 353, in start_run\n",
      "    active_run_obj = client.create_run(experiment_id=exp_id_for_run, tags=resolved_tags)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/client.py\", line 265, in create_run\n",
      "    return self._tracking_client.create_run(experiment_id, start_time, tags)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/_tracking_service/client.py\", line 107, in create_run\n",
      "    return self.store.create_run(\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 583, in create_run\n",
      "    experiment = self.get_experiment(experiment_id)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 430, in get_experiment\n",
      "    experiment = self._get_experiment(experiment_id)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 403, in _get_experiment\n",
      "    meta = FileStore._read_yaml(experiment_dir, FileStore.META_DATA_FILE_NAME)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 1075, in _read_yaml\n",
      "    return _read_helper(root, file_name, attempts_remaining=retries)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 1068, in _read_helper\n",
      "    result = read_yaml(root, file_name)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/utils/file_utils.py\", line 181, in read_yaml\n",
      "    raise MissingConfigException(\"Yaml file '%s' does not exist.\" % file_path)\n",
      "mlflow.exceptions.MissingConfigException: Yaml file '/home/debankurs/Desktop/learningspark/spark-standalone-cluster-on-docker/build/workspace/mlruns/0/meta.yaml' does not exist.\n",
      "\n",
      "22/09/24 01:09:30 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onStageCompleted(CustomListener.scala:343)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:35)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:117)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:101)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:105)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:105)\n",
      "\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:100)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:96)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1381)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:96)\n",
      "trial task 2 failed, exception is Yaml file '/home/debankurs/Desktop/learningspark/spark-standalone-cluster-on-docker/build/workspace/mlruns/0/meta.yaml' does not exist..\n",
      " Traceback (most recent call last):\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/hyperopt/spark.py\", line 467, in run_task_on_executor\n",
      "    result = domain.evaluate(\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/hyperopt/base.py\", line 892, in evaluate\n",
      "    rval = self.fn(pyll_rval)\n",
      "  File \"/tmp/ipykernel_229044/4099234749.py\", line 11, in train_model\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/fluent.py\", line 353, in start_run\n",
      "    active_run_obj = client.create_run(experiment_id=exp_id_for_run, tags=resolved_tags)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/client.py\", line 265, in create_run\n",
      "    return self._tracking_client.create_run(experiment_id, start_time, tags)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/_tracking_service/client.py\", line 107, in create_run\n",
      "    return self.store.create_run(\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 583, in create_run\n",
      "    experiment = self.get_experiment(experiment_id)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 430, in get_experiment\n",
      "    experiment = self._get_experiment(experiment_id)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 403, in _get_experiment\n",
      "    meta = FileStore._read_yaml(experiment_dir, FileStore.META_DATA_FILE_NAME)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 1075, in _read_yaml\n",
      "    return _read_helper(root, file_name, attempts_remaining=retries)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 1068, in _read_helper\n",
      "    result = read_yaml(root, file_name)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/utils/file_utils.py\", line 181, in read_yaml\n",
      "    raise MissingConfigException(\"Yaml file '%s' does not exist.\" % file_path)\n",
      "mlflow.exceptions.MissingConfigException: Yaml file '/home/debankurs/Desktop/learningspark/spark-standalone-cluster-on-docker/build/workspace/mlruns/0/meta.yaml' does not exist.\n",
      "\n",
      "22/09/24 01:09:30 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implFlushBuffer(StreamEncoder.java:312)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implFlush(StreamEncoder.java:316)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.flush(StreamEncoder.java:153)\n",
      "\tat java.base/java.io.OutputStreamWriter.flush(OutputStreamWriter.java:251)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:54)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onJobEnd(CustomListener.scala:288)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:39)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:117)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:101)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:105)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:105)\n",
      "\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:100)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:96)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1381)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:96)\n",
      "22/09/24 01:09:31 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.$anonfun$onStageStatusActive$2(CustomListener.scala:402)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.$anonfun$onStageStatusActive$2$adapted(CustomListener.scala:385)\n",
      "\tat scala.collection.TraversableLike$WithFilter.$anonfun$foreach$1(TraversableLike.scala:877)\n",
      "\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)\n",
      "\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)\n",
      "\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:149)\n",
      "\tat scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:876)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onStageStatusActive(CustomListener.scala:385)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener$$anon$1.run(CustomListener.scala:71)\n",
      "\tat java.base/java.util.TimerThread.mainLoop(Timer.java:556)\n",
      "\tat java.base/java.util.TimerThread.run(Timer.java:506)\n",
      "22/09/24 01:09:31 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onStageStatusActive(CustomListener.scala:409)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener$$anon$1.run(CustomListener.scala:71)\n",
      "\tat java.base/java.util.TimerThread.mainLoop(Timer.java:556)\n",
      "\tat java.base/java.util.TimerThread.run(Timer.java:506)\n",
      "22/09/24 01:09:31 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onStageStatusActive(CustomListener.scala:409)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener$$anon$1.run(CustomListener.scala:71)\n",
      "\tat java.base/java.util.TimerThread.mainLoop(Timer.java:556)\n",
      "\tat java.base/java.util.TimerThread.run(Timer.java:506)\n",
      "2022/09/24 01:09:31 INFO mlflow.tracking.fluent: Autologging successfully enabled for sklearn.\n",
      "2022/09/24 01:09:31 INFO mlflow.tracking.fluent: Autologging successfully enabled for pyspark.\n",
      "2022/09/24 01:09:31 INFO mlflow.pyspark.ml: No SparkSession detected. Autologging will log pyspark.ml models contained in the default allowlist. To specify a custom allowlist, initialize a SparkSession prior to calling mlflow.pyspark.ml.autolog() and specify the path to your allowlist file via the spark.mlflow.pysparkml.autolog.logModelAllowlistFile conf.\n",
      "2022/09/24 01:09:31 INFO mlflow.tracking.fluent: Autologging successfully enabled for pyspark.ml.\n",
      "22/09/24 01:09:31 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onJobStart(CustomListener.scala:236)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:37)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:117)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:101)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:105)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:105)\n",
      "\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:100)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:96)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1381)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:96)\n",
      "22/09/24 01:09:31 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onStageSubmitted(CustomListener.scala:379)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:33)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:117)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:101)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:105)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:105)\n",
      "\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:100)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:96)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1381)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:96)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/hyperopt/spark.py\", line 467, in run_task_on_executor\n",
      "    result = domain.evaluate(\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/hyperopt/base.py\", line 892, in evaluate\n",
      "    rval = self.fn(pyll_rval)\n",
      "  File \"/tmp/ipykernel_229044/4099234749.py\", line 11, in train_model\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/fluent.py\", line 353, in start_run\n",
      "    active_run_obj = client.create_run(experiment_id=exp_id_for_run, tags=resolved_tags)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/client.py\", line 265, in create_run\n",
      "    return self._tracking_client.create_run(experiment_id, start_time, tags)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/_tracking_service/client.py\", line 107, in create_run\n",
      "    return self.store.create_run(\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 583, in create_run\n",
      "    experiment = self.get_experiment(experiment_id)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 430, in get_experiment\n",
      "    experiment = self._get_experiment(experiment_id)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 403, in _get_experiment\n",
      "    meta = FileStore._read_yaml(experiment_dir, FileStore.META_DATA_FILE_NAME)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 1075, in _read_yaml\n",
      "    return _read_helper(root, file_name, attempts_remaining=retries)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 1068, in _read_helper\n",
      "    result = read_yaml(root, file_name)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/utils/file_utils.py\", line 181, in read_yaml\n",
      "    raise MissingConfigException(\"Yaml file '%s' does not exist.\" % file_path)\n",
      "mlflow.exceptions.MissingConfigException: Yaml file '/home/debankurs/Desktop/learningspark/spark-standalone-cluster-on-docker/build/workspace/mlruns/0/meta.yaml' does not exist.\n",
      "\n",
      "22/09/24 01:09:31 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onStageCompleted(CustomListener.scala:343)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:35)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:117)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:101)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:105)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:105)\n",
      "\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:100)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:96)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1381)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:96)\n",
      "22/09/24 01:09:31 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implFlushBuffer(StreamEncoder.java:312)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implFlush(StreamEncoder.java:316)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.flush(StreamEncoder.java:153)\n",
      "\tat java.base/java.io.OutputStreamWriter.flush(OutputStreamWriter.java:251)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:54)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onJobEnd(CustomListener.scala:288)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:39)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:117)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:101)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:105)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:105)\n",
      "\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:100)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:96)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1381)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:96)\n",
      "trial task 3 failed, exception is Yaml file '/home/debankurs/Desktop/learningspark/spark-standalone-cluster-on-docker/build/workspace/mlruns/0/meta.yaml' does not exist..\n",
      " Traceback (most recent call last):\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/hyperopt/spark.py\", line 467, in run_task_on_executor\n",
      "    result = domain.evaluate(\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/hyperopt/base.py\", line 892, in evaluate\n",
      "    rval = self.fn(pyll_rval)\n",
      "  File \"/tmp/ipykernel_229044/4099234749.py\", line 11, in train_model\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/fluent.py\", line 353, in start_run\n",
      "    active_run_obj = client.create_run(experiment_id=exp_id_for_run, tags=resolved_tags)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/client.py\", line 265, in create_run\n",
      "    return self._tracking_client.create_run(experiment_id, start_time, tags)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/_tracking_service/client.py\", line 107, in create_run\n",
      "    return self.store.create_run(\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 583, in create_run\n",
      "    experiment = self.get_experiment(experiment_id)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 430, in get_experiment\n",
      "    experiment = self._get_experiment(experiment_id)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 403, in _get_experiment\n",
      "    meta = FileStore._read_yaml(experiment_dir, FileStore.META_DATA_FILE_NAME)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 1075, in _read_yaml\n",
      "    return _read_helper(root, file_name, attempts_remaining=retries)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 1068, in _read_helper\n",
      "    result = read_yaml(root, file_name)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/utils/file_utils.py\", line 181, in read_yaml\n",
      "    raise MissingConfigException(\"Yaml file '%s' does not exist.\" % file_path)\n",
      "mlflow.exceptions.MissingConfigException: Yaml file '/home/debankurs/Desktop/learningspark/spark-standalone-cluster-on-docker/build/workspace/mlruns/0/meta.yaml' does not exist.\n",
      "\n",
      "22/09/24 01:09:32 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.$anonfun$onStageStatusActive$2(CustomListener.scala:402)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.$anonfun$onStageStatusActive$2$adapted(CustomListener.scala:385)\n",
      "\tat scala.collection.TraversableLike$WithFilter.$anonfun$foreach$1(TraversableLike.scala:877)\n",
      "\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)\n",
      "\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)\n",
      "\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:149)\n",
      "\tat scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:876)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onStageStatusActive(CustomListener.scala:385)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener$$anon$1.run(CustomListener.scala:71)\n",
      "\tat java.base/java.util.TimerThread.mainLoop(Timer.java:556)\n",
      "\tat java.base/java.util.TimerThread.run(Timer.java:506)\n",
      "22/09/24 01:09:32 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onStageStatusActive(CustomListener.scala:409)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener$$anon$1.run(CustomListener.scala:71)\n",
      "\tat java.base/java.util.TimerThread.mainLoop(Timer.java:556)\n",
      "\tat java.base/java.util.TimerThread.run(Timer.java:506)\n",
      "22/09/24 01:09:32 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onStageStatusActive(CustomListener.scala:409)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener$$anon$1.run(CustomListener.scala:71)\n",
      "\tat java.base/java.util.TimerThread.mainLoop(Timer.java:556)\n",
      "\tat java.base/java.util.TimerThread.run(Timer.java:506)\n",
      "2022/09/24 01:09:32 INFO mlflow.tracking.fluent: Autologging successfully enabled for sklearn.\n",
      "2022/09/24 01:09:32 INFO mlflow.tracking.fluent: Autologging successfully enabled for pyspark.\n",
      "2022/09/24 01:09:32 INFO mlflow.pyspark.ml: No SparkSession detected. Autologging will log pyspark.ml models contained in the default allowlist. To specify a custom allowlist, initialize a SparkSession prior to calling mlflow.pyspark.ml.autolog() and specify the path to your allowlist file via the spark.mlflow.pysparkml.autolog.logModelAllowlistFile conf.\n",
      "22/09/24 01:09:32 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onJobStart(CustomListener.scala:236)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:37)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:117)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:101)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:105)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:105)\n",
      "\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:100)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:96)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1381)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:96)\n",
      "2022/09/24 01:09:32 INFO mlflow.tracking.fluent: Autologging successfully enabled for pyspark.ml.\n",
      "22/09/24 01:09:32 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onStageSubmitted(CustomListener.scala:379)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:33)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:117)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:101)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:105)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:105)\n",
      "\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:100)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:96)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1381)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:96)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/hyperopt/spark.py\", line 467, in run_task_on_executor\n",
      "    result = domain.evaluate(\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/hyperopt/base.py\", line 892, in evaluate\n",
      "    rval = self.fn(pyll_rval)\n",
      "  File \"/tmp/ipykernel_229044/4099234749.py\", line 11, in train_model\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/fluent.py\", line 353, in start_run\n",
      "    active_run_obj = client.create_run(experiment_id=exp_id_for_run, tags=resolved_tags)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/client.py\", line 265, in create_run\n",
      "    return self._tracking_client.create_run(experiment_id, start_time, tags)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/_tracking_service/client.py\", line 107, in create_run\n",
      "    return self.store.create_run(\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 583, in create_run\n",
      "    experiment = self.get_experiment(experiment_id)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 430, in get_experiment\n",
      "    experiment = self._get_experiment(experiment_id)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 403, in _get_experiment\n",
      "    meta = FileStore._read_yaml(experiment_dir, FileStore.META_DATA_FILE_NAME)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 1075, in _read_yaml\n",
      "    return _read_helper(root, file_name, attempts_remaining=retries)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 1068, in _read_helper\n",
      "    result = read_yaml(root, file_name)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/utils/file_utils.py\", line 181, in read_yaml\n",
      "    raise MissingConfigException(\"Yaml file '%s' does not exist.\" % file_path)\n",
      "mlflow.exceptions.MissingConfigException: Yaml file '/home/debankurs/Desktop/learningspark/spark-standalone-cluster-on-docker/build/workspace/mlruns/0/meta.yaml' does not exist.\n",
      "\n",
      "22/09/24 01:09:32 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onStageCompleted(CustomListener.scala:343)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:35)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:117)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:101)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:105)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:105)\n",
      "\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:100)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:96)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1381)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:96)\n",
      "22/09/24 01:09:32 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implFlushBuffer(StreamEncoder.java:312)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implFlush(StreamEncoder.java:316)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.flush(StreamEncoder.java:153)\n",
      "\tat java.base/java.io.OutputStreamWriter.flush(OutputStreamWriter.java:251)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:54)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onJobEnd(CustomListener.scala:288)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:39)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:117)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:101)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:105)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:105)\n",
      "\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:100)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:96)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1381)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:96)\n",
      "trial task 4 failed, exception is Yaml file '/home/debankurs/Desktop/learningspark/spark-standalone-cluster-on-docker/build/workspace/mlruns/0/meta.yaml' does not exist..\n",
      " Traceback (most recent call last):\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/hyperopt/spark.py\", line 467, in run_task_on_executor\n",
      "    result = domain.evaluate(\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/hyperopt/base.py\", line 892, in evaluate\n",
      "    rval = self.fn(pyll_rval)\n",
      "  File \"/tmp/ipykernel_229044/4099234749.py\", line 11, in train_model\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/fluent.py\", line 353, in start_run\n",
      "    active_run_obj = client.create_run(experiment_id=exp_id_for_run, tags=resolved_tags)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/client.py\", line 265, in create_run\n",
      "    return self._tracking_client.create_run(experiment_id, start_time, tags)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/_tracking_service/client.py\", line 107, in create_run\n",
      "    return self.store.create_run(\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 583, in create_run\n",
      "    experiment = self.get_experiment(experiment_id)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 430, in get_experiment\n",
      "    experiment = self._get_experiment(experiment_id)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 403, in _get_experiment\n",
      "    meta = FileStore._read_yaml(experiment_dir, FileStore.META_DATA_FILE_NAME)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 1075, in _read_yaml\n",
      "    return _read_helper(root, file_name, attempts_remaining=retries)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 1068, in _read_helper\n",
      "    result = read_yaml(root, file_name)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/utils/file_utils.py\", line 181, in read_yaml\n",
      "    raise MissingConfigException(\"Yaml file '%s' does not exist.\" % file_path)\n",
      "mlflow.exceptions.MissingConfigException: Yaml file '/home/debankurs/Desktop/learningspark/spark-standalone-cluster-on-docker/build/workspace/mlruns/0/meta.yaml' does not exist.\n",
      "\n",
      "22/09/24 01:09:33 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.$anonfun$onStageStatusActive$2(CustomListener.scala:402)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.$anonfun$onStageStatusActive$2$adapted(CustomListener.scala:385)\n",
      "\tat scala.collection.TraversableLike$WithFilter.$anonfun$foreach$1(TraversableLike.scala:877)\n",
      "\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)\n",
      "\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)\n",
      "\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:149)\n",
      "\tat scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:876)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onStageStatusActive(CustomListener.scala:385)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener$$anon$1.run(CustomListener.scala:71)\n",
      "\tat java.base/java.util.TimerThread.mainLoop(Timer.java:556)\n",
      "\tat java.base/java.util.TimerThread.run(Timer.java:506)\n",
      "22/09/24 01:09:33 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onStageStatusActive(CustomListener.scala:409)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener$$anon$1.run(CustomListener.scala:71)\n",
      "\tat java.base/java.util.TimerThread.mainLoop(Timer.java:556)\n",
      "\tat java.base/java.util.TimerThread.run(Timer.java:506)\n",
      "22/09/24 01:09:33 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onStageStatusActive(CustomListener.scala:409)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener$$anon$1.run(CustomListener.scala:71)\n",
      "\tat java.base/java.util.TimerThread.mainLoop(Timer.java:556)\n",
      "\tat java.base/java.util.TimerThread.run(Timer.java:506)\n",
      "2022/09/24 01:09:33 INFO mlflow.tracking.fluent: Autologging successfully enabled for sklearn.\n",
      "2022/09/24 01:09:33 INFO mlflow.tracking.fluent: Autologging successfully enabled for pyspark.\n",
      "2022/09/24 01:09:33 INFO mlflow.pyspark.ml: No SparkSession detected. Autologging will log pyspark.ml models contained in the default allowlist. To specify a custom allowlist, initialize a SparkSession prior to calling mlflow.pyspark.ml.autolog() and specify the path to your allowlist file via the spark.mlflow.pysparkml.autolog.logModelAllowlistFile conf.\n",
      "2022/09/24 01:09:33 INFO mlflow.tracking.fluent: Autologging successfully enabled for pyspark.ml.\n",
      "22/09/24 01:09:33 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onJobStart(CustomListener.scala:236)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:37)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:117)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:101)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:105)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:105)\n",
      "\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:100)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:96)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1381)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:96)\n",
      "22/09/24 01:09:33 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onStageSubmitted(CustomListener.scala:379)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:33)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:117)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:101)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:105)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:105)\n",
      "\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:100)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:96)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1381)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:96)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/hyperopt/spark.py\", line 467, in run_task_on_executor\n",
      "    result = domain.evaluate(\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/hyperopt/base.py\", line 892, in evaluate\n",
      "    rval = self.fn(pyll_rval)\n",
      "  File \"/tmp/ipykernel_229044/4099234749.py\", line 11, in train_model\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/fluent.py\", line 353, in start_run\n",
      "    active_run_obj = client.create_run(experiment_id=exp_id_for_run, tags=resolved_tags)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/client.py\", line 265, in create_run\n",
      "    return self._tracking_client.create_run(experiment_id, start_time, tags)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/_tracking_service/client.py\", line 107, in create_run\n",
      "    return self.store.create_run(\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 583, in create_run\n",
      "    experiment = self.get_experiment(experiment_id)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 430, in get_experiment\n",
      "    experiment = self._get_experiment(experiment_id)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 403, in _get_experiment\n",
      "    meta = FileStore._read_yaml(experiment_dir, FileStore.META_DATA_FILE_NAME)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 1075, in _read_yaml\n",
      "    return _read_helper(root, file_name, attempts_remaining=retries)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 1068, in _read_helper\n",
      "    result = read_yaml(root, file_name)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/utils/file_utils.py\", line 181, in read_yaml\n",
      "    raise MissingConfigException(\"Yaml file '%s' does not exist.\" % file_path)\n",
      "mlflow.exceptions.MissingConfigException: Yaml file '/home/debankurs/Desktop/learningspark/spark-standalone-cluster-on-docker/build/workspace/mlruns/0/meta.yaml' does not exist.\n",
      "\n",
      "22/09/24 01:09:33 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onStageCompleted(CustomListener.scala:343)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:35)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:117)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:101)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:105)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:105)\n",
      "\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:100)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:96)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1381)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:96)\n",
      "22/09/24 01:09:33 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implFlushBuffer(StreamEncoder.java:312)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implFlush(StreamEncoder.java:316)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.flush(StreamEncoder.java:153)\n",
      "\tat java.base/java.io.OutputStreamWriter.flush(OutputStreamWriter.java:251)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:54)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onJobEnd(CustomListener.scala:288)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:39)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:117)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:101)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:105)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:105)\n",
      "\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:100)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:96)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1381)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:96)\n",
      "trial task 5 failed, exception is Yaml file '/home/debankurs/Desktop/learningspark/spark-standalone-cluster-on-docker/build/workspace/mlruns/0/meta.yaml' does not exist..\n",
      " Traceback (most recent call last):\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/hyperopt/spark.py\", line 467, in run_task_on_executor\n",
      "    result = domain.evaluate(\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/hyperopt/base.py\", line 892, in evaluate\n",
      "    rval = self.fn(pyll_rval)\n",
      "  File \"/tmp/ipykernel_229044/4099234749.py\", line 11, in train_model\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/fluent.py\", line 353, in start_run\n",
      "    active_run_obj = client.create_run(experiment_id=exp_id_for_run, tags=resolved_tags)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/client.py\", line 265, in create_run\n",
      "    return self._tracking_client.create_run(experiment_id, start_time, tags)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/_tracking_service/client.py\", line 107, in create_run\n",
      "    return self.store.create_run(\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 583, in create_run\n",
      "    experiment = self.get_experiment(experiment_id)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 430, in get_experiment\n",
      "    experiment = self._get_experiment(experiment_id)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 403, in _get_experiment\n",
      "    meta = FileStore._read_yaml(experiment_dir, FileStore.META_DATA_FILE_NAME)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 1075, in _read_yaml\n",
      "    return _read_helper(root, file_name, attempts_remaining=retries)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 1068, in _read_helper\n",
      "    result = read_yaml(root, file_name)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/utils/file_utils.py\", line 181, in read_yaml\n",
      "    raise MissingConfigException(\"Yaml file '%s' does not exist.\" % file_path)\n",
      "mlflow.exceptions.MissingConfigException: Yaml file '/home/debankurs/Desktop/learningspark/spark-standalone-cluster-on-docker/build/workspace/mlruns/0/meta.yaml' does not exist.\n",
      "\n",
      "22/09/24 01:09:34 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.$anonfun$onStageStatusActive$2(CustomListener.scala:402)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.$anonfun$onStageStatusActive$2$adapted(CustomListener.scala:385)\n",
      "\tat scala.collection.TraversableLike$WithFilter.$anonfun$foreach$1(TraversableLike.scala:877)\n",
      "\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)\n",
      "\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)\n",
      "\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:149)\n",
      "\tat scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:876)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onStageStatusActive(CustomListener.scala:385)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener$$anon$1.run(CustomListener.scala:71)\n",
      "\tat java.base/java.util.TimerThread.mainLoop(Timer.java:556)\n",
      "\tat java.base/java.util.TimerThread.run(Timer.java:506)\n",
      "22/09/24 01:09:34 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onStageStatusActive(CustomListener.scala:409)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener$$anon$1.run(CustomListener.scala:71)\n",
      "\tat java.base/java.util.TimerThread.mainLoop(Timer.java:556)\n",
      "\tat java.base/java.util.TimerThread.run(Timer.java:506)\n",
      "22/09/24 01:09:34 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onStageStatusActive(CustomListener.scala:409)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener$$anon$1.run(CustomListener.scala:71)\n",
      "\tat java.base/java.util.TimerThread.mainLoop(Timer.java:556)\n",
      "\tat java.base/java.util.TimerThread.run(Timer.java:506)\n",
      "2022/09/24 01:09:34 INFO mlflow.tracking.fluent: Autologging successfully enabled for sklearn.\n",
      "2022/09/24 01:09:34 INFO mlflow.tracking.fluent: Autologging successfully enabled for pyspark.\n",
      "2022/09/24 01:09:34 INFO mlflow.pyspark.ml: No SparkSession detected. Autologging will log pyspark.ml models contained in the default allowlist. To specify a custom allowlist, initialize a SparkSession prior to calling mlflow.pyspark.ml.autolog() and specify the path to your allowlist file via the spark.mlflow.pysparkml.autolog.logModelAllowlistFile conf.\n",
      "2022/09/24 01:09:34 INFO mlflow.tracking.fluent: Autologging successfully enabled for pyspark.ml.\n",
      "22/09/24 01:09:34 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onJobStart(CustomListener.scala:236)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:37)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:117)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:101)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:105)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:105)\n",
      "\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:100)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:96)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1381)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:96)\n",
      "22/09/24 01:09:34 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onStageSubmitted(CustomListener.scala:379)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:33)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:117)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:101)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:105)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:105)\n",
      "\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:100)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:96)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1381)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:96)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/hyperopt/spark.py\", line 467, in run_task_on_executor\n",
      "    result = domain.evaluate(\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/hyperopt/base.py\", line 892, in evaluate\n",
      "    rval = self.fn(pyll_rval)\n",
      "  File \"/tmp/ipykernel_229044/4099234749.py\", line 11, in train_model\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/fluent.py\", line 353, in start_run\n",
      "    active_run_obj = client.create_run(experiment_id=exp_id_for_run, tags=resolved_tags)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/client.py\", line 265, in create_run\n",
      "    return self._tracking_client.create_run(experiment_id, start_time, tags)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/_tracking_service/client.py\", line 107, in create_run\n",
      "    return self.store.create_run(\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 583, in create_run\n",
      "    experiment = self.get_experiment(experiment_id)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 430, in get_experiment\n",
      "    experiment = self._get_experiment(experiment_id)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 403, in _get_experiment\n",
      "    meta = FileStore._read_yaml(experiment_dir, FileStore.META_DATA_FILE_NAME)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 1075, in _read_yaml\n",
      "    return _read_helper(root, file_name, attempts_remaining=retries)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 1068, in _read_helper\n",
      "    result = read_yaml(root, file_name)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/utils/file_utils.py\", line 181, in read_yaml\n",
      "    raise MissingConfigException(\"Yaml file '%s' does not exist.\" % file_path)\n",
      "mlflow.exceptions.MissingConfigException: Yaml file '/home/debankurs/Desktop/learningspark/spark-standalone-cluster-on-docker/build/workspace/mlruns/0/meta.yaml' does not exist.\n",
      "\n",
      "22/09/24 01:09:34 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onStageCompleted(CustomListener.scala:343)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:35)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:117)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:101)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:105)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:105)\n",
      "\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:100)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:96)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1381)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:96)\n",
      "22/09/24 01:09:34 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implFlushBuffer(StreamEncoder.java:312)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implFlush(StreamEncoder.java:316)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.flush(StreamEncoder.java:153)\n",
      "\tat java.base/java.io.OutputStreamWriter.flush(OutputStreamWriter.java:251)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:54)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onJobEnd(CustomListener.scala:288)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:39)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:117)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:101)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:105)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:105)\n",
      "\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:100)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:96)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1381)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:96)\n",
      "trial task 6 failed, exception is Yaml file '/home/debankurs/Desktop/learningspark/spark-standalone-cluster-on-docker/build/workspace/mlruns/0/meta.yaml' does not exist..\n",
      " Traceback (most recent call last):\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/hyperopt/spark.py\", line 467, in run_task_on_executor\n",
      "    result = domain.evaluate(\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/hyperopt/base.py\", line 892, in evaluate\n",
      "    rval = self.fn(pyll_rval)\n",
      "  File \"/tmp/ipykernel_229044/4099234749.py\", line 11, in train_model\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/fluent.py\", line 353, in start_run\n",
      "    active_run_obj = client.create_run(experiment_id=exp_id_for_run, tags=resolved_tags)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/client.py\", line 265, in create_run\n",
      "    return self._tracking_client.create_run(experiment_id, start_time, tags)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/_tracking_service/client.py\", line 107, in create_run\n",
      "    return self.store.create_run(\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 583, in create_run\n",
      "    experiment = self.get_experiment(experiment_id)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 430, in get_experiment\n",
      "    experiment = self._get_experiment(experiment_id)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 403, in _get_experiment\n",
      "    meta = FileStore._read_yaml(experiment_dir, FileStore.META_DATA_FILE_NAME)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 1075, in _read_yaml\n",
      "    return _read_helper(root, file_name, attempts_remaining=retries)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 1068, in _read_helper\n",
      "    result = read_yaml(root, file_name)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/utils/file_utils.py\", line 181, in read_yaml\n",
      "    raise MissingConfigException(\"Yaml file '%s' does not exist.\" % file_path)\n",
      "mlflow.exceptions.MissingConfigException: Yaml file '/home/debankurs/Desktop/learningspark/spark-standalone-cluster-on-docker/build/workspace/mlruns/0/meta.yaml' does not exist.\n",
      "\n",
      "22/09/24 01:09:35 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.$anonfun$onStageStatusActive$2(CustomListener.scala:402)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.$anonfun$onStageStatusActive$2$adapted(CustomListener.scala:385)\n",
      "\tat scala.collection.TraversableLike$WithFilter.$anonfun$foreach$1(TraversableLike.scala:877)\n",
      "\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)\n",
      "\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)\n",
      "\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:149)\n",
      "\tat scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:876)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onStageStatusActive(CustomListener.scala:385)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener$$anon$1.run(CustomListener.scala:71)\n",
      "\tat java.base/java.util.TimerThread.mainLoop(Timer.java:556)\n",
      "\tat java.base/java.util.TimerThread.run(Timer.java:506)\n",
      "22/09/24 01:09:35 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onStageStatusActive(CustomListener.scala:409)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener$$anon$1.run(CustomListener.scala:71)\n",
      "\tat java.base/java.util.TimerThread.mainLoop(Timer.java:556)\n",
      "\tat java.base/java.util.TimerThread.run(Timer.java:506)\n",
      "22/09/24 01:09:35 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onStageStatusActive(CustomListener.scala:409)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener$$anon$1.run(CustomListener.scala:71)\n",
      "\tat java.base/java.util.TimerThread.mainLoop(Timer.java:556)\n",
      "\tat java.base/java.util.TimerThread.run(Timer.java:506)\n",
      "2022/09/24 01:09:35 INFO mlflow.tracking.fluent: Autologging successfully enabled for sklearn.\n",
      "2022/09/24 01:09:35 INFO mlflow.tracking.fluent: Autologging successfully enabled for pyspark.\n",
      "2022/09/24 01:09:35 INFO mlflow.pyspark.ml: No SparkSession detected. Autologging will log pyspark.ml models contained in the default allowlist. To specify a custom allowlist, initialize a SparkSession prior to calling mlflow.pyspark.ml.autolog() and specify the path to your allowlist file via the spark.mlflow.pysparkml.autolog.logModelAllowlistFile conf.\n",
      "2022/09/24 01:09:35 INFO mlflow.tracking.fluent: Autologging successfully enabled for pyspark.ml.\n",
      "22/09/24 01:09:35 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onJobStart(CustomListener.scala:236)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:37)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:117)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:101)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:105)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:105)\n",
      "\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:100)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:96)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1381)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:96)\n",
      "22/09/24 01:09:35 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onStageSubmitted(CustomListener.scala:379)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:33)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:117)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:101)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:105)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:105)\n",
      "\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:100)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:96)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1381)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:96)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/hyperopt/spark.py\", line 467, in run_task_on_executor\n",
      "    result = domain.evaluate(\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/hyperopt/base.py\", line 892, in evaluate\n",
      "    rval = self.fn(pyll_rval)\n",
      "  File \"/tmp/ipykernel_229044/4099234749.py\", line 11, in train_model\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/fluent.py\", line 353, in start_run\n",
      "    active_run_obj = client.create_run(experiment_id=exp_id_for_run, tags=resolved_tags)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/client.py\", line 265, in create_run\n",
      "    return self._tracking_client.create_run(experiment_id, start_time, tags)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/_tracking_service/client.py\", line 107, in create_run\n",
      "    return self.store.create_run(\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 583, in create_run\n",
      "    experiment = self.get_experiment(experiment_id)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 430, in get_experiment\n",
      "    experiment = self._get_experiment(experiment_id)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 403, in _get_experiment\n",
      "    meta = FileStore._read_yaml(experiment_dir, FileStore.META_DATA_FILE_NAME)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 1075, in _read_yaml\n",
      "    return _read_helper(root, file_name, attempts_remaining=retries)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 1068, in _read_helper\n",
      "    result = read_yaml(root, file_name)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/utils/file_utils.py\", line 181, in read_yaml\n",
      "    raise MissingConfigException(\"Yaml file '%s' does not exist.\" % file_path)\n",
      "mlflow.exceptions.MissingConfigException: Yaml file '/home/debankurs/Desktop/learningspark/spark-standalone-cluster-on-docker/build/workspace/mlruns/0/meta.yaml' does not exist.\n",
      "\n",
      "22/09/24 01:09:35 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onStageCompleted(CustomListener.scala:343)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:35)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:117)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:101)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:105)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:105)\n",
      "\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:100)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:96)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1381)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:96)\n",
      "22/09/24 01:09:35 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implFlushBuffer(StreamEncoder.java:312)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implFlush(StreamEncoder.java:316)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.flush(StreamEncoder.java:153)\n",
      "\tat java.base/java.io.OutputStreamWriter.flush(OutputStreamWriter.java:251)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:54)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onJobEnd(CustomListener.scala:288)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:39)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:117)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:101)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:105)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:105)\n",
      "\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:100)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:96)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1381)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:96)\n",
      "trial task 7 failed, exception is Yaml file '/home/debankurs/Desktop/learningspark/spark-standalone-cluster-on-docker/build/workspace/mlruns/0/meta.yaml' does not exist..\n",
      " Traceback (most recent call last):\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/hyperopt/spark.py\", line 467, in run_task_on_executor\n",
      "    result = domain.evaluate(\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/hyperopt/base.py\", line 892, in evaluate\n",
      "    rval = self.fn(pyll_rval)\n",
      "  File \"/tmp/ipykernel_229044/4099234749.py\", line 11, in train_model\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/fluent.py\", line 353, in start_run\n",
      "    active_run_obj = client.create_run(experiment_id=exp_id_for_run, tags=resolved_tags)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/client.py\", line 265, in create_run\n",
      "    return self._tracking_client.create_run(experiment_id, start_time, tags)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/_tracking_service/client.py\", line 107, in create_run\n",
      "    return self.store.create_run(\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 583, in create_run\n",
      "    experiment = self.get_experiment(experiment_id)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 430, in get_experiment\n",
      "    experiment = self._get_experiment(experiment_id)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 403, in _get_experiment\n",
      "    meta = FileStore._read_yaml(experiment_dir, FileStore.META_DATA_FILE_NAME)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 1075, in _read_yaml\n",
      "    return _read_helper(root, file_name, attempts_remaining=retries)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 1068, in _read_helper\n",
      "    result = read_yaml(root, file_name)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/utils/file_utils.py\", line 181, in read_yaml\n",
      "    raise MissingConfigException(\"Yaml file '%s' does not exist.\" % file_path)\n",
      "mlflow.exceptions.MissingConfigException: Yaml file '/home/debankurs/Desktop/learningspark/spark-standalone-cluster-on-docker/build/workspace/mlruns/0/meta.yaml' does not exist.\n",
      "\n",
      "22/09/24 01:09:36 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.$anonfun$onStageStatusActive$2(CustomListener.scala:402)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.$anonfun$onStageStatusActive$2$adapted(CustomListener.scala:385)\n",
      "\tat scala.collection.TraversableLike$WithFilter.$anonfun$foreach$1(TraversableLike.scala:877)\n",
      "\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)\n",
      "\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)\n",
      "\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:149)\n",
      "\tat scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:876)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onStageStatusActive(CustomListener.scala:385)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener$$anon$1.run(CustomListener.scala:71)\n",
      "\tat java.base/java.util.TimerThread.mainLoop(Timer.java:556)\n",
      "\tat java.base/java.util.TimerThread.run(Timer.java:506)\n",
      "22/09/24 01:09:36 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onStageStatusActive(CustomListener.scala:409)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener$$anon$1.run(CustomListener.scala:71)\n",
      "\tat java.base/java.util.TimerThread.mainLoop(Timer.java:556)\n",
      "\tat java.base/java.util.TimerThread.run(Timer.java:506)\n",
      "22/09/24 01:09:36 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onStageStatusActive(CustomListener.scala:409)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener$$anon$1.run(CustomListener.scala:71)\n",
      "\tat java.base/java.util.TimerThread.mainLoop(Timer.java:556)\n",
      "\tat java.base/java.util.TimerThread.run(Timer.java:506)\n",
      "2022/09/24 01:09:36 INFO mlflow.tracking.fluent: Autologging successfully enabled for sklearn.\n",
      "2022/09/24 01:09:36 INFO mlflow.tracking.fluent: Autologging successfully enabled for pyspark.\n",
      "2022/09/24 01:09:36 INFO mlflow.pyspark.ml: No SparkSession detected. Autologging will log pyspark.ml models contained in the default allowlist. To specify a custom allowlist, initialize a SparkSession prior to calling mlflow.pyspark.ml.autolog() and specify the path to your allowlist file via the spark.mlflow.pysparkml.autolog.logModelAllowlistFile conf.\n",
      "2022/09/24 01:09:36 INFO mlflow.tracking.fluent: Autologging successfully enabled for pyspark.ml.\n",
      "22/09/24 01:09:36 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onJobStart(CustomListener.scala:236)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:37)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:117)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:101)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:105)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:105)\n",
      "\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:100)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:96)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1381)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:96)\n",
      "22/09/24 01:09:36 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onStageSubmitted(CustomListener.scala:379)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:33)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:117)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:101)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:105)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:105)\n",
      "\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:100)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:96)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1381)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:96)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/hyperopt/spark.py\", line 467, in run_task_on_executor\n",
      "    result = domain.evaluate(\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/hyperopt/base.py\", line 892, in evaluate\n",
      "    rval = self.fn(pyll_rval)\n",
      "  File \"/tmp/ipykernel_229044/4099234749.py\", line 11, in train_model\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/fluent.py\", line 353, in start_run\n",
      "    active_run_obj = client.create_run(experiment_id=exp_id_for_run, tags=resolved_tags)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/client.py\", line 265, in create_run\n",
      "    return self._tracking_client.create_run(experiment_id, start_time, tags)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/_tracking_service/client.py\", line 107, in create_run\n",
      "    return self.store.create_run(\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 583, in create_run\n",
      "    experiment = self.get_experiment(experiment_id)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 430, in get_experiment\n",
      "    experiment = self._get_experiment(experiment_id)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 403, in _get_experiment\n",
      "    meta = FileStore._read_yaml(experiment_dir, FileStore.META_DATA_FILE_NAME)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 1075, in _read_yaml\n",
      "    return _read_helper(root, file_name, attempts_remaining=retries)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 1068, in _read_helper\n",
      "    result = read_yaml(root, file_name)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/utils/file_utils.py\", line 181, in read_yaml\n",
      "    raise MissingConfigException(\"Yaml file '%s' does not exist.\" % file_path)\n",
      "mlflow.exceptions.MissingConfigException: Yaml file '/home/debankurs/Desktop/learningspark/spark-standalone-cluster-on-docker/build/workspace/mlruns/0/meta.yaml' does not exist.\n",
      "\n",
      "22/09/24 01:09:36 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onStageCompleted(CustomListener.scala:343)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:35)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:117)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:101)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:105)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:105)\n",
      "\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:100)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:96)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1381)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:96)\n",
      "22/09/24 01:09:36 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implFlushBuffer(StreamEncoder.java:312)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implFlush(StreamEncoder.java:316)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.flush(StreamEncoder.java:153)\n",
      "\tat java.base/java.io.OutputStreamWriter.flush(OutputStreamWriter.java:251)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:54)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onJobEnd(CustomListener.scala:288)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:39)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:117)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:101)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:105)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:105)\n",
      "\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:100)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:96)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1381)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:96)\n",
      "trial task 8 failed, exception is Yaml file '/home/debankurs/Desktop/learningspark/spark-standalone-cluster-on-docker/build/workspace/mlruns/0/meta.yaml' does not exist..\n",
      " Traceback (most recent call last):\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/hyperopt/spark.py\", line 467, in run_task_on_executor\n",
      "    result = domain.evaluate(\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/hyperopt/base.py\", line 892, in evaluate\n",
      "    rval = self.fn(pyll_rval)\n",
      "  File \"/tmp/ipykernel_229044/4099234749.py\", line 11, in train_model\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/fluent.py\", line 353, in start_run\n",
      "    active_run_obj = client.create_run(experiment_id=exp_id_for_run, tags=resolved_tags)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/client.py\", line 265, in create_run\n",
      "    return self._tracking_client.create_run(experiment_id, start_time, tags)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/_tracking_service/client.py\", line 107, in create_run\n",
      "    return self.store.create_run(\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 583, in create_run\n",
      "    experiment = self.get_experiment(experiment_id)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 430, in get_experiment\n",
      "    experiment = self._get_experiment(experiment_id)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 403, in _get_experiment\n",
      "    meta = FileStore._read_yaml(experiment_dir, FileStore.META_DATA_FILE_NAME)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 1075, in _read_yaml\n",
      "    return _read_helper(root, file_name, attempts_remaining=retries)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 1068, in _read_helper\n",
      "    result = read_yaml(root, file_name)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/utils/file_utils.py\", line 181, in read_yaml\n",
      "    raise MissingConfigException(\"Yaml file '%s' does not exist.\" % file_path)\n",
      "mlflow.exceptions.MissingConfigException: Yaml file '/home/debankurs/Desktop/learningspark/spark-standalone-cluster-on-docker/build/workspace/mlruns/0/meta.yaml' does not exist.\n",
      "\n",
      "22/09/24 01:09:37 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.$anonfun$onStageStatusActive$2(CustomListener.scala:402)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.$anonfun$onStageStatusActive$2$adapted(CustomListener.scala:385)\n",
      "\tat scala.collection.TraversableLike$WithFilter.$anonfun$foreach$1(TraversableLike.scala:877)\n",
      "\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)\n",
      "\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)\n",
      "\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:149)\n",
      "\tat scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:876)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onStageStatusActive(CustomListener.scala:385)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener$$anon$1.run(CustomListener.scala:71)\n",
      "\tat java.base/java.util.TimerThread.mainLoop(Timer.java:556)\n",
      "\tat java.base/java.util.TimerThread.run(Timer.java:506)\n",
      "22/09/24 01:09:37 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onStageStatusActive(CustomListener.scala:409)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener$$anon$1.run(CustomListener.scala:71)\n",
      "\tat java.base/java.util.TimerThread.mainLoop(Timer.java:556)\n",
      "\tat java.base/java.util.TimerThread.run(Timer.java:506)\n",
      "22/09/24 01:09:37 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onStageStatusActive(CustomListener.scala:409)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener$$anon$1.run(CustomListener.scala:71)\n",
      "\tat java.base/java.util.TimerThread.mainLoop(Timer.java:556)\n",
      "\tat java.base/java.util.TimerThread.run(Timer.java:506)\n",
      "2022/09/24 01:09:37 INFO mlflow.tracking.fluent: Autologging successfully enabled for sklearn.\n",
      "2022/09/24 01:09:37 INFO mlflow.tracking.fluent: Autologging successfully enabled for pyspark.\n",
      "2022/09/24 01:09:37 INFO mlflow.pyspark.ml: No SparkSession detected. Autologging will log pyspark.ml models contained in the default allowlist. To specify a custom allowlist, initialize a SparkSession prior to calling mlflow.pyspark.ml.autolog() and specify the path to your allowlist file via the spark.mlflow.pysparkml.autolog.logModelAllowlistFile conf.\n",
      "2022/09/24 01:09:37 INFO mlflow.tracking.fluent: Autologging successfully enabled for pyspark.ml.\n",
      "22/09/24 01:09:37 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onJobStart(CustomListener.scala:236)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:37)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:117)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:101)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:105)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:105)\n",
      "\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:100)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:96)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1381)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:96)\n",
      "22/09/24 01:09:37 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onStageSubmitted(CustomListener.scala:379)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:33)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:117)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:101)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:105)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:105)\n",
      "\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:100)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:96)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1381)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:96)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/hyperopt/spark.py\", line 467, in run_task_on_executor\n",
      "    result = domain.evaluate(\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/hyperopt/base.py\", line 892, in evaluate\n",
      "    rval = self.fn(pyll_rval)\n",
      "  File \"/tmp/ipykernel_229044/4099234749.py\", line 11, in train_model\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/fluent.py\", line 353, in start_run\n",
      "    active_run_obj = client.create_run(experiment_id=exp_id_for_run, tags=resolved_tags)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/client.py\", line 265, in create_run\n",
      "    return self._tracking_client.create_run(experiment_id, start_time, tags)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/_tracking_service/client.py\", line 107, in create_run\n",
      "    return self.store.create_run(\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 583, in create_run\n",
      "    experiment = self.get_experiment(experiment_id)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 430, in get_experiment\n",
      "    experiment = self._get_experiment(experiment_id)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 403, in _get_experiment\n",
      "    meta = FileStore._read_yaml(experiment_dir, FileStore.META_DATA_FILE_NAME)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 1075, in _read_yaml\n",
      "    return _read_helper(root, file_name, attempts_remaining=retries)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 1068, in _read_helper\n",
      "    result = read_yaml(root, file_name)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/utils/file_utils.py\", line 181, in read_yaml\n",
      "    raise MissingConfigException(\"Yaml file '%s' does not exist.\" % file_path)\n",
      "mlflow.exceptions.MissingConfigException: Yaml file '/home/debankurs/Desktop/learningspark/spark-standalone-cluster-on-docker/build/workspace/mlruns/0/meta.yaml' does not exist.\n",
      "\n",
      "22/09/24 01:09:37 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onStageCompleted(CustomListener.scala:343)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:35)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:117)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:101)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:105)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:105)\n",
      "\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:100)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:96)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1381)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:96)\n",
      "22/09/24 01:09:37 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implFlushBuffer(StreamEncoder.java:312)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implFlush(StreamEncoder.java:316)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.flush(StreamEncoder.java:153)\n",
      "\tat java.base/java.io.OutputStreamWriter.flush(OutputStreamWriter.java:251)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:54)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onJobEnd(CustomListener.scala:288)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:39)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:117)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:101)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:105)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:105)\n",
      "\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:100)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:96)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1381)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:96)\n",
      "trial task 9 failed, exception is Yaml file '/home/debankurs/Desktop/learningspark/spark-standalone-cluster-on-docker/build/workspace/mlruns/0/meta.yaml' does not exist..\n",
      " Traceback (most recent call last):\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/hyperopt/spark.py\", line 467, in run_task_on_executor\n",
      "    result = domain.evaluate(\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/hyperopt/base.py\", line 892, in evaluate\n",
      "    rval = self.fn(pyll_rval)\n",
      "  File \"/tmp/ipykernel_229044/4099234749.py\", line 11, in train_model\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/fluent.py\", line 353, in start_run\n",
      "    active_run_obj = client.create_run(experiment_id=exp_id_for_run, tags=resolved_tags)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/client.py\", line 265, in create_run\n",
      "    return self._tracking_client.create_run(experiment_id, start_time, tags)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/_tracking_service/client.py\", line 107, in create_run\n",
      "    return self.store.create_run(\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 583, in create_run\n",
      "    experiment = self.get_experiment(experiment_id)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 430, in get_experiment\n",
      "    experiment = self._get_experiment(experiment_id)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 403, in _get_experiment\n",
      "    meta = FileStore._read_yaml(experiment_dir, FileStore.META_DATA_FILE_NAME)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 1075, in _read_yaml\n",
      "    return _read_helper(root, file_name, attempts_remaining=retries)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 1068, in _read_helper\n",
      "    result = read_yaml(root, file_name)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/utils/file_utils.py\", line 181, in read_yaml\n",
      "    raise MissingConfigException(\"Yaml file '%s' does not exist.\" % file_path)\n",
      "mlflow.exceptions.MissingConfigException: Yaml file '/home/debankurs/Desktop/learningspark/spark-standalone-cluster-on-docker/build/workspace/mlruns/0/meta.yaml' does not exist.\n",
      "\n",
      "22/09/24 01:09:38 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.$anonfun$onStageStatusActive$2(CustomListener.scala:402)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.$anonfun$onStageStatusActive$2$adapted(CustomListener.scala:385)\n",
      "\tat scala.collection.TraversableLike$WithFilter.$anonfun$foreach$1(TraversableLike.scala:877)\n",
      "\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)\n",
      "\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)\n",
      "\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:149)\n",
      "\tat scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:876)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onStageStatusActive(CustomListener.scala:385)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener$$anon$1.run(CustomListener.scala:71)\n",
      "\tat java.base/java.util.TimerThread.mainLoop(Timer.java:556)\n",
      "\tat java.base/java.util.TimerThread.run(Timer.java:506)\n",
      "22/09/24 01:09:38 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onStageStatusActive(CustomListener.scala:409)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener$$anon$1.run(CustomListener.scala:71)\n",
      "\tat java.base/java.util.TimerThread.mainLoop(Timer.java:556)\n",
      "\tat java.base/java.util.TimerThread.run(Timer.java:506)\n",
      "22/09/24 01:09:38 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onStageStatusActive(CustomListener.scala:409)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener$$anon$1.run(CustomListener.scala:71)\n",
      "\tat java.base/java.util.TimerThread.mainLoop(Timer.java:556)\n",
      "\tat java.base/java.util.TimerThread.run(Timer.java:506)\n",
      "2022/09/24 01:09:38 INFO mlflow.tracking.fluent: Autologging successfully enabled for sklearn.\n",
      "2022/09/24 01:09:38 INFO mlflow.tracking.fluent: Autologging successfully enabled for pyspark.\n",
      "2022/09/24 01:09:38 INFO mlflow.pyspark.ml: No SparkSession detected. Autologging will log pyspark.ml models contained in the default allowlist. To specify a custom allowlist, initialize a SparkSession prior to calling mlflow.pyspark.ml.autolog() and specify the path to your allowlist file via the spark.mlflow.pysparkml.autolog.logModelAllowlistFile conf.\n",
      "2022/09/24 01:09:38 INFO mlflow.tracking.fluent: Autologging successfully enabled for pyspark.ml.\n",
      "22/09/24 01:09:38 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onJobStart(CustomListener.scala:236)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:37)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:117)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:101)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:105)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:105)\n",
      "\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:100)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:96)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1381)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:96)\n",
      "22/09/24 01:09:38 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onStageSubmitted(CustomListener.scala:379)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:33)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:117)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:101)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:105)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:105)\n",
      "\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:100)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:96)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1381)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:96)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/hyperopt/spark.py\", line 467, in run_task_on_executor\n",
      "    result = domain.evaluate(\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/hyperopt/base.py\", line 892, in evaluate\n",
      "    rval = self.fn(pyll_rval)\n",
      "  File \"/tmp/ipykernel_229044/4099234749.py\", line 11, in train_model\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/fluent.py\", line 353, in start_run\n",
      "    active_run_obj = client.create_run(experiment_id=exp_id_for_run, tags=resolved_tags)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/client.py\", line 265, in create_run\n",
      "    return self._tracking_client.create_run(experiment_id, start_time, tags)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/_tracking_service/client.py\", line 107, in create_run\n",
      "    return self.store.create_run(\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 583, in create_run\n",
      "    experiment = self.get_experiment(experiment_id)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 430, in get_experiment\n",
      "    experiment = self._get_experiment(experiment_id)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 403, in _get_experiment\n",
      "    meta = FileStore._read_yaml(experiment_dir, FileStore.META_DATA_FILE_NAME)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 1075, in _read_yaml\n",
      "    return _read_helper(root, file_name, attempts_remaining=retries)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 1068, in _read_helper\n",
      "    result = read_yaml(root, file_name)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/utils/file_utils.py\", line 181, in read_yaml\n",
      "    raise MissingConfigException(\"Yaml file '%s' does not exist.\" % file_path)\n",
      "mlflow.exceptions.MissingConfigException: Yaml file '/home/debankurs/Desktop/learningspark/spark-standalone-cluster-on-docker/build/workspace/mlruns/0/meta.yaml' does not exist.\n",
      "\n",
      "22/09/24 01:09:38 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onStageCompleted(CustomListener.scala:343)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:35)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:117)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:101)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:105)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:105)\n",
      "\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:100)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:96)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1381)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:96)\n",
      "22/09/24 01:09:38 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onJobEnd(CustomListener.scala:288)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:39)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:117)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:101)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:105)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:105)\n",
      "\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:100)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:96)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1381)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:96)\n",
      "trial task 10 failed, exception is Yaml file '/home/debankurs/Desktop/learningspark/spark-standalone-cluster-on-docker/build/workspace/mlruns/0/meta.yaml' does not exist..\n",
      " Traceback (most recent call last):\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/hyperopt/spark.py\", line 467, in run_task_on_executor\n",
      "    result = domain.evaluate(\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/hyperopt/base.py\", line 892, in evaluate\n",
      "    rval = self.fn(pyll_rval)\n",
      "  File \"/tmp/ipykernel_229044/4099234749.py\", line 11, in train_model\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/fluent.py\", line 353, in start_run\n",
      "    active_run_obj = client.create_run(experiment_id=exp_id_for_run, tags=resolved_tags)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/client.py\", line 265, in create_run\n",
      "    return self._tracking_client.create_run(experiment_id, start_time, tags)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/_tracking_service/client.py\", line 107, in create_run\n",
      "    return self.store.create_run(\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 583, in create_run\n",
      "    experiment = self.get_experiment(experiment_id)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 430, in get_experiment\n",
      "    experiment = self._get_experiment(experiment_id)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 403, in _get_experiment\n",
      "    meta = FileStore._read_yaml(experiment_dir, FileStore.META_DATA_FILE_NAME)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 1075, in _read_yaml\n",
      "    return _read_helper(root, file_name, attempts_remaining=retries)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 1068, in _read_helper\n",
      "    result = read_yaml(root, file_name)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/utils/file_utils.py\", line 181, in read_yaml\n",
      "    raise MissingConfigException(\"Yaml file '%s' does not exist.\" % file_path)\n",
      "mlflow.exceptions.MissingConfigException: Yaml file '/home/debankurs/Desktop/learningspark/spark-standalone-cluster-on-docker/build/workspace/mlruns/0/meta.yaml' does not exist.\n",
      "\n",
      "22/09/24 01:09:39 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.$anonfun$onStageStatusActive$2(CustomListener.scala:402)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.$anonfun$onStageStatusActive$2$adapted(CustomListener.scala:385)\n",
      "\tat scala.collection.TraversableLike$WithFilter.$anonfun$foreach$1(TraversableLike.scala:877)\n",
      "\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)\n",
      "\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)\n",
      "\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:149)\n",
      "\tat scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:876)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onStageStatusActive(CustomListener.scala:385)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener$$anon$1.run(CustomListener.scala:71)\n",
      "\tat java.base/java.util.TimerThread.mainLoop(Timer.java:556)\n",
      "\tat java.base/java.util.TimerThread.run(Timer.java:506)\n",
      "22/09/24 01:09:39 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onStageStatusActive(CustomListener.scala:409)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener$$anon$1.run(CustomListener.scala:71)\n",
      "\tat java.base/java.util.TimerThread.mainLoop(Timer.java:556)\n",
      "\tat java.base/java.util.TimerThread.run(Timer.java:506)\n",
      "22/09/24 01:09:39 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onStageStatusActive(CustomListener.scala:409)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener$$anon$1.run(CustomListener.scala:71)\n",
      "\tat java.base/java.util.TimerThread.mainLoop(Timer.java:556)\n",
      "\tat java.base/java.util.TimerThread.run(Timer.java:506)\n",
      "2022/09/24 01:09:39 INFO mlflow.tracking.fluent: Autologging successfully enabled for sklearn.\n",
      "2022/09/24 01:09:39 INFO mlflow.tracking.fluent: Autologging successfully enabled for pyspark.\n",
      "2022/09/24 01:09:39 INFO mlflow.pyspark.ml: No SparkSession detected. Autologging will log pyspark.ml models contained in the default allowlist. To specify a custom allowlist, initialize a SparkSession prior to calling mlflow.pyspark.ml.autolog() and specify the path to your allowlist file via the spark.mlflow.pysparkml.autolog.logModelAllowlistFile conf.\n",
      "2022/09/24 01:09:39 INFO mlflow.tracking.fluent: Autologging successfully enabled for pyspark.ml.\n",
      "22/09/24 01:09:39 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onJobStart(CustomListener.scala:236)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:37)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:117)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:101)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:105)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:105)\n",
      "\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:100)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:96)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1381)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:96)\n",
      "22/09/24 01:09:39 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onStageSubmitted(CustomListener.scala:379)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:33)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:117)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:101)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:105)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:105)\n",
      "\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:100)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:96)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1381)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:96)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/hyperopt/spark.py\", line 467, in run_task_on_executor\n",
      "    result = domain.evaluate(\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/hyperopt/base.py\", line 892, in evaluate\n",
      "    rval = self.fn(pyll_rval)\n",
      "  File \"/tmp/ipykernel_229044/4099234749.py\", line 11, in train_model\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/fluent.py\", line 353, in start_run\n",
      "    active_run_obj = client.create_run(experiment_id=exp_id_for_run, tags=resolved_tags)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/client.py\", line 265, in create_run\n",
      "    return self._tracking_client.create_run(experiment_id, start_time, tags)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/_tracking_service/client.py\", line 107, in create_run\n",
      "    return self.store.create_run(\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 583, in create_run\n",
      "    experiment = self.get_experiment(experiment_id)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 430, in get_experiment\n",
      "    experiment = self._get_experiment(experiment_id)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 403, in _get_experiment\n",
      "    meta = FileStore._read_yaml(experiment_dir, FileStore.META_DATA_FILE_NAME)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 1075, in _read_yaml\n",
      "    return _read_helper(root, file_name, attempts_remaining=retries)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 1068, in _read_helper\n",
      "    result = read_yaml(root, file_name)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/utils/file_utils.py\", line 181, in read_yaml\n",
      "    raise MissingConfigException(\"Yaml file '%s' does not exist.\" % file_path)\n",
      "mlflow.exceptions.MissingConfigException: Yaml file '/home/debankurs/Desktop/learningspark/spark-standalone-cluster-on-docker/build/workspace/mlruns/0/meta.yaml' does not exist.\n",
      "\n",
      "22/09/24 01:09:39 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onStageCompleted(CustomListener.scala:343)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:35)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:117)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:101)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:105)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:105)\n",
      "\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:100)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:96)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1381)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:96)\n",
      "22/09/24 01:09:39 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onJobEnd(CustomListener.scala:288)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:39)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:117)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:101)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:105)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:105)\n",
      "\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:100)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:96)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1381)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:96)\n",
      "trial task 11 failed, exception is Yaml file '/home/debankurs/Desktop/learningspark/spark-standalone-cluster-on-docker/build/workspace/mlruns/0/meta.yaml' does not exist..\n",
      " Traceback (most recent call last):\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/hyperopt/spark.py\", line 467, in run_task_on_executor\n",
      "    result = domain.evaluate(\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/hyperopt/base.py\", line 892, in evaluate\n",
      "    rval = self.fn(pyll_rval)\n",
      "  File \"/tmp/ipykernel_229044/4099234749.py\", line 11, in train_model\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/fluent.py\", line 353, in start_run\n",
      "    active_run_obj = client.create_run(experiment_id=exp_id_for_run, tags=resolved_tags)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/client.py\", line 265, in create_run\n",
      "    return self._tracking_client.create_run(experiment_id, start_time, tags)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/_tracking_service/client.py\", line 107, in create_run\n",
      "    return self.store.create_run(\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 583, in create_run\n",
      "    experiment = self.get_experiment(experiment_id)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 430, in get_experiment\n",
      "    experiment = self._get_experiment(experiment_id)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 403, in _get_experiment\n",
      "    meta = FileStore._read_yaml(experiment_dir, FileStore.META_DATA_FILE_NAME)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 1075, in _read_yaml\n",
      "    return _read_helper(root, file_name, attempts_remaining=retries)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 1068, in _read_helper\n",
      "    result = read_yaml(root, file_name)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/utils/file_utils.py\", line 181, in read_yaml\n",
      "    raise MissingConfigException(\"Yaml file '%s' does not exist.\" % file_path)\n",
      "mlflow.exceptions.MissingConfigException: Yaml file '/home/debankurs/Desktop/learningspark/spark-standalone-cluster-on-docker/build/workspace/mlruns/0/meta.yaml' does not exist.\n",
      "\n",
      "22/09/24 01:09:40 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.$anonfun$onStageStatusActive$2(CustomListener.scala:402)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.$anonfun$onStageStatusActive$2$adapted(CustomListener.scala:385)\n",
      "\tat scala.collection.TraversableLike$WithFilter.$anonfun$foreach$1(TraversableLike.scala:877)\n",
      "\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)\n",
      "\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)\n",
      "\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:149)\n",
      "\tat scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:876)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onStageStatusActive(CustomListener.scala:385)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener$$anon$1.run(CustomListener.scala:71)\n",
      "\tat java.base/java.util.TimerThread.mainLoop(Timer.java:556)\n",
      "\tat java.base/java.util.TimerThread.run(Timer.java:506)\n",
      "22/09/24 01:09:40 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onStageStatusActive(CustomListener.scala:409)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener$$anon$1.run(CustomListener.scala:71)\n",
      "\tat java.base/java.util.TimerThread.mainLoop(Timer.java:556)\n",
      "\tat java.base/java.util.TimerThread.run(Timer.java:506)\n",
      "22/09/24 01:09:40 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onStageStatusActive(CustomListener.scala:409)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener$$anon$1.run(CustomListener.scala:71)\n",
      "\tat java.base/java.util.TimerThread.mainLoop(Timer.java:556)\n",
      "\tat java.base/java.util.TimerThread.run(Timer.java:506)\n",
      "2022/09/24 01:09:40 INFO mlflow.tracking.fluent: Autologging successfully enabled for sklearn.\n",
      "2022/09/24 01:09:40 INFO mlflow.tracking.fluent: Autologging successfully enabled for pyspark.\n",
      "2022/09/24 01:09:40 INFO mlflow.pyspark.ml: No SparkSession detected. Autologging will log pyspark.ml models contained in the default allowlist. To specify a custom allowlist, initialize a SparkSession prior to calling mlflow.pyspark.ml.autolog() and specify the path to your allowlist file via the spark.mlflow.pysparkml.autolog.logModelAllowlistFile conf.\n",
      "2022/09/24 01:09:40 INFO mlflow.tracking.fluent: Autologging successfully enabled for pyspark.ml.\n",
      "22/09/24 01:09:40 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onJobStart(CustomListener.scala:236)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:37)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:117)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:101)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:105)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:105)\n",
      "\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:100)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:96)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1381)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:96)\n",
      "22/09/24 01:09:40 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onStageSubmitted(CustomListener.scala:379)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:33)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:117)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:101)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:105)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:105)\n",
      "\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:100)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:96)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1381)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:96)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/hyperopt/spark.py\", line 467, in run_task_on_executor\n",
      "    result = domain.evaluate(\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/hyperopt/base.py\", line 892, in evaluate\n",
      "    rval = self.fn(pyll_rval)\n",
      "  File \"/tmp/ipykernel_229044/4099234749.py\", line 11, in train_model\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/fluent.py\", line 353, in start_run\n",
      "    active_run_obj = client.create_run(experiment_id=exp_id_for_run, tags=resolved_tags)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/client.py\", line 265, in create_run\n",
      "    return self._tracking_client.create_run(experiment_id, start_time, tags)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/_tracking_service/client.py\", line 107, in create_run\n",
      "    return self.store.create_run(\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 583, in create_run\n",
      "    experiment = self.get_experiment(experiment_id)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 430, in get_experiment\n",
      "    experiment = self._get_experiment(experiment_id)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 403, in _get_experiment\n",
      "    meta = FileStore._read_yaml(experiment_dir, FileStore.META_DATA_FILE_NAME)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 1075, in _read_yaml\n",
      "    return _read_helper(root, file_name, attempts_remaining=retries)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 1068, in _read_helper\n",
      "    result = read_yaml(root, file_name)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/utils/file_utils.py\", line 181, in read_yaml\n",
      "    raise MissingConfigException(\"Yaml file '%s' does not exist.\" % file_path)\n",
      "mlflow.exceptions.MissingConfigException: Yaml file '/home/debankurs/Desktop/learningspark/spark-standalone-cluster-on-docker/build/workspace/mlruns/0/meta.yaml' does not exist.\n",
      "\n",
      "22/09/24 01:09:40 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onStageCompleted(CustomListener.scala:343)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:35)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:117)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:101)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:105)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:105)\n",
      "\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:100)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:96)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1381)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:96)\n",
      "22/09/24 01:09:40 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onJobEnd(CustomListener.scala:288)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:39)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:117)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:101)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:105)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:105)\n",
      "\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:100)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:96)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1381)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:96)\n",
      "trial task 12 failed, exception is Yaml file '/home/debankurs/Desktop/learningspark/spark-standalone-cluster-on-docker/build/workspace/mlruns/0/meta.yaml' does not exist..\n",
      " Traceback (most recent call last):\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/hyperopt/spark.py\", line 467, in run_task_on_executor\n",
      "    result = domain.evaluate(\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/hyperopt/base.py\", line 892, in evaluate\n",
      "    rval = self.fn(pyll_rval)\n",
      "  File \"/tmp/ipykernel_229044/4099234749.py\", line 11, in train_model\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/fluent.py\", line 353, in start_run\n",
      "    active_run_obj = client.create_run(experiment_id=exp_id_for_run, tags=resolved_tags)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/client.py\", line 265, in create_run\n",
      "    return self._tracking_client.create_run(experiment_id, start_time, tags)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/_tracking_service/client.py\", line 107, in create_run\n",
      "    return self.store.create_run(\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 583, in create_run\n",
      "    experiment = self.get_experiment(experiment_id)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 430, in get_experiment\n",
      "    experiment = self._get_experiment(experiment_id)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 403, in _get_experiment\n",
      "    meta = FileStore._read_yaml(experiment_dir, FileStore.META_DATA_FILE_NAME)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 1075, in _read_yaml\n",
      "    return _read_helper(root, file_name, attempts_remaining=retries)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 1068, in _read_helper\n",
      "    result = read_yaml(root, file_name)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/utils/file_utils.py\", line 181, in read_yaml\n",
      "    raise MissingConfigException(\"Yaml file '%s' does not exist.\" % file_path)\n",
      "mlflow.exceptions.MissingConfigException: Yaml file '/home/debankurs/Desktop/learningspark/spark-standalone-cluster-on-docker/build/workspace/mlruns/0/meta.yaml' does not exist.\n",
      "\n",
      "22/09/24 01:09:41 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.$anonfun$onStageStatusActive$2(CustomListener.scala:402)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.$anonfun$onStageStatusActive$2$adapted(CustomListener.scala:385)\n",
      "\tat scala.collection.TraversableLike$WithFilter.$anonfun$foreach$1(TraversableLike.scala:877)\n",
      "\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)\n",
      "\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)\n",
      "\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:149)\n",
      "\tat scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:876)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onStageStatusActive(CustomListener.scala:385)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener$$anon$1.run(CustomListener.scala:71)\n",
      "\tat java.base/java.util.TimerThread.mainLoop(Timer.java:556)\n",
      "\tat java.base/java.util.TimerThread.run(Timer.java:506)\n",
      "22/09/24 01:09:41 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onStageStatusActive(CustomListener.scala:409)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener$$anon$1.run(CustomListener.scala:71)\n",
      "\tat java.base/java.util.TimerThread.mainLoop(Timer.java:556)\n",
      "\tat java.base/java.util.TimerThread.run(Timer.java:506)\n",
      "22/09/24 01:09:41 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onStageStatusActive(CustomListener.scala:409)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener$$anon$1.run(CustomListener.scala:71)\n",
      "\tat java.base/java.util.TimerThread.mainLoop(Timer.java:556)\n",
      "\tat java.base/java.util.TimerThread.run(Timer.java:506)\n",
      "2022/09/24 01:09:41 INFO mlflow.tracking.fluent: Autologging successfully enabled for sklearn.\n",
      "2022/09/24 01:09:41 INFO mlflow.tracking.fluent: Autologging successfully enabled for pyspark.\n",
      "2022/09/24 01:09:41 INFO mlflow.pyspark.ml: No SparkSession detected. Autologging will log pyspark.ml models contained in the default allowlist. To specify a custom allowlist, initialize a SparkSession prior to calling mlflow.pyspark.ml.autolog() and specify the path to your allowlist file via the spark.mlflow.pysparkml.autolog.logModelAllowlistFile conf.\n",
      "2022/09/24 01:09:41 INFO mlflow.tracking.fluent: Autologging successfully enabled for pyspark.ml.\n",
      "22/09/24 01:09:41 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onJobStart(CustomListener.scala:236)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:37)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:117)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:101)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:105)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:105)\n",
      "\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:100)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:96)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1381)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:96)\n",
      "22/09/24 01:09:41 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onStageSubmitted(CustomListener.scala:379)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:33)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:117)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:101)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:105)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:105)\n",
      "\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:100)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:96)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1381)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:96)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/hyperopt/spark.py\", line 467, in run_task_on_executor\n",
      "    result = domain.evaluate(\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/hyperopt/base.py\", line 892, in evaluate\n",
      "    rval = self.fn(pyll_rval)\n",
      "  File \"/tmp/ipykernel_229044/4099234749.py\", line 11, in train_model\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/fluent.py\", line 353, in start_run\n",
      "    active_run_obj = client.create_run(experiment_id=exp_id_for_run, tags=resolved_tags)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/client.py\", line 265, in create_run\n",
      "    return self._tracking_client.create_run(experiment_id, start_time, tags)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/_tracking_service/client.py\", line 107, in create_run\n",
      "    return self.store.create_run(\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 583, in create_run\n",
      "    experiment = self.get_experiment(experiment_id)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 430, in get_experiment\n",
      "    experiment = self._get_experiment(experiment_id)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 403, in _get_experiment\n",
      "    meta = FileStore._read_yaml(experiment_dir, FileStore.META_DATA_FILE_NAME)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 1075, in _read_yaml\n",
      "    return _read_helper(root, file_name, attempts_remaining=retries)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 1068, in _read_helper\n",
      "    result = read_yaml(root, file_name)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/utils/file_utils.py\", line 181, in read_yaml\n",
      "    raise MissingConfigException(\"Yaml file '%s' does not exist.\" % file_path)\n",
      "mlflow.exceptions.MissingConfigException: Yaml file '/home/debankurs/Desktop/learningspark/spark-standalone-cluster-on-docker/build/workspace/mlruns/0/meta.yaml' does not exist.\n",
      "\n",
      "22/09/24 01:09:41 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onStageCompleted(CustomListener.scala:343)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:35)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:117)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:101)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:105)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:105)\n",
      "\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:100)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:96)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1381)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:96)\n",
      "22/09/24 01:09:41 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onJobEnd(CustomListener.scala:288)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:39)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:117)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:101)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:105)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:105)\n",
      "\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:100)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:96)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1381)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:96)\n",
      "trial task 13 failed, exception is Yaml file '/home/debankurs/Desktop/learningspark/spark-standalone-cluster-on-docker/build/workspace/mlruns/0/meta.yaml' does not exist..\n",
      " Traceback (most recent call last):\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/hyperopt/spark.py\", line 467, in run_task_on_executor\n",
      "    result = domain.evaluate(\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/hyperopt/base.py\", line 892, in evaluate\n",
      "    rval = self.fn(pyll_rval)\n",
      "  File \"/tmp/ipykernel_229044/4099234749.py\", line 11, in train_model\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/fluent.py\", line 353, in start_run\n",
      "    active_run_obj = client.create_run(experiment_id=exp_id_for_run, tags=resolved_tags)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/client.py\", line 265, in create_run\n",
      "    return self._tracking_client.create_run(experiment_id, start_time, tags)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/_tracking_service/client.py\", line 107, in create_run\n",
      "    return self.store.create_run(\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 583, in create_run\n",
      "    experiment = self.get_experiment(experiment_id)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 430, in get_experiment\n",
      "    experiment = self._get_experiment(experiment_id)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 403, in _get_experiment\n",
      "    meta = FileStore._read_yaml(experiment_dir, FileStore.META_DATA_FILE_NAME)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 1075, in _read_yaml\n",
      "    return _read_helper(root, file_name, attempts_remaining=retries)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 1068, in _read_helper\n",
      "    result = read_yaml(root, file_name)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/utils/file_utils.py\", line 181, in read_yaml\n",
      "    raise MissingConfigException(\"Yaml file '%s' does not exist.\" % file_path)\n",
      "mlflow.exceptions.MissingConfigException: Yaml file '/home/debankurs/Desktop/learningspark/spark-standalone-cluster-on-docker/build/workspace/mlruns/0/meta.yaml' does not exist.\n",
      "\n",
      "22/09/24 01:09:42 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.$anonfun$onStageStatusActive$2(CustomListener.scala:402)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.$anonfun$onStageStatusActive$2$adapted(CustomListener.scala:385)\n",
      "\tat scala.collection.TraversableLike$WithFilter.$anonfun$foreach$1(TraversableLike.scala:877)\n",
      "\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)\n",
      "\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)\n",
      "\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:149)\n",
      "\tat scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:876)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onStageStatusActive(CustomListener.scala:385)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener$$anon$1.run(CustomListener.scala:71)\n",
      "\tat java.base/java.util.TimerThread.mainLoop(Timer.java:556)\n",
      "\tat java.base/java.util.TimerThread.run(Timer.java:506)\n",
      "22/09/24 01:09:42 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onStageStatusActive(CustomListener.scala:409)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener$$anon$1.run(CustomListener.scala:71)\n",
      "\tat java.base/java.util.TimerThread.mainLoop(Timer.java:556)\n",
      "\tat java.base/java.util.TimerThread.run(Timer.java:506)\n",
      "22/09/24 01:09:42 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onStageStatusActive(CustomListener.scala:409)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener$$anon$1.run(CustomListener.scala:71)\n",
      "\tat java.base/java.util.TimerThread.mainLoop(Timer.java:556)\n",
      "\tat java.base/java.util.TimerThread.run(Timer.java:506)\n",
      "2022/09/24 01:09:42 INFO mlflow.tracking.fluent: Autologging successfully enabled for sklearn.\n",
      "2022/09/24 01:09:42 INFO mlflow.tracking.fluent: Autologging successfully enabled for pyspark.\n",
      "2022/09/24 01:09:42 INFO mlflow.pyspark.ml: No SparkSession detected. Autologging will log pyspark.ml models contained in the default allowlist. To specify a custom allowlist, initialize a SparkSession prior to calling mlflow.pyspark.ml.autolog() and specify the path to your allowlist file via the spark.mlflow.pysparkml.autolog.logModelAllowlistFile conf.\n",
      "2022/09/24 01:09:42 INFO mlflow.tracking.fluent: Autologging successfully enabled for pyspark.ml.\n",
      "22/09/24 01:09:42 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onJobStart(CustomListener.scala:236)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:37)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:117)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:101)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:105)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:105)\n",
      "\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:100)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:96)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1381)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:96)\n",
      "22/09/24 01:09:42 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onStageSubmitted(CustomListener.scala:379)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:33)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:117)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:101)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:105)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:105)\n",
      "\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:100)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:96)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1381)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:96)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/hyperopt/spark.py\", line 467, in run_task_on_executor\n",
      "    result = domain.evaluate(\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/hyperopt/base.py\", line 892, in evaluate\n",
      "    rval = self.fn(pyll_rval)\n",
      "  File \"/tmp/ipykernel_229044/4099234749.py\", line 11, in train_model\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/fluent.py\", line 353, in start_run\n",
      "    active_run_obj = client.create_run(experiment_id=exp_id_for_run, tags=resolved_tags)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/client.py\", line 265, in create_run\n",
      "    return self._tracking_client.create_run(experiment_id, start_time, tags)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/_tracking_service/client.py\", line 107, in create_run\n",
      "    return self.store.create_run(\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 583, in create_run\n",
      "    experiment = self.get_experiment(experiment_id)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 430, in get_experiment\n",
      "    experiment = self._get_experiment(experiment_id)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 403, in _get_experiment\n",
      "    meta = FileStore._read_yaml(experiment_dir, FileStore.META_DATA_FILE_NAME)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 1075, in _read_yaml\n",
      "    return _read_helper(root, file_name, attempts_remaining=retries)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 1068, in _read_helper\n",
      "    result = read_yaml(root, file_name)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/utils/file_utils.py\", line 181, in read_yaml\n",
      "    raise MissingConfigException(\"Yaml file '%s' does not exist.\" % file_path)\n",
      "mlflow.exceptions.MissingConfigException: Yaml file '/home/debankurs/Desktop/learningspark/spark-standalone-cluster-on-docker/build/workspace/mlruns/0/meta.yaml' does not exist.\n",
      "\n",
      "22/09/24 01:09:42 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onStageCompleted(CustomListener.scala:343)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:35)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:117)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:101)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:105)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:105)\n",
      "\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:100)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:96)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1381)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:96)\n",
      "22/09/24 01:09:42 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onJobEnd(CustomListener.scala:288)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:39)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:117)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:101)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:105)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:105)\n",
      "\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:100)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:96)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1381)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:96)\n",
      "trial task 14 failed, exception is Yaml file '/home/debankurs/Desktop/learningspark/spark-standalone-cluster-on-docker/build/workspace/mlruns/0/meta.yaml' does not exist..\n",
      " Traceback (most recent call last):\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/hyperopt/spark.py\", line 467, in run_task_on_executor\n",
      "    result = domain.evaluate(\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/hyperopt/base.py\", line 892, in evaluate\n",
      "    rval = self.fn(pyll_rval)\n",
      "  File \"/tmp/ipykernel_229044/4099234749.py\", line 11, in train_model\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/fluent.py\", line 353, in start_run\n",
      "    active_run_obj = client.create_run(experiment_id=exp_id_for_run, tags=resolved_tags)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/client.py\", line 265, in create_run\n",
      "    return self._tracking_client.create_run(experiment_id, start_time, tags)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/_tracking_service/client.py\", line 107, in create_run\n",
      "    return self.store.create_run(\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 583, in create_run\n",
      "    experiment = self.get_experiment(experiment_id)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 430, in get_experiment\n",
      "    experiment = self._get_experiment(experiment_id)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 403, in _get_experiment\n",
      "    meta = FileStore._read_yaml(experiment_dir, FileStore.META_DATA_FILE_NAME)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 1075, in _read_yaml\n",
      "    return _read_helper(root, file_name, attempts_remaining=retries)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 1068, in _read_helper\n",
      "    result = read_yaml(root, file_name)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/utils/file_utils.py\", line 181, in read_yaml\n",
      "    raise MissingConfigException(\"Yaml file '%s' does not exist.\" % file_path)\n",
      "mlflow.exceptions.MissingConfigException: Yaml file '/home/debankurs/Desktop/learningspark/spark-standalone-cluster-on-docker/build/workspace/mlruns/0/meta.yaml' does not exist.\n",
      "\n",
      "22/09/24 01:09:43 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.$anonfun$onStageStatusActive$2(CustomListener.scala:402)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.$anonfun$onStageStatusActive$2$adapted(CustomListener.scala:385)\n",
      "\tat scala.collection.TraversableLike$WithFilter.$anonfun$foreach$1(TraversableLike.scala:877)\n",
      "\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)\n",
      "\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)\n",
      "\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:149)\n",
      "\tat scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:876)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onStageStatusActive(CustomListener.scala:385)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener$$anon$1.run(CustomListener.scala:71)\n",
      "\tat java.base/java.util.TimerThread.mainLoop(Timer.java:556)\n",
      "\tat java.base/java.util.TimerThread.run(Timer.java:506)\n",
      "22/09/24 01:09:43 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onStageStatusActive(CustomListener.scala:409)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener$$anon$1.run(CustomListener.scala:71)\n",
      "\tat java.base/java.util.TimerThread.mainLoop(Timer.java:556)\n",
      "\tat java.base/java.util.TimerThread.run(Timer.java:506)\n",
      "22/09/24 01:09:43 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onStageStatusActive(CustomListener.scala:409)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener$$anon$1.run(CustomListener.scala:71)\n",
      "\tat java.base/java.util.TimerThread.mainLoop(Timer.java:556)\n",
      "\tat java.base/java.util.TimerThread.run(Timer.java:506)\n",
      "2022/09/24 01:09:43 INFO mlflow.tracking.fluent: Autologging successfully enabled for sklearn.\n",
      "22/09/24 01:09:43 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onJobStart(CustomListener.scala:236)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:37)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:117)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:101)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:105)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:105)\n",
      "\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:100)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:96)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1381)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:96)\n",
      "22/09/24 01:09:43 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onStageSubmitted(CustomListener.scala:379)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:33)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:117)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:101)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:105)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:105)\n",
      "\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:100)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:96)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1381)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:96)\n",
      "2022/09/24 01:09:43 INFO mlflow.tracking.fluent: Autologging successfully enabled for pyspark.\n",
      "2022/09/24 01:09:43 INFO mlflow.pyspark.ml: No SparkSession detected. Autologging will log pyspark.ml models contained in the default allowlist. To specify a custom allowlist, initialize a SparkSession prior to calling mlflow.pyspark.ml.autolog() and specify the path to your allowlist file via the spark.mlflow.pysparkml.autolog.logModelAllowlistFile conf.\n",
      "2022/09/24 01:09:43 INFO mlflow.tracking.fluent: Autologging successfully enabled for pyspark.ml.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/hyperopt/spark.py\", line 467, in run_task_on_executor\n",
      "    result = domain.evaluate(\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/hyperopt/base.py\", line 892, in evaluate\n",
      "    rval = self.fn(pyll_rval)\n",
      "  File \"/tmp/ipykernel_229044/4099234749.py\", line 11, in train_model\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/fluent.py\", line 353, in start_run\n",
      "    active_run_obj = client.create_run(experiment_id=exp_id_for_run, tags=resolved_tags)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/client.py\", line 265, in create_run\n",
      "    return self._tracking_client.create_run(experiment_id, start_time, tags)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/_tracking_service/client.py\", line 107, in create_run\n",
      "    return self.store.create_run(\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 583, in create_run\n",
      "    experiment = self.get_experiment(experiment_id)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 430, in get_experiment\n",
      "    experiment = self._get_experiment(experiment_id)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 403, in _get_experiment\n",
      "    meta = FileStore._read_yaml(experiment_dir, FileStore.META_DATA_FILE_NAME)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 1075, in _read_yaml\n",
      "    return _read_helper(root, file_name, attempts_remaining=retries)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 1068, in _read_helper\n",
      "    result = read_yaml(root, file_name)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/utils/file_utils.py\", line 181, in read_yaml\n",
      "    raise MissingConfigException(\"Yaml file '%s' does not exist.\" % file_path)\n",
      "mlflow.exceptions.MissingConfigException: Yaml file '/home/debankurs/Desktop/learningspark/spark-standalone-cluster-on-docker/build/workspace/mlruns/0/meta.yaml' does not exist.\n",
      "\n",
      "22/09/24 01:09:43 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onStageCompleted(CustomListener.scala:343)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:35)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:117)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:101)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:105)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:105)\n",
      "\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:100)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:96)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1381)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:96)\n",
      "22/09/24 01:09:43 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onJobEnd(CustomListener.scala:288)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:39)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:117)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:101)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:105)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:105)\n",
      "\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:100)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:96)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1381)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:96)\n",
      "trial task 15 failed, exception is Yaml file '/home/debankurs/Desktop/learningspark/spark-standalone-cluster-on-docker/build/workspace/mlruns/0/meta.yaml' does not exist..\n",
      " Traceback (most recent call last):\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/hyperopt/spark.py\", line 467, in run_task_on_executor\n",
      "    result = domain.evaluate(\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/hyperopt/base.py\", line 892, in evaluate\n",
      "    rval = self.fn(pyll_rval)\n",
      "  File \"/tmp/ipykernel_229044/4099234749.py\", line 11, in train_model\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/fluent.py\", line 353, in start_run\n",
      "    active_run_obj = client.create_run(experiment_id=exp_id_for_run, tags=resolved_tags)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/client.py\", line 265, in create_run\n",
      "    return self._tracking_client.create_run(experiment_id, start_time, tags)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/_tracking_service/client.py\", line 107, in create_run\n",
      "    return self.store.create_run(\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 583, in create_run\n",
      "    experiment = self.get_experiment(experiment_id)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 430, in get_experiment\n",
      "    experiment = self._get_experiment(experiment_id)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 403, in _get_experiment\n",
      "    meta = FileStore._read_yaml(experiment_dir, FileStore.META_DATA_FILE_NAME)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 1075, in _read_yaml\n",
      "    return _read_helper(root, file_name, attempts_remaining=retries)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 1068, in _read_helper\n",
      "    result = read_yaml(root, file_name)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/utils/file_utils.py\", line 181, in read_yaml\n",
      "    raise MissingConfigException(\"Yaml file '%s' does not exist.\" % file_path)\n",
      "mlflow.exceptions.MissingConfigException: Yaml file '/home/debankurs/Desktop/learningspark/spark-standalone-cluster-on-docker/build/workspace/mlruns/0/meta.yaml' does not exist.\n",
      "\n",
      "22/09/24 01:09:44 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.$anonfun$onStageStatusActive$2(CustomListener.scala:402)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.$anonfun$onStageStatusActive$2$adapted(CustomListener.scala:385)\n",
      "\tat scala.collection.TraversableLike$WithFilter.$anonfun$foreach$1(TraversableLike.scala:877)\n",
      "\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)\n",
      "\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)\n",
      "\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:149)\n",
      "\tat scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:876)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onStageStatusActive(CustomListener.scala:385)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener$$anon$1.run(CustomListener.scala:71)\n",
      "\tat java.base/java.util.TimerThread.mainLoop(Timer.java:556)\n",
      "\tat java.base/java.util.TimerThread.run(Timer.java:506)\n",
      "22/09/24 01:09:44 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onStageStatusActive(CustomListener.scala:409)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener$$anon$1.run(CustomListener.scala:71)\n",
      "\tat java.base/java.util.TimerThread.mainLoop(Timer.java:556)\n",
      "\tat java.base/java.util.TimerThread.run(Timer.java:506)\n",
      "22/09/24 01:09:44 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onStageStatusActive(CustomListener.scala:409)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener$$anon$1.run(CustomListener.scala:71)\n",
      "\tat java.base/java.util.TimerThread.mainLoop(Timer.java:556)\n",
      "\tat java.base/java.util.TimerThread.run(Timer.java:506)\n",
      "2022/09/24 01:09:44 INFO mlflow.tracking.fluent: Autologging successfully enabled for sklearn.\n",
      "2022/09/24 01:09:44 INFO mlflow.tracking.fluent: Autologging successfully enabled for pyspark.\n",
      "2022/09/24 01:09:44 INFO mlflow.pyspark.ml: No SparkSession detected. Autologging will log pyspark.ml models contained in the default allowlist. To specify a custom allowlist, initialize a SparkSession prior to calling mlflow.pyspark.ml.autolog() and specify the path to your allowlist file via the spark.mlflow.pysparkml.autolog.logModelAllowlistFile conf.\n",
      "2022/09/24 01:09:44 INFO mlflow.tracking.fluent: Autologging successfully enabled for pyspark.ml.\n",
      "22/09/24 01:09:44 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onJobStart(CustomListener.scala:236)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:37)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:117)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:101)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:105)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:105)\n",
      "\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:100)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:96)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1381)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:96)\n",
      "22/09/24 01:09:44 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onStageSubmitted(CustomListener.scala:379)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:33)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:117)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:101)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:105)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:105)\n",
      "\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:100)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:96)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1381)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:96)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/hyperopt/spark.py\", line 467, in run_task_on_executor\n",
      "    result = domain.evaluate(\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/hyperopt/base.py\", line 892, in evaluate\n",
      "    rval = self.fn(pyll_rval)\n",
      "  File \"/tmp/ipykernel_229044/4099234749.py\", line 11, in train_model\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/fluent.py\", line 353, in start_run\n",
      "    active_run_obj = client.create_run(experiment_id=exp_id_for_run, tags=resolved_tags)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/client.py\", line 265, in create_run\n",
      "    return self._tracking_client.create_run(experiment_id, start_time, tags)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/_tracking_service/client.py\", line 107, in create_run\n",
      "    return self.store.create_run(\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 583, in create_run\n",
      "    experiment = self.get_experiment(experiment_id)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 430, in get_experiment\n",
      "    experiment = self._get_experiment(experiment_id)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 403, in _get_experiment\n",
      "    meta = FileStore._read_yaml(experiment_dir, FileStore.META_DATA_FILE_NAME)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 1075, in _read_yaml\n",
      "    return _read_helper(root, file_name, attempts_remaining=retries)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 1068, in _read_helper\n",
      "    result = read_yaml(root, file_name)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/utils/file_utils.py\", line 181, in read_yaml\n",
      "    raise MissingConfigException(\"Yaml file '%s' does not exist.\" % file_path)\n",
      "mlflow.exceptions.MissingConfigException: Yaml file '/home/debankurs/Desktop/learningspark/spark-standalone-cluster-on-docker/build/workspace/mlruns/0/meta.yaml' does not exist.\n",
      "\n",
      "22/09/24 01:09:44 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onStageCompleted(CustomListener.scala:343)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:35)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:117)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:101)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:105)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:105)\n",
      "\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:100)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:96)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1381)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:96)\n",
      "22/09/24 01:09:44 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onJobEnd(CustomListener.scala:288)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:39)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:117)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:101)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:105)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:105)\n",
      "\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:100)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:96)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1381)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:96)\n",
      "trial task 16 failed, exception is Yaml file '/home/debankurs/Desktop/learningspark/spark-standalone-cluster-on-docker/build/workspace/mlruns/0/meta.yaml' does not exist..\n",
      " Traceback (most recent call last):\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/hyperopt/spark.py\", line 467, in run_task_on_executor\n",
      "    result = domain.evaluate(\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/hyperopt/base.py\", line 892, in evaluate\n",
      "    rval = self.fn(pyll_rval)\n",
      "  File \"/tmp/ipykernel_229044/4099234749.py\", line 11, in train_model\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/fluent.py\", line 353, in start_run\n",
      "    active_run_obj = client.create_run(experiment_id=exp_id_for_run, tags=resolved_tags)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/client.py\", line 265, in create_run\n",
      "    return self._tracking_client.create_run(experiment_id, start_time, tags)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/_tracking_service/client.py\", line 107, in create_run\n",
      "    return self.store.create_run(\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 583, in create_run\n",
      "    experiment = self.get_experiment(experiment_id)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 430, in get_experiment\n",
      "    experiment = self._get_experiment(experiment_id)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 403, in _get_experiment\n",
      "    meta = FileStore._read_yaml(experiment_dir, FileStore.META_DATA_FILE_NAME)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 1075, in _read_yaml\n",
      "    return _read_helper(root, file_name, attempts_remaining=retries)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 1068, in _read_helper\n",
      "    result = read_yaml(root, file_name)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/utils/file_utils.py\", line 181, in read_yaml\n",
      "    raise MissingConfigException(\"Yaml file '%s' does not exist.\" % file_path)\n",
      "mlflow.exceptions.MissingConfigException: Yaml file '/home/debankurs/Desktop/learningspark/spark-standalone-cluster-on-docker/build/workspace/mlruns/0/meta.yaml' does not exist.\n",
      "\n",
      "22/09/24 01:09:45 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.$anonfun$onStageStatusActive$2(CustomListener.scala:402)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.$anonfun$onStageStatusActive$2$adapted(CustomListener.scala:385)\n",
      "\tat scala.collection.TraversableLike$WithFilter.$anonfun$foreach$1(TraversableLike.scala:877)\n",
      "\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)\n",
      "\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)\n",
      "\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:149)\n",
      "\tat scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:876)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onStageStatusActive(CustomListener.scala:385)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener$$anon$1.run(CustomListener.scala:71)\n",
      "\tat java.base/java.util.TimerThread.mainLoop(Timer.java:556)\n",
      "\tat java.base/java.util.TimerThread.run(Timer.java:506)\n",
      "22/09/24 01:09:45 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onStageStatusActive(CustomListener.scala:409)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener$$anon$1.run(CustomListener.scala:71)\n",
      "\tat java.base/java.util.TimerThread.mainLoop(Timer.java:556)\n",
      "\tat java.base/java.util.TimerThread.run(Timer.java:506)\n",
      "22/09/24 01:09:45 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onStageStatusActive(CustomListener.scala:409)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener$$anon$1.run(CustomListener.scala:71)\n",
      "\tat java.base/java.util.TimerThread.mainLoop(Timer.java:556)\n",
      "\tat java.base/java.util.TimerThread.run(Timer.java:506)\n",
      "2022/09/24 01:09:45 INFO mlflow.tracking.fluent: Autologging successfully enabled for sklearn.\n",
      "2022/09/24 01:09:45 INFO mlflow.tracking.fluent: Autologging successfully enabled for pyspark.\n",
      "2022/09/24 01:09:45 INFO mlflow.pyspark.ml: No SparkSession detected. Autologging will log pyspark.ml models contained in the default allowlist. To specify a custom allowlist, initialize a SparkSession prior to calling mlflow.pyspark.ml.autolog() and specify the path to your allowlist file via the spark.mlflow.pysparkml.autolog.logModelAllowlistFile conf.\n",
      "2022/09/24 01:09:45 INFO mlflow.tracking.fluent: Autologging successfully enabled for pyspark.ml.\n",
      "22/09/24 01:09:45 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onJobStart(CustomListener.scala:236)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:37)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:117)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:101)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:105)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:105)\n",
      "\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:100)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:96)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1381)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:96)\n",
      "22/09/24 01:09:45 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onStageSubmitted(CustomListener.scala:379)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:33)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:117)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:101)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:105)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:105)\n",
      "\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:100)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:96)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1381)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:96)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/hyperopt/spark.py\", line 467, in run_task_on_executor\n",
      "    result = domain.evaluate(\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/hyperopt/base.py\", line 892, in evaluate\n",
      "    rval = self.fn(pyll_rval)\n",
      "  File \"/tmp/ipykernel_229044/4099234749.py\", line 11, in train_model\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/fluent.py\", line 353, in start_run\n",
      "    active_run_obj = client.create_run(experiment_id=exp_id_for_run, tags=resolved_tags)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/client.py\", line 265, in create_run\n",
      "    return self._tracking_client.create_run(experiment_id, start_time, tags)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/_tracking_service/client.py\", line 107, in create_run\n",
      "    return self.store.create_run(\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 583, in create_run\n",
      "    experiment = self.get_experiment(experiment_id)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 430, in get_experiment\n",
      "    experiment = self._get_experiment(experiment_id)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 403, in _get_experiment\n",
      "    meta = FileStore._read_yaml(experiment_dir, FileStore.META_DATA_FILE_NAME)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 1075, in _read_yaml\n",
      "    return _read_helper(root, file_name, attempts_remaining=retries)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 1068, in _read_helper\n",
      "    result = read_yaml(root, file_name)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/utils/file_utils.py\", line 181, in read_yaml\n",
      "    raise MissingConfigException(\"Yaml file '%s' does not exist.\" % file_path)\n",
      "mlflow.exceptions.MissingConfigException: Yaml file '/home/debankurs/Desktop/learningspark/spark-standalone-cluster-on-docker/build/workspace/mlruns/0/meta.yaml' does not exist.\n",
      "\n",
      "22/09/24 01:09:45 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onStageCompleted(CustomListener.scala:343)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:35)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:117)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:101)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:105)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:105)\n",
      "\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:100)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:96)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1381)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:96)\n",
      "22/09/24 01:09:45 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onJobEnd(CustomListener.scala:288)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:39)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:117)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:101)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:105)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:105)\n",
      "\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:100)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:96)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1381)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:96)\n",
      "trial task 17 failed, exception is Yaml file '/home/debankurs/Desktop/learningspark/spark-standalone-cluster-on-docker/build/workspace/mlruns/0/meta.yaml' does not exist..\n",
      " Traceback (most recent call last):\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/hyperopt/spark.py\", line 467, in run_task_on_executor\n",
      "    result = domain.evaluate(\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/hyperopt/base.py\", line 892, in evaluate\n",
      "    rval = self.fn(pyll_rval)\n",
      "  File \"/tmp/ipykernel_229044/4099234749.py\", line 11, in train_model\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/fluent.py\", line 353, in start_run\n",
      "    active_run_obj = client.create_run(experiment_id=exp_id_for_run, tags=resolved_tags)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/client.py\", line 265, in create_run\n",
      "    return self._tracking_client.create_run(experiment_id, start_time, tags)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/_tracking_service/client.py\", line 107, in create_run\n",
      "    return self.store.create_run(\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 583, in create_run\n",
      "    experiment = self.get_experiment(experiment_id)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 430, in get_experiment\n",
      "    experiment = self._get_experiment(experiment_id)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 403, in _get_experiment\n",
      "    meta = FileStore._read_yaml(experiment_dir, FileStore.META_DATA_FILE_NAME)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 1075, in _read_yaml\n",
      "    return _read_helper(root, file_name, attempts_remaining=retries)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 1068, in _read_helper\n",
      "    result = read_yaml(root, file_name)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/utils/file_utils.py\", line 181, in read_yaml\n",
      "    raise MissingConfigException(\"Yaml file '%s' does not exist.\" % file_path)\n",
      "mlflow.exceptions.MissingConfigException: Yaml file '/home/debankurs/Desktop/learningspark/spark-standalone-cluster-on-docker/build/workspace/mlruns/0/meta.yaml' does not exist.\n",
      "\n",
      "22/09/24 01:09:46 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.$anonfun$onStageStatusActive$2(CustomListener.scala:402)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.$anonfun$onStageStatusActive$2$adapted(CustomListener.scala:385)\n",
      "\tat scala.collection.TraversableLike$WithFilter.$anonfun$foreach$1(TraversableLike.scala:877)\n",
      "\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)\n",
      "\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)\n",
      "\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:149)\n",
      "\tat scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:876)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onStageStatusActive(CustomListener.scala:385)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener$$anon$1.run(CustomListener.scala:71)\n",
      "\tat java.base/java.util.TimerThread.mainLoop(Timer.java:556)\n",
      "\tat java.base/java.util.TimerThread.run(Timer.java:506)\n",
      "22/09/24 01:09:46 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onStageStatusActive(CustomListener.scala:409)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener$$anon$1.run(CustomListener.scala:71)\n",
      "\tat java.base/java.util.TimerThread.mainLoop(Timer.java:556)\n",
      "\tat java.base/java.util.TimerThread.run(Timer.java:506)\n",
      "22/09/24 01:09:46 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onStageStatusActive(CustomListener.scala:409)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener$$anon$1.run(CustomListener.scala:71)\n",
      "\tat java.base/java.util.TimerThread.mainLoop(Timer.java:556)\n",
      "\tat java.base/java.util.TimerThread.run(Timer.java:506)\n",
      "2022/09/24 01:09:46 INFO mlflow.tracking.fluent: Autologging successfully enabled for sklearn.\n",
      "2022/09/24 01:09:46 INFO mlflow.tracking.fluent: Autologging successfully enabled for pyspark.\n",
      "2022/09/24 01:09:46 INFO mlflow.pyspark.ml: No SparkSession detected. Autologging will log pyspark.ml models contained in the default allowlist. To specify a custom allowlist, initialize a SparkSession prior to calling mlflow.pyspark.ml.autolog() and specify the path to your allowlist file via the spark.mlflow.pysparkml.autolog.logModelAllowlistFile conf.\n",
      "2022/09/24 01:09:46 INFO mlflow.tracking.fluent: Autologging successfully enabled for pyspark.ml.\n",
      "22/09/24 01:09:46 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onJobStart(CustomListener.scala:236)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:37)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:117)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:101)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:105)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:105)\n",
      "\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:100)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:96)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1381)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:96)\n",
      "22/09/24 01:09:46 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onStageSubmitted(CustomListener.scala:379)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:33)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:117)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:101)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:105)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:105)\n",
      "\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:100)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:96)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1381)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:96)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/hyperopt/spark.py\", line 467, in run_task_on_executor\n",
      "    result = domain.evaluate(\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/hyperopt/base.py\", line 892, in evaluate\n",
      "    rval = self.fn(pyll_rval)\n",
      "  File \"/tmp/ipykernel_229044/4099234749.py\", line 11, in train_model\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/fluent.py\", line 353, in start_run\n",
      "    active_run_obj = client.create_run(experiment_id=exp_id_for_run, tags=resolved_tags)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/client.py\", line 265, in create_run\n",
      "    return self._tracking_client.create_run(experiment_id, start_time, tags)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/_tracking_service/client.py\", line 107, in create_run\n",
      "    return self.store.create_run(\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 583, in create_run\n",
      "    experiment = self.get_experiment(experiment_id)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 430, in get_experiment\n",
      "    experiment = self._get_experiment(experiment_id)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 403, in _get_experiment\n",
      "    meta = FileStore._read_yaml(experiment_dir, FileStore.META_DATA_FILE_NAME)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 1075, in _read_yaml\n",
      "    return _read_helper(root, file_name, attempts_remaining=retries)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 1068, in _read_helper\n",
      "    result = read_yaml(root, file_name)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/utils/file_utils.py\", line 181, in read_yaml\n",
      "    raise MissingConfigException(\"Yaml file '%s' does not exist.\" % file_path)\n",
      "mlflow.exceptions.MissingConfigException: Yaml file '/home/debankurs/Desktop/learningspark/spark-standalone-cluster-on-docker/build/workspace/mlruns/0/meta.yaml' does not exist.\n",
      "\n",
      "22/09/24 01:09:46 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onStageCompleted(CustomListener.scala:343)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:35)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:117)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:101)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:105)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:105)\n",
      "\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:100)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:96)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1381)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:96)\n",
      "22/09/24 01:09:46 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onJobEnd(CustomListener.scala:288)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:39)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:117)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:101)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:105)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:105)\n",
      "\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:100)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:96)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1381)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:96)\n",
      "trial task 18 failed, exception is Yaml file '/home/debankurs/Desktop/learningspark/spark-standalone-cluster-on-docker/build/workspace/mlruns/0/meta.yaml' does not exist..\n",
      " Traceback (most recent call last):\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/hyperopt/spark.py\", line 467, in run_task_on_executor\n",
      "    result = domain.evaluate(\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/hyperopt/base.py\", line 892, in evaluate\n",
      "    rval = self.fn(pyll_rval)\n",
      "  File \"/tmp/ipykernel_229044/4099234749.py\", line 11, in train_model\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/fluent.py\", line 353, in start_run\n",
      "    active_run_obj = client.create_run(experiment_id=exp_id_for_run, tags=resolved_tags)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/client.py\", line 265, in create_run\n",
      "    return self._tracking_client.create_run(experiment_id, start_time, tags)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/_tracking_service/client.py\", line 107, in create_run\n",
      "    return self.store.create_run(\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 583, in create_run\n",
      "    experiment = self.get_experiment(experiment_id)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 430, in get_experiment\n",
      "    experiment = self._get_experiment(experiment_id)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 403, in _get_experiment\n",
      "    meta = FileStore._read_yaml(experiment_dir, FileStore.META_DATA_FILE_NAME)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 1075, in _read_yaml\n",
      "    return _read_helper(root, file_name, attempts_remaining=retries)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 1068, in _read_helper\n",
      "    result = read_yaml(root, file_name)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/utils/file_utils.py\", line 181, in read_yaml\n",
      "    raise MissingConfigException(\"Yaml file '%s' does not exist.\" % file_path)\n",
      "mlflow.exceptions.MissingConfigException: Yaml file '/home/debankurs/Desktop/learningspark/spark-standalone-cluster-on-docker/build/workspace/mlruns/0/meta.yaml' does not exist.\n",
      "\n",
      "22/09/24 01:09:47 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.$anonfun$onStageStatusActive$2(CustomListener.scala:402)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.$anonfun$onStageStatusActive$2$adapted(CustomListener.scala:385)\n",
      "\tat scala.collection.TraversableLike$WithFilter.$anonfun$foreach$1(TraversableLike.scala:877)\n",
      "\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)\n",
      "\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)\n",
      "\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:149)\n",
      "\tat scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:876)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onStageStatusActive(CustomListener.scala:385)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener$$anon$1.run(CustomListener.scala:71)\n",
      "\tat java.base/java.util.TimerThread.mainLoop(Timer.java:556)\n",
      "\tat java.base/java.util.TimerThread.run(Timer.java:506)\n",
      "22/09/24 01:09:47 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onStageStatusActive(CustomListener.scala:409)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener$$anon$1.run(CustomListener.scala:71)\n",
      "\tat java.base/java.util.TimerThread.mainLoop(Timer.java:556)\n",
      "\tat java.base/java.util.TimerThread.run(Timer.java:506)\n",
      "22/09/24 01:09:47 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onStageStatusActive(CustomListener.scala:409)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener$$anon$1.run(CustomListener.scala:71)\n",
      "\tat java.base/java.util.TimerThread.mainLoop(Timer.java:556)\n",
      "\tat java.base/java.util.TimerThread.run(Timer.java:506)\n",
      "2022/09/24 01:09:47 INFO mlflow.tracking.fluent: Autologging successfully enabled for sklearn.\n",
      "2022/09/24 01:09:47 INFO mlflow.tracking.fluent: Autologging successfully enabled for pyspark.\n",
      "2022/09/24 01:09:47 INFO mlflow.pyspark.ml: No SparkSession detected. Autologging will log pyspark.ml models contained in the default allowlist. To specify a custom allowlist, initialize a SparkSession prior to calling mlflow.pyspark.ml.autolog() and specify the path to your allowlist file via the spark.mlflow.pysparkml.autolog.logModelAllowlistFile conf.\n",
      "2022/09/24 01:09:47 INFO mlflow.tracking.fluent: Autologging successfully enabled for pyspark.ml.\n",
      "22/09/24 01:09:47 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onJobStart(CustomListener.scala:236)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:37)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:117)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:101)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:105)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:105)\n",
      "\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:100)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:96)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1381)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:96)\n",
      "22/09/24 01:09:47 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onStageSubmitted(CustomListener.scala:379)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:33)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:117)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:101)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:105)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:105)\n",
      "\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:100)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:96)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1381)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:96)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/hyperopt/spark.py\", line 467, in run_task_on_executor\n",
      "    result = domain.evaluate(\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/hyperopt/base.py\", line 892, in evaluate\n",
      "    rval = self.fn(pyll_rval)\n",
      "  File \"/tmp/ipykernel_229044/4099234749.py\", line 11, in train_model\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/fluent.py\", line 353, in start_run\n",
      "    active_run_obj = client.create_run(experiment_id=exp_id_for_run, tags=resolved_tags)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/client.py\", line 265, in create_run\n",
      "    return self._tracking_client.create_run(experiment_id, start_time, tags)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/_tracking_service/client.py\", line 107, in create_run\n",
      "    return self.store.create_run(\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 583, in create_run\n",
      "    experiment = self.get_experiment(experiment_id)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 430, in get_experiment\n",
      "    experiment = self._get_experiment(experiment_id)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 403, in _get_experiment\n",
      "    meta = FileStore._read_yaml(experiment_dir, FileStore.META_DATA_FILE_NAME)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 1075, in _read_yaml\n",
      "    return _read_helper(root, file_name, attempts_remaining=retries)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 1068, in _read_helper\n",
      "    result = read_yaml(root, file_name)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/utils/file_utils.py\", line 181, in read_yaml\n",
      "    raise MissingConfigException(\"Yaml file '%s' does not exist.\" % file_path)\n",
      "mlflow.exceptions.MissingConfigException: Yaml file '/home/debankurs/Desktop/learningspark/spark-standalone-cluster-on-docker/build/workspace/mlruns/0/meta.yaml' does not exist.\n",
      "\n",
      "22/09/24 01:09:47 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onStageCompleted(CustomListener.scala:343)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:35)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:117)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:101)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:105)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:105)\n",
      "\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:100)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:96)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1381)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:96)\n",
      "22/09/24 01:09:47 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onJobEnd(CustomListener.scala:288)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:39)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:117)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:101)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:105)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:105)\n",
      "\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:100)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:96)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1381)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:96)\n",
      "trial task 19 failed, exception is Yaml file '/home/debankurs/Desktop/learningspark/spark-standalone-cluster-on-docker/build/workspace/mlruns/0/meta.yaml' does not exist..\n",
      " Traceback (most recent call last):\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/hyperopt/spark.py\", line 467, in run_task_on_executor\n",
      "    result = domain.evaluate(\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/hyperopt/base.py\", line 892, in evaluate\n",
      "    rval = self.fn(pyll_rval)\n",
      "  File \"/tmp/ipykernel_229044/4099234749.py\", line 11, in train_model\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/fluent.py\", line 353, in start_run\n",
      "    active_run_obj = client.create_run(experiment_id=exp_id_for_run, tags=resolved_tags)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/client.py\", line 265, in create_run\n",
      "    return self._tracking_client.create_run(experiment_id, start_time, tags)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/_tracking_service/client.py\", line 107, in create_run\n",
      "    return self.store.create_run(\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 583, in create_run\n",
      "    experiment = self.get_experiment(experiment_id)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 430, in get_experiment\n",
      "    experiment = self._get_experiment(experiment_id)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 403, in _get_experiment\n",
      "    meta = FileStore._read_yaml(experiment_dir, FileStore.META_DATA_FILE_NAME)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 1075, in _read_yaml\n",
      "    return _read_helper(root, file_name, attempts_remaining=retries)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 1068, in _read_helper\n",
      "    result = read_yaml(root, file_name)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/utils/file_utils.py\", line 181, in read_yaml\n",
      "    raise MissingConfigException(\"Yaml file '%s' does not exist.\" % file_path)\n",
      "mlflow.exceptions.MissingConfigException: Yaml file '/home/debankurs/Desktop/learningspark/spark-standalone-cluster-on-docker/build/workspace/mlruns/0/meta.yaml' does not exist.\n",
      "\n",
      "22/09/24 01:09:48 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.$anonfun$onStageStatusActive$2(CustomListener.scala:402)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.$anonfun$onStageStatusActive$2$adapted(CustomListener.scala:385)\n",
      "\tat scala.collection.TraversableLike$WithFilter.$anonfun$foreach$1(TraversableLike.scala:877)\n",
      "\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)\n",
      "\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)\n",
      "\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:149)\n",
      "\tat scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:876)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onStageStatusActive(CustomListener.scala:385)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener$$anon$1.run(CustomListener.scala:71)\n",
      "\tat java.base/java.util.TimerThread.mainLoop(Timer.java:556)\n",
      "\tat java.base/java.util.TimerThread.run(Timer.java:506)\n",
      "22/09/24 01:09:48 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onStageStatusActive(CustomListener.scala:409)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener$$anon$1.run(CustomListener.scala:71)\n",
      "\tat java.base/java.util.TimerThread.mainLoop(Timer.java:556)\n",
      "\tat java.base/java.util.TimerThread.run(Timer.java:506)\n",
      "22/09/24 01:09:48 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onStageStatusActive(CustomListener.scala:409)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener$$anon$1.run(CustomListener.scala:71)\n",
      "\tat java.base/java.util.TimerThread.mainLoop(Timer.java:556)\n",
      "\tat java.base/java.util.TimerThread.run(Timer.java:506)\n",
      "2022/09/24 01:09:48 INFO mlflow.tracking.fluent: Autologging successfully enabled for sklearn.\n",
      "2022/09/24 01:09:48 INFO mlflow.tracking.fluent: Autologging successfully enabled for pyspark.\n",
      "2022/09/24 01:09:48 INFO mlflow.pyspark.ml: No SparkSession detected. Autologging will log pyspark.ml models contained in the default allowlist. To specify a custom allowlist, initialize a SparkSession prior to calling mlflow.pyspark.ml.autolog() and specify the path to your allowlist file via the spark.mlflow.pysparkml.autolog.logModelAllowlistFile conf.\n",
      "2022/09/24 01:09:48 INFO mlflow.tracking.fluent: Autologging successfully enabled for pyspark.ml.\n",
      "22/09/24 01:09:48 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onJobStart(CustomListener.scala:236)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:37)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:117)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:101)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:105)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:105)\n",
      "\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:100)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:96)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1381)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:96)\n",
      "22/09/24 01:09:48 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onStageSubmitted(CustomListener.scala:379)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:33)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:117)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:101)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:105)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:105)\n",
      "\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:100)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:96)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1381)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:96)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/hyperopt/spark.py\", line 467, in run_task_on_executor\n",
      "    result = domain.evaluate(\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/hyperopt/base.py\", line 892, in evaluate\n",
      "    rval = self.fn(pyll_rval)\n",
      "  File \"/tmp/ipykernel_229044/4099234749.py\", line 11, in train_model\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/fluent.py\", line 353, in start_run\n",
      "    active_run_obj = client.create_run(experiment_id=exp_id_for_run, tags=resolved_tags)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/client.py\", line 265, in create_run\n",
      "    return self._tracking_client.create_run(experiment_id, start_time, tags)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/_tracking_service/client.py\", line 107, in create_run\n",
      "    return self.store.create_run(\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 583, in create_run\n",
      "    experiment = self.get_experiment(experiment_id)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 430, in get_experiment\n",
      "    experiment = self._get_experiment(experiment_id)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 403, in _get_experiment\n",
      "    meta = FileStore._read_yaml(experiment_dir, FileStore.META_DATA_FILE_NAME)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 1075, in _read_yaml\n",
      "    return _read_helper(root, file_name, attempts_remaining=retries)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 1068, in _read_helper\n",
      "    result = read_yaml(root, file_name)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/utils/file_utils.py\", line 181, in read_yaml\n",
      "    raise MissingConfigException(\"Yaml file '%s' does not exist.\" % file_path)\n",
      "mlflow.exceptions.MissingConfigException: Yaml file '/home/debankurs/Desktop/learningspark/spark-standalone-cluster-on-docker/build/workspace/mlruns/0/meta.yaml' does not exist.\n",
      "\n",
      "22/09/24 01:09:48 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onStageCompleted(CustomListener.scala:343)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:35)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:117)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:101)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:105)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:105)\n",
      "\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:100)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:96)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1381)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:96)\n",
      "22/09/24 01:09:48 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onJobEnd(CustomListener.scala:288)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:39)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:117)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:101)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:105)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:105)\n",
      "\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:100)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:96)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1381)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:96)\n",
      "trial task 20 failed, exception is Yaml file '/home/debankurs/Desktop/learningspark/spark-standalone-cluster-on-docker/build/workspace/mlruns/0/meta.yaml' does not exist..\n",
      " Traceback (most recent call last):\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/hyperopt/spark.py\", line 467, in run_task_on_executor\n",
      "    result = domain.evaluate(\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/hyperopt/base.py\", line 892, in evaluate\n",
      "    rval = self.fn(pyll_rval)\n",
      "  File \"/tmp/ipykernel_229044/4099234749.py\", line 11, in train_model\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/fluent.py\", line 353, in start_run\n",
      "    active_run_obj = client.create_run(experiment_id=exp_id_for_run, tags=resolved_tags)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/client.py\", line 265, in create_run\n",
      "    return self._tracking_client.create_run(experiment_id, start_time, tags)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/_tracking_service/client.py\", line 107, in create_run\n",
      "    return self.store.create_run(\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 583, in create_run\n",
      "    experiment = self.get_experiment(experiment_id)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 430, in get_experiment\n",
      "    experiment = self._get_experiment(experiment_id)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 403, in _get_experiment\n",
      "    meta = FileStore._read_yaml(experiment_dir, FileStore.META_DATA_FILE_NAME)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 1075, in _read_yaml\n",
      "    return _read_helper(root, file_name, attempts_remaining=retries)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 1068, in _read_helper\n",
      "    result = read_yaml(root, file_name)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/utils/file_utils.py\", line 181, in read_yaml\n",
      "    raise MissingConfigException(\"Yaml file '%s' does not exist.\" % file_path)\n",
      "mlflow.exceptions.MissingConfigException: Yaml file '/home/debankurs/Desktop/learningspark/spark-standalone-cluster-on-docker/build/workspace/mlruns/0/meta.yaml' does not exist.\n",
      "\n",
      "22/09/24 01:09:49 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.$anonfun$onStageStatusActive$2(CustomListener.scala:402)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.$anonfun$onStageStatusActive$2$adapted(CustomListener.scala:385)\n",
      "\tat scala.collection.TraversableLike$WithFilter.$anonfun$foreach$1(TraversableLike.scala:877)\n",
      "\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)\n",
      "\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)\n",
      "\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:149)\n",
      "\tat scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:876)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onStageStatusActive(CustomListener.scala:385)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener$$anon$1.run(CustomListener.scala:71)\n",
      "\tat java.base/java.util.TimerThread.mainLoop(Timer.java:556)\n",
      "\tat java.base/java.util.TimerThread.run(Timer.java:506)\n",
      "22/09/24 01:09:49 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onStageStatusActive(CustomListener.scala:409)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener$$anon$1.run(CustomListener.scala:71)\n",
      "\tat java.base/java.util.TimerThread.mainLoop(Timer.java:556)\n",
      "\tat java.base/java.util.TimerThread.run(Timer.java:506)\n",
      "22/09/24 01:09:49 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onStageStatusActive(CustomListener.scala:409)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener$$anon$1.run(CustomListener.scala:71)\n",
      "\tat java.base/java.util.TimerThread.mainLoop(Timer.java:556)\n",
      "\tat java.base/java.util.TimerThread.run(Timer.java:506)\n",
      "2022/09/24 01:09:49 INFO mlflow.tracking.fluent: Autologging successfully enabled for sklearn.\n",
      "2022/09/24 01:09:49 INFO mlflow.tracking.fluent: Autologging successfully enabled for pyspark.\n",
      "2022/09/24 01:09:49 INFO mlflow.pyspark.ml: No SparkSession detected. Autologging will log pyspark.ml models contained in the default allowlist. To specify a custom allowlist, initialize a SparkSession prior to calling mlflow.pyspark.ml.autolog() and specify the path to your allowlist file via the spark.mlflow.pysparkml.autolog.logModelAllowlistFile conf.\n",
      "2022/09/24 01:09:49 INFO mlflow.tracking.fluent: Autologging successfully enabled for pyspark.ml.\n",
      "22/09/24 01:09:49 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onJobStart(CustomListener.scala:236)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:37)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:117)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:101)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:105)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:105)\n",
      "\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:100)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:96)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1381)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:96)\n",
      "22/09/24 01:09:49 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onStageSubmitted(CustomListener.scala:379)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:33)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:117)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:101)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:105)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:105)\n",
      "\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:100)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:96)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1381)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:96)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/hyperopt/spark.py\", line 467, in run_task_on_executor\n",
      "    result = domain.evaluate(\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/hyperopt/base.py\", line 892, in evaluate\n",
      "    rval = self.fn(pyll_rval)\n",
      "  File \"/tmp/ipykernel_229044/4099234749.py\", line 11, in train_model\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/fluent.py\", line 353, in start_run\n",
      "    active_run_obj = client.create_run(experiment_id=exp_id_for_run, tags=resolved_tags)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/client.py\", line 265, in create_run\n",
      "    return self._tracking_client.create_run(experiment_id, start_time, tags)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/_tracking_service/client.py\", line 107, in create_run\n",
      "    return self.store.create_run(\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 583, in create_run\n",
      "    experiment = self.get_experiment(experiment_id)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 430, in get_experiment\n",
      "    experiment = self._get_experiment(experiment_id)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 403, in _get_experiment\n",
      "    meta = FileStore._read_yaml(experiment_dir, FileStore.META_DATA_FILE_NAME)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 1075, in _read_yaml\n",
      "    return _read_helper(root, file_name, attempts_remaining=retries)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 1068, in _read_helper\n",
      "    result = read_yaml(root, file_name)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/utils/file_utils.py\", line 181, in read_yaml\n",
      "    raise MissingConfigException(\"Yaml file '%s' does not exist.\" % file_path)\n",
      "mlflow.exceptions.MissingConfigException: Yaml file '/home/debankurs/Desktop/learningspark/spark-standalone-cluster-on-docker/build/workspace/mlruns/0/meta.yaml' does not exist.\n",
      "\n",
      "22/09/24 01:09:49 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onStageCompleted(CustomListener.scala:343)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:35)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:117)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:101)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:105)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:105)\n",
      "\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:100)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:96)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1381)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:96)\n",
      "22/09/24 01:09:49 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onJobEnd(CustomListener.scala:288)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:39)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:117)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:101)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:105)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:105)\n",
      "\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:100)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:96)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1381)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:96)\n",
      "trial task 21 failed, exception is Yaml file '/home/debankurs/Desktop/learningspark/spark-standalone-cluster-on-docker/build/workspace/mlruns/0/meta.yaml' does not exist..\n",
      " Traceback (most recent call last):\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/hyperopt/spark.py\", line 467, in run_task_on_executor\n",
      "    result = domain.evaluate(\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/hyperopt/base.py\", line 892, in evaluate\n",
      "    rval = self.fn(pyll_rval)\n",
      "  File \"/tmp/ipykernel_229044/4099234749.py\", line 11, in train_model\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/fluent.py\", line 353, in start_run\n",
      "    active_run_obj = client.create_run(experiment_id=exp_id_for_run, tags=resolved_tags)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/client.py\", line 265, in create_run\n",
      "    return self._tracking_client.create_run(experiment_id, start_time, tags)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/_tracking_service/client.py\", line 107, in create_run\n",
      "    return self.store.create_run(\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 583, in create_run\n",
      "    experiment = self.get_experiment(experiment_id)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 430, in get_experiment\n",
      "    experiment = self._get_experiment(experiment_id)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 403, in _get_experiment\n",
      "    meta = FileStore._read_yaml(experiment_dir, FileStore.META_DATA_FILE_NAME)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 1075, in _read_yaml\n",
      "    return _read_helper(root, file_name, attempts_remaining=retries)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 1068, in _read_helper\n",
      "    result = read_yaml(root, file_name)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/utils/file_utils.py\", line 181, in read_yaml\n",
      "    raise MissingConfigException(\"Yaml file '%s' does not exist.\" % file_path)\n",
      "mlflow.exceptions.MissingConfigException: Yaml file '/home/debankurs/Desktop/learningspark/spark-standalone-cluster-on-docker/build/workspace/mlruns/0/meta.yaml' does not exist.\n",
      "\n",
      "22/09/24 01:09:50 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.$anonfun$onStageStatusActive$2(CustomListener.scala:402)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.$anonfun$onStageStatusActive$2$adapted(CustomListener.scala:385)\n",
      "\tat scala.collection.TraversableLike$WithFilter.$anonfun$foreach$1(TraversableLike.scala:877)\n",
      "\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)\n",
      "\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)\n",
      "\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:149)\n",
      "\tat scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:876)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onStageStatusActive(CustomListener.scala:385)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener$$anon$1.run(CustomListener.scala:71)\n",
      "\tat java.base/java.util.TimerThread.mainLoop(Timer.java:556)\n",
      "\tat java.base/java.util.TimerThread.run(Timer.java:506)\n",
      "22/09/24 01:09:50 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onStageStatusActive(CustomListener.scala:409)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener$$anon$1.run(CustomListener.scala:71)\n",
      "\tat java.base/java.util.TimerThread.mainLoop(Timer.java:556)\n",
      "\tat java.base/java.util.TimerThread.run(Timer.java:506)\n",
      "22/09/24 01:09:50 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onStageStatusActive(CustomListener.scala:409)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener$$anon$1.run(CustomListener.scala:71)\n",
      "\tat java.base/java.util.TimerThread.mainLoop(Timer.java:556)\n",
      "\tat java.base/java.util.TimerThread.run(Timer.java:506)\n",
      "2022/09/24 01:09:50 INFO mlflow.tracking.fluent: Autologging successfully enabled for sklearn.\n",
      "2022/09/24 01:09:50 INFO mlflow.tracking.fluent: Autologging successfully enabled for pyspark.\n",
      "2022/09/24 01:09:50 INFO mlflow.pyspark.ml: No SparkSession detected. Autologging will log pyspark.ml models contained in the default allowlist. To specify a custom allowlist, initialize a SparkSession prior to calling mlflow.pyspark.ml.autolog() and specify the path to your allowlist file via the spark.mlflow.pysparkml.autolog.logModelAllowlistFile conf.\n",
      "2022/09/24 01:09:50 INFO mlflow.tracking.fluent: Autologging successfully enabled for pyspark.ml.\n",
      "22/09/24 01:09:50 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onJobStart(CustomListener.scala:236)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:37)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:117)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:101)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:105)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:105)\n",
      "\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:100)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:96)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1381)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:96)\n",
      "22/09/24 01:09:50 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onStageSubmitted(CustomListener.scala:379)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:33)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:117)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:101)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:105)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:105)\n",
      "\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:100)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:96)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1381)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:96)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/hyperopt/spark.py\", line 467, in run_task_on_executor\n",
      "    result = domain.evaluate(\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/hyperopt/base.py\", line 892, in evaluate\n",
      "    rval = self.fn(pyll_rval)\n",
      "  File \"/tmp/ipykernel_229044/4099234749.py\", line 11, in train_model\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/fluent.py\", line 353, in start_run\n",
      "    active_run_obj = client.create_run(experiment_id=exp_id_for_run, tags=resolved_tags)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/client.py\", line 265, in create_run\n",
      "    return self._tracking_client.create_run(experiment_id, start_time, tags)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/_tracking_service/client.py\", line 107, in create_run\n",
      "    return self.store.create_run(\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 583, in create_run\n",
      "    experiment = self.get_experiment(experiment_id)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 430, in get_experiment\n",
      "    experiment = self._get_experiment(experiment_id)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 403, in _get_experiment\n",
      "    meta = FileStore._read_yaml(experiment_dir, FileStore.META_DATA_FILE_NAME)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 1075, in _read_yaml\n",
      "    return _read_helper(root, file_name, attempts_remaining=retries)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 1068, in _read_helper\n",
      "    result = read_yaml(root, file_name)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/utils/file_utils.py\", line 181, in read_yaml\n",
      "    raise MissingConfigException(\"Yaml file '%s' does not exist.\" % file_path)\n",
      "mlflow.exceptions.MissingConfigException: Yaml file '/home/debankurs/Desktop/learningspark/spark-standalone-cluster-on-docker/build/workspace/mlruns/0/meta.yaml' does not exist.\n",
      "\n",
      "22/09/24 01:09:50 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onStageCompleted(CustomListener.scala:343)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:35)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:117)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:101)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:105)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:105)\n",
      "\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:100)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:96)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1381)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:96)\n",
      "22/09/24 01:09:50 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onJobEnd(CustomListener.scala:288)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:39)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:117)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:101)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:105)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:105)\n",
      "\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:100)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:96)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1381)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:96)\n",
      "trial task 22 failed, exception is Yaml file '/home/debankurs/Desktop/learningspark/spark-standalone-cluster-on-docker/build/workspace/mlruns/0/meta.yaml' does not exist..\n",
      " Traceback (most recent call last):\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/hyperopt/spark.py\", line 467, in run_task_on_executor\n",
      "    result = domain.evaluate(\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/hyperopt/base.py\", line 892, in evaluate\n",
      "    rval = self.fn(pyll_rval)\n",
      "  File \"/tmp/ipykernel_229044/4099234749.py\", line 11, in train_model\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/fluent.py\", line 353, in start_run\n",
      "    active_run_obj = client.create_run(experiment_id=exp_id_for_run, tags=resolved_tags)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/client.py\", line 265, in create_run\n",
      "    return self._tracking_client.create_run(experiment_id, start_time, tags)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/_tracking_service/client.py\", line 107, in create_run\n",
      "    return self.store.create_run(\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 583, in create_run\n",
      "    experiment = self.get_experiment(experiment_id)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 430, in get_experiment\n",
      "    experiment = self._get_experiment(experiment_id)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 403, in _get_experiment\n",
      "    meta = FileStore._read_yaml(experiment_dir, FileStore.META_DATA_FILE_NAME)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 1075, in _read_yaml\n",
      "    return _read_helper(root, file_name, attempts_remaining=retries)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 1068, in _read_helper\n",
      "    result = read_yaml(root, file_name)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/utils/file_utils.py\", line 181, in read_yaml\n",
      "    raise MissingConfigException(\"Yaml file '%s' does not exist.\" % file_path)\n",
      "mlflow.exceptions.MissingConfigException: Yaml file '/home/debankurs/Desktop/learningspark/spark-standalone-cluster-on-docker/build/workspace/mlruns/0/meta.yaml' does not exist.\n",
      "\n",
      "22/09/24 01:09:51 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.$anonfun$onStageStatusActive$2(CustomListener.scala:402)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.$anonfun$onStageStatusActive$2$adapted(CustomListener.scala:385)\n",
      "\tat scala.collection.TraversableLike$WithFilter.$anonfun$foreach$1(TraversableLike.scala:877)\n",
      "\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)\n",
      "\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)\n",
      "\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:149)\n",
      "\tat scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:876)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onStageStatusActive(CustomListener.scala:385)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener$$anon$1.run(CustomListener.scala:71)\n",
      "\tat java.base/java.util.TimerThread.mainLoop(Timer.java:556)\n",
      "\tat java.base/java.util.TimerThread.run(Timer.java:506)\n",
      "22/09/24 01:09:51 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onStageStatusActive(CustomListener.scala:409)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener$$anon$1.run(CustomListener.scala:71)\n",
      "\tat java.base/java.util.TimerThread.mainLoop(Timer.java:556)\n",
      "\tat java.base/java.util.TimerThread.run(Timer.java:506)\n",
      "22/09/24 01:09:51 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onStageStatusActive(CustomListener.scala:409)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener$$anon$1.run(CustomListener.scala:71)\n",
      "\tat java.base/java.util.TimerThread.mainLoop(Timer.java:556)\n",
      "\tat java.base/java.util.TimerThread.run(Timer.java:506)\n",
      "2022/09/24 01:09:51 INFO mlflow.tracking.fluent: Autologging successfully enabled for sklearn.\n",
      "2022/09/24 01:09:51 INFO mlflow.tracking.fluent: Autologging successfully enabled for pyspark.\n",
      "2022/09/24 01:09:51 INFO mlflow.pyspark.ml: No SparkSession detected. Autologging will log pyspark.ml models contained in the default allowlist. To specify a custom allowlist, initialize a SparkSession prior to calling mlflow.pyspark.ml.autolog() and specify the path to your allowlist file via the spark.mlflow.pysparkml.autolog.logModelAllowlistFile conf.\n",
      "2022/09/24 01:09:51 INFO mlflow.tracking.fluent: Autologging successfully enabled for pyspark.ml.\n",
      "22/09/24 01:09:51 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onJobStart(CustomListener.scala:236)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:37)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:117)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:101)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:105)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:105)\n",
      "\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:100)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:96)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1381)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:96)\n",
      "22/09/24 01:09:51 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onStageSubmitted(CustomListener.scala:379)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:33)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:117)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:101)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:105)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:105)\n",
      "\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:100)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:96)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1381)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:96)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/hyperopt/spark.py\", line 467, in run_task_on_executor\n",
      "    result = domain.evaluate(\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/hyperopt/base.py\", line 892, in evaluate\n",
      "    rval = self.fn(pyll_rval)\n",
      "  File \"/tmp/ipykernel_229044/4099234749.py\", line 11, in train_model\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/fluent.py\", line 353, in start_run\n",
      "    active_run_obj = client.create_run(experiment_id=exp_id_for_run, tags=resolved_tags)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/client.py\", line 265, in create_run\n",
      "    return self._tracking_client.create_run(experiment_id, start_time, tags)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/_tracking_service/client.py\", line 107, in create_run\n",
      "    return self.store.create_run(\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 583, in create_run\n",
      "    experiment = self.get_experiment(experiment_id)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 430, in get_experiment\n",
      "    experiment = self._get_experiment(experiment_id)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 403, in _get_experiment\n",
      "    meta = FileStore._read_yaml(experiment_dir, FileStore.META_DATA_FILE_NAME)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 1075, in _read_yaml\n",
      "    return _read_helper(root, file_name, attempts_remaining=retries)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 1068, in _read_helper\n",
      "    result = read_yaml(root, file_name)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/utils/file_utils.py\", line 181, in read_yaml\n",
      "    raise MissingConfigException(\"Yaml file '%s' does not exist.\" % file_path)\n",
      "mlflow.exceptions.MissingConfigException: Yaml file '/home/debankurs/Desktop/learningspark/spark-standalone-cluster-on-docker/build/workspace/mlruns/0/meta.yaml' does not exist.\n",
      "\n",
      "22/09/24 01:09:51 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onStageCompleted(CustomListener.scala:343)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:35)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:117)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:101)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:105)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:105)\n",
      "\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:100)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:96)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1381)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:96)\n",
      "22/09/24 01:09:51 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onJobEnd(CustomListener.scala:288)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:39)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:117)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:101)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:105)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:105)\n",
      "\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:100)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:96)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1381)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:96)\n",
      "trial task 23 failed, exception is Yaml file '/home/debankurs/Desktop/learningspark/spark-standalone-cluster-on-docker/build/workspace/mlruns/0/meta.yaml' does not exist..\n",
      " Traceback (most recent call last):\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/hyperopt/spark.py\", line 467, in run_task_on_executor\n",
      "    result = domain.evaluate(\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/hyperopt/base.py\", line 892, in evaluate\n",
      "    rval = self.fn(pyll_rval)\n",
      "  File \"/tmp/ipykernel_229044/4099234749.py\", line 11, in train_model\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/fluent.py\", line 353, in start_run\n",
      "    active_run_obj = client.create_run(experiment_id=exp_id_for_run, tags=resolved_tags)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/client.py\", line 265, in create_run\n",
      "    return self._tracking_client.create_run(experiment_id, start_time, tags)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/_tracking_service/client.py\", line 107, in create_run\n",
      "    return self.store.create_run(\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 583, in create_run\n",
      "    experiment = self.get_experiment(experiment_id)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 430, in get_experiment\n",
      "    experiment = self._get_experiment(experiment_id)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 403, in _get_experiment\n",
      "    meta = FileStore._read_yaml(experiment_dir, FileStore.META_DATA_FILE_NAME)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 1075, in _read_yaml\n",
      "    return _read_helper(root, file_name, attempts_remaining=retries)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 1068, in _read_helper\n",
      "    result = read_yaml(root, file_name)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/utils/file_utils.py\", line 181, in read_yaml\n",
      "    raise MissingConfigException(\"Yaml file '%s' does not exist.\" % file_path)\n",
      "mlflow.exceptions.MissingConfigException: Yaml file '/home/debankurs/Desktop/learningspark/spark-standalone-cluster-on-docker/build/workspace/mlruns/0/meta.yaml' does not exist.\n",
      "\n",
      "22/09/24 01:09:52 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.$anonfun$onStageStatusActive$2(CustomListener.scala:402)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.$anonfun$onStageStatusActive$2$adapted(CustomListener.scala:385)\n",
      "\tat scala.collection.TraversableLike$WithFilter.$anonfun$foreach$1(TraversableLike.scala:877)\n",
      "\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)\n",
      "\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)\n",
      "\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:149)\n",
      "\tat scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:876)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onStageStatusActive(CustomListener.scala:385)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener$$anon$1.run(CustomListener.scala:71)\n",
      "\tat java.base/java.util.TimerThread.mainLoop(Timer.java:556)\n",
      "\tat java.base/java.util.TimerThread.run(Timer.java:506)\n",
      "22/09/24 01:09:52 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onStageStatusActive(CustomListener.scala:409)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener$$anon$1.run(CustomListener.scala:71)\n",
      "\tat java.base/java.util.TimerThread.mainLoop(Timer.java:556)\n",
      "\tat java.base/java.util.TimerThread.run(Timer.java:506)\n",
      "22/09/24 01:09:52 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onStageStatusActive(CustomListener.scala:409)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener$$anon$1.run(CustomListener.scala:71)\n",
      "\tat java.base/java.util.TimerThread.mainLoop(Timer.java:556)\n",
      "\tat java.base/java.util.TimerThread.run(Timer.java:506)\n",
      "2022/09/24 01:09:52 INFO mlflow.tracking.fluent: Autologging successfully enabled for sklearn.\n",
      "2022/09/24 01:09:52 INFO mlflow.tracking.fluent: Autologging successfully enabled for pyspark.\n",
      "2022/09/24 01:09:52 INFO mlflow.pyspark.ml: No SparkSession detected. Autologging will log pyspark.ml models contained in the default allowlist. To specify a custom allowlist, initialize a SparkSession prior to calling mlflow.pyspark.ml.autolog() and specify the path to your allowlist file via the spark.mlflow.pysparkml.autolog.logModelAllowlistFile conf.\n",
      "2022/09/24 01:09:52 INFO mlflow.tracking.fluent: Autologging successfully enabled for pyspark.ml.\n",
      "22/09/24 01:09:52 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onJobStart(CustomListener.scala:236)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:37)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:117)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:101)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:105)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:105)\n",
      "\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:100)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:96)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1381)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:96)\n",
      "22/09/24 01:09:52 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onStageSubmitted(CustomListener.scala:379)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:33)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:117)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:101)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:105)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:105)\n",
      "\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:100)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:96)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1381)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:96)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/hyperopt/spark.py\", line 467, in run_task_on_executor\n",
      "    result = domain.evaluate(\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/hyperopt/base.py\", line 892, in evaluate\n",
      "    rval = self.fn(pyll_rval)\n",
      "  File \"/tmp/ipykernel_229044/4099234749.py\", line 11, in train_model\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/fluent.py\", line 353, in start_run\n",
      "    active_run_obj = client.create_run(experiment_id=exp_id_for_run, tags=resolved_tags)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/client.py\", line 265, in create_run\n",
      "    return self._tracking_client.create_run(experiment_id, start_time, tags)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/_tracking_service/client.py\", line 107, in create_run\n",
      "    return self.store.create_run(\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 583, in create_run\n",
      "    experiment = self.get_experiment(experiment_id)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 430, in get_experiment\n",
      "    experiment = self._get_experiment(experiment_id)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 403, in _get_experiment\n",
      "    meta = FileStore._read_yaml(experiment_dir, FileStore.META_DATA_FILE_NAME)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 1075, in _read_yaml\n",
      "    return _read_helper(root, file_name, attempts_remaining=retries)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 1068, in _read_helper\n",
      "    result = read_yaml(root, file_name)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/utils/file_utils.py\", line 181, in read_yaml\n",
      "    raise MissingConfigException(\"Yaml file '%s' does not exist.\" % file_path)\n",
      "mlflow.exceptions.MissingConfigException: Yaml file '/home/debankurs/Desktop/learningspark/spark-standalone-cluster-on-docker/build/workspace/mlruns/0/meta.yaml' does not exist.\n",
      "\n",
      "22/09/24 01:09:52 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onStageCompleted(CustomListener.scala:343)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:35)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:117)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:101)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:105)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:105)\n",
      "\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:100)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:96)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1381)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:96)\n",
      "22/09/24 01:09:52 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onJobEnd(CustomListener.scala:288)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:39)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:117)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:101)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:105)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:105)\n",
      "\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:100)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:96)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1381)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:96)\n",
      "trial task 24 failed, exception is Yaml file '/home/debankurs/Desktop/learningspark/spark-standalone-cluster-on-docker/build/workspace/mlruns/0/meta.yaml' does not exist..\n",
      " Traceback (most recent call last):\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/hyperopt/spark.py\", line 467, in run_task_on_executor\n",
      "    result = domain.evaluate(\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/hyperopt/base.py\", line 892, in evaluate\n",
      "    rval = self.fn(pyll_rval)\n",
      "  File \"/tmp/ipykernel_229044/4099234749.py\", line 11, in train_model\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/fluent.py\", line 353, in start_run\n",
      "    active_run_obj = client.create_run(experiment_id=exp_id_for_run, tags=resolved_tags)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/client.py\", line 265, in create_run\n",
      "    return self._tracking_client.create_run(experiment_id, start_time, tags)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/_tracking_service/client.py\", line 107, in create_run\n",
      "    return self.store.create_run(\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 583, in create_run\n",
      "    experiment = self.get_experiment(experiment_id)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 430, in get_experiment\n",
      "    experiment = self._get_experiment(experiment_id)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 403, in _get_experiment\n",
      "    meta = FileStore._read_yaml(experiment_dir, FileStore.META_DATA_FILE_NAME)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 1075, in _read_yaml\n",
      "    return _read_helper(root, file_name, attempts_remaining=retries)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 1068, in _read_helper\n",
      "    result = read_yaml(root, file_name)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/utils/file_utils.py\", line 181, in read_yaml\n",
      "    raise MissingConfigException(\"Yaml file '%s' does not exist.\" % file_path)\n",
      "mlflow.exceptions.MissingConfigException: Yaml file '/home/debankurs/Desktop/learningspark/spark-standalone-cluster-on-docker/build/workspace/mlruns/0/meta.yaml' does not exist.\n",
      "\n",
      "22/09/24 01:09:53 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.$anonfun$onStageStatusActive$2(CustomListener.scala:402)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.$anonfun$onStageStatusActive$2$adapted(CustomListener.scala:385)\n",
      "\tat scala.collection.TraversableLike$WithFilter.$anonfun$foreach$1(TraversableLike.scala:877)\n",
      "\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)\n",
      "\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)\n",
      "\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:149)\n",
      "\tat scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:876)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onStageStatusActive(CustomListener.scala:385)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener$$anon$1.run(CustomListener.scala:71)\n",
      "\tat java.base/java.util.TimerThread.mainLoop(Timer.java:556)\n",
      "\tat java.base/java.util.TimerThread.run(Timer.java:506)\n",
      "22/09/24 01:09:53 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onStageStatusActive(CustomListener.scala:409)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener$$anon$1.run(CustomListener.scala:71)\n",
      "\tat java.base/java.util.TimerThread.mainLoop(Timer.java:556)\n",
      "\tat java.base/java.util.TimerThread.run(Timer.java:506)\n",
      "22/09/24 01:09:53 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onStageStatusActive(CustomListener.scala:409)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener$$anon$1.run(CustomListener.scala:71)\n",
      "\tat java.base/java.util.TimerThread.mainLoop(Timer.java:556)\n",
      "\tat java.base/java.util.TimerThread.run(Timer.java:506)\n",
      "2022/09/24 01:09:53 INFO mlflow.tracking.fluent: Autologging successfully enabled for sklearn.\n",
      "2022/09/24 01:09:53 INFO mlflow.tracking.fluent: Autologging successfully enabled for pyspark.\n",
      "2022/09/24 01:09:53 INFO mlflow.pyspark.ml: No SparkSession detected. Autologging will log pyspark.ml models contained in the default allowlist. To specify a custom allowlist, initialize a SparkSession prior to calling mlflow.pyspark.ml.autolog() and specify the path to your allowlist file via the spark.mlflow.pysparkml.autolog.logModelAllowlistFile conf.\n",
      "2022/09/24 01:09:53 INFO mlflow.tracking.fluent: Autologging successfully enabled for pyspark.ml.\n",
      "22/09/24 01:09:53 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onJobStart(CustomListener.scala:236)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:37)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:117)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:101)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:105)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:105)\n",
      "\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:100)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:96)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1381)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:96)\n",
      "22/09/24 01:09:53 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onStageSubmitted(CustomListener.scala:379)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:33)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:117)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:101)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:105)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:105)\n",
      "\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:100)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:96)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1381)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:96)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/hyperopt/spark.py\", line 467, in run_task_on_executor\n",
      "    result = domain.evaluate(\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/hyperopt/base.py\", line 892, in evaluate\n",
      "    rval = self.fn(pyll_rval)\n",
      "  File \"/tmp/ipykernel_229044/4099234749.py\", line 11, in train_model\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/fluent.py\", line 353, in start_run\n",
      "    active_run_obj = client.create_run(experiment_id=exp_id_for_run, tags=resolved_tags)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/client.py\", line 265, in create_run\n",
      "    return self._tracking_client.create_run(experiment_id, start_time, tags)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/_tracking_service/client.py\", line 107, in create_run\n",
      "    return self.store.create_run(\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 583, in create_run\n",
      "    experiment = self.get_experiment(experiment_id)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 430, in get_experiment\n",
      "    experiment = self._get_experiment(experiment_id)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 403, in _get_experiment\n",
      "    meta = FileStore._read_yaml(experiment_dir, FileStore.META_DATA_FILE_NAME)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 1075, in _read_yaml\n",
      "    return _read_helper(root, file_name, attempts_remaining=retries)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 1068, in _read_helper\n",
      "    result = read_yaml(root, file_name)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/utils/file_utils.py\", line 181, in read_yaml\n",
      "    raise MissingConfigException(\"Yaml file '%s' does not exist.\" % file_path)\n",
      "mlflow.exceptions.MissingConfigException: Yaml file '/home/debankurs/Desktop/learningspark/spark-standalone-cluster-on-docker/build/workspace/mlruns/0/meta.yaml' does not exist.\n",
      "\n",
      "22/09/24 01:09:53 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onStageCompleted(CustomListener.scala:343)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:35)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:117)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:101)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:105)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:105)\n",
      "\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:100)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:96)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1381)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:96)\n",
      "22/09/24 01:09:53 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onJobEnd(CustomListener.scala:288)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:39)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:117)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:101)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:105)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:105)\n",
      "\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:100)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:96)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1381)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:96)\n",
      "trial task 25 failed, exception is Yaml file '/home/debankurs/Desktop/learningspark/spark-standalone-cluster-on-docker/build/workspace/mlruns/0/meta.yaml' does not exist..\n",
      " Traceback (most recent call last):\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/hyperopt/spark.py\", line 467, in run_task_on_executor\n",
      "    result = domain.evaluate(\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/hyperopt/base.py\", line 892, in evaluate\n",
      "    rval = self.fn(pyll_rval)\n",
      "  File \"/tmp/ipykernel_229044/4099234749.py\", line 11, in train_model\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/fluent.py\", line 353, in start_run\n",
      "    active_run_obj = client.create_run(experiment_id=exp_id_for_run, tags=resolved_tags)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/client.py\", line 265, in create_run\n",
      "    return self._tracking_client.create_run(experiment_id, start_time, tags)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/_tracking_service/client.py\", line 107, in create_run\n",
      "    return self.store.create_run(\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 583, in create_run\n",
      "    experiment = self.get_experiment(experiment_id)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 430, in get_experiment\n",
      "    experiment = self._get_experiment(experiment_id)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 403, in _get_experiment\n",
      "    meta = FileStore._read_yaml(experiment_dir, FileStore.META_DATA_FILE_NAME)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 1075, in _read_yaml\n",
      "    return _read_helper(root, file_name, attempts_remaining=retries)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 1068, in _read_helper\n",
      "    result = read_yaml(root, file_name)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/utils/file_utils.py\", line 181, in read_yaml\n",
      "    raise MissingConfigException(\"Yaml file '%s' does not exist.\" % file_path)\n",
      "mlflow.exceptions.MissingConfigException: Yaml file '/home/debankurs/Desktop/learningspark/spark-standalone-cluster-on-docker/build/workspace/mlruns/0/meta.yaml' does not exist.\n",
      "\n",
      "22/09/24 01:09:54 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.$anonfun$onStageStatusActive$2(CustomListener.scala:402)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.$anonfun$onStageStatusActive$2$adapted(CustomListener.scala:385)\n",
      "\tat scala.collection.TraversableLike$WithFilter.$anonfun$foreach$1(TraversableLike.scala:877)\n",
      "\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)\n",
      "\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)\n",
      "\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:149)\n",
      "\tat scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:876)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onStageStatusActive(CustomListener.scala:385)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener$$anon$1.run(CustomListener.scala:71)\n",
      "\tat java.base/java.util.TimerThread.mainLoop(Timer.java:556)\n",
      "\tat java.base/java.util.TimerThread.run(Timer.java:506)\n",
      "22/09/24 01:09:54 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onStageStatusActive(CustomListener.scala:409)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener$$anon$1.run(CustomListener.scala:71)\n",
      "\tat java.base/java.util.TimerThread.mainLoop(Timer.java:556)\n",
      "\tat java.base/java.util.TimerThread.run(Timer.java:506)\n",
      "22/09/24 01:09:54 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onStageStatusActive(CustomListener.scala:409)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener$$anon$1.run(CustomListener.scala:71)\n",
      "\tat java.base/java.util.TimerThread.mainLoop(Timer.java:556)\n",
      "\tat java.base/java.util.TimerThread.run(Timer.java:506)\n",
      "2022/09/24 01:09:54 INFO mlflow.tracking.fluent: Autologging successfully enabled for sklearn.\n",
      "2022/09/24 01:09:54 INFO mlflow.tracking.fluent: Autologging successfully enabled for pyspark.\n",
      "2022/09/24 01:09:54 INFO mlflow.pyspark.ml: No SparkSession detected. Autologging will log pyspark.ml models contained in the default allowlist. To specify a custom allowlist, initialize a SparkSession prior to calling mlflow.pyspark.ml.autolog() and specify the path to your allowlist file via the spark.mlflow.pysparkml.autolog.logModelAllowlistFile conf.\n",
      "2022/09/24 01:09:54 INFO mlflow.tracking.fluent: Autologging successfully enabled for pyspark.ml.\n",
      "22/09/24 01:09:54 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onJobStart(CustomListener.scala:236)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:37)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:117)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:101)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:105)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:105)\n",
      "\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:100)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:96)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1381)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:96)\n",
      "22/09/24 01:09:54 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onStageSubmitted(CustomListener.scala:379)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:33)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:117)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:101)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:105)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:105)\n",
      "\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:100)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:96)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1381)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:96)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/hyperopt/spark.py\", line 467, in run_task_on_executor\n",
      "    result = domain.evaluate(\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/hyperopt/base.py\", line 892, in evaluate\n",
      "    rval = self.fn(pyll_rval)\n",
      "  File \"/tmp/ipykernel_229044/4099234749.py\", line 11, in train_model\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/fluent.py\", line 353, in start_run\n",
      "    active_run_obj = client.create_run(experiment_id=exp_id_for_run, tags=resolved_tags)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/client.py\", line 265, in create_run\n",
      "    return self._tracking_client.create_run(experiment_id, start_time, tags)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/_tracking_service/client.py\", line 107, in create_run\n",
      "    return self.store.create_run(\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 583, in create_run\n",
      "    experiment = self.get_experiment(experiment_id)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 430, in get_experiment\n",
      "    experiment = self._get_experiment(experiment_id)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 403, in _get_experiment\n",
      "    meta = FileStore._read_yaml(experiment_dir, FileStore.META_DATA_FILE_NAME)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 1075, in _read_yaml\n",
      "    return _read_helper(root, file_name, attempts_remaining=retries)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 1068, in _read_helper\n",
      "    result = read_yaml(root, file_name)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/utils/file_utils.py\", line 181, in read_yaml\n",
      "    raise MissingConfigException(\"Yaml file '%s' does not exist.\" % file_path)\n",
      "mlflow.exceptions.MissingConfigException: Yaml file '/home/debankurs/Desktop/learningspark/spark-standalone-cluster-on-docker/build/workspace/mlruns/0/meta.yaml' does not exist.\n",
      "\n",
      "22/09/24 01:09:54 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onStageCompleted(CustomListener.scala:343)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:35)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:117)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:101)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:105)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:105)\n",
      "\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:100)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:96)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1381)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:96)\n",
      "22/09/24 01:09:54 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onJobEnd(CustomListener.scala:288)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:39)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:117)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:101)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:105)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:105)\n",
      "\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:100)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:96)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1381)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:96)\n",
      "trial task 26 failed, exception is Yaml file '/home/debankurs/Desktop/learningspark/spark-standalone-cluster-on-docker/build/workspace/mlruns/0/meta.yaml' does not exist..\n",
      " Traceback (most recent call last):\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/hyperopt/spark.py\", line 467, in run_task_on_executor\n",
      "    result = domain.evaluate(\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/hyperopt/base.py\", line 892, in evaluate\n",
      "    rval = self.fn(pyll_rval)\n",
      "  File \"/tmp/ipykernel_229044/4099234749.py\", line 11, in train_model\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/fluent.py\", line 353, in start_run\n",
      "    active_run_obj = client.create_run(experiment_id=exp_id_for_run, tags=resolved_tags)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/client.py\", line 265, in create_run\n",
      "    return self._tracking_client.create_run(experiment_id, start_time, tags)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/_tracking_service/client.py\", line 107, in create_run\n",
      "    return self.store.create_run(\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 583, in create_run\n",
      "    experiment = self.get_experiment(experiment_id)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 430, in get_experiment\n",
      "    experiment = self._get_experiment(experiment_id)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 403, in _get_experiment\n",
      "    meta = FileStore._read_yaml(experiment_dir, FileStore.META_DATA_FILE_NAME)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 1075, in _read_yaml\n",
      "    return _read_helper(root, file_name, attempts_remaining=retries)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 1068, in _read_helper\n",
      "    result = read_yaml(root, file_name)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/utils/file_utils.py\", line 181, in read_yaml\n",
      "    raise MissingConfigException(\"Yaml file '%s' does not exist.\" % file_path)\n",
      "mlflow.exceptions.MissingConfigException: Yaml file '/home/debankurs/Desktop/learningspark/spark-standalone-cluster-on-docker/build/workspace/mlruns/0/meta.yaml' does not exist.\n",
      "\n",
      "22/09/24 01:09:55 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.$anonfun$onStageStatusActive$2(CustomListener.scala:402)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.$anonfun$onStageStatusActive$2$adapted(CustomListener.scala:385)\n",
      "\tat scala.collection.TraversableLike$WithFilter.$anonfun$foreach$1(TraversableLike.scala:877)\n",
      "\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)\n",
      "\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)\n",
      "\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:149)\n",
      "\tat scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:876)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onStageStatusActive(CustomListener.scala:385)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener$$anon$1.run(CustomListener.scala:71)\n",
      "\tat java.base/java.util.TimerThread.mainLoop(Timer.java:556)\n",
      "\tat java.base/java.util.TimerThread.run(Timer.java:506)\n",
      "22/09/24 01:09:55 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onStageStatusActive(CustomListener.scala:409)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener$$anon$1.run(CustomListener.scala:71)\n",
      "\tat java.base/java.util.TimerThread.mainLoop(Timer.java:556)\n",
      "\tat java.base/java.util.TimerThread.run(Timer.java:506)\n",
      "22/09/24 01:09:55 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onStageStatusActive(CustomListener.scala:409)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener$$anon$1.run(CustomListener.scala:71)\n",
      "\tat java.base/java.util.TimerThread.mainLoop(Timer.java:556)\n",
      "\tat java.base/java.util.TimerThread.run(Timer.java:506)\n",
      "2022/09/24 01:09:55 INFO mlflow.tracking.fluent: Autologging successfully enabled for sklearn.\n",
      "2022/09/24 01:09:55 INFO mlflow.tracking.fluent: Autologging successfully enabled for pyspark.\n",
      "2022/09/24 01:09:55 INFO mlflow.pyspark.ml: No SparkSession detected. Autologging will log pyspark.ml models contained in the default allowlist. To specify a custom allowlist, initialize a SparkSession prior to calling mlflow.pyspark.ml.autolog() and specify the path to your allowlist file via the spark.mlflow.pysparkml.autolog.logModelAllowlistFile conf.\n",
      "2022/09/24 01:09:55 INFO mlflow.tracking.fluent: Autologging successfully enabled for pyspark.ml.\n",
      "22/09/24 01:09:55 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onJobStart(CustomListener.scala:236)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:37)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:117)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:101)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:105)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:105)\n",
      "\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:100)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:96)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1381)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:96)\n",
      "22/09/24 01:09:55 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onStageSubmitted(CustomListener.scala:379)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:33)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:117)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:101)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:105)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:105)\n",
      "\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:100)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:96)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1381)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:96)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/hyperopt/spark.py\", line 467, in run_task_on_executor\n",
      "    result = domain.evaluate(\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/hyperopt/base.py\", line 892, in evaluate\n",
      "    rval = self.fn(pyll_rval)\n",
      "  File \"/tmp/ipykernel_229044/4099234749.py\", line 11, in train_model\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/fluent.py\", line 353, in start_run\n",
      "    active_run_obj = client.create_run(experiment_id=exp_id_for_run, tags=resolved_tags)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/client.py\", line 265, in create_run\n",
      "    return self._tracking_client.create_run(experiment_id, start_time, tags)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/_tracking_service/client.py\", line 107, in create_run\n",
      "    return self.store.create_run(\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 583, in create_run\n",
      "    experiment = self.get_experiment(experiment_id)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 430, in get_experiment\n",
      "    experiment = self._get_experiment(experiment_id)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 403, in _get_experiment\n",
      "    meta = FileStore._read_yaml(experiment_dir, FileStore.META_DATA_FILE_NAME)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 1075, in _read_yaml\n",
      "    return _read_helper(root, file_name, attempts_remaining=retries)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 1068, in _read_helper\n",
      "    result = read_yaml(root, file_name)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/utils/file_utils.py\", line 181, in read_yaml\n",
      "    raise MissingConfigException(\"Yaml file '%s' does not exist.\" % file_path)\n",
      "mlflow.exceptions.MissingConfigException: Yaml file '/home/debankurs/Desktop/learningspark/spark-standalone-cluster-on-docker/build/workspace/mlruns/0/meta.yaml' does not exist.\n",
      "\n",
      "22/09/24 01:09:55 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onStageCompleted(CustomListener.scala:343)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:35)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:117)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:101)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:105)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:105)\n",
      "\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:100)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:96)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1381)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:96)\n",
      "22/09/24 01:09:55 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onJobEnd(CustomListener.scala:288)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:39)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:117)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:101)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:105)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:105)\n",
      "\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:100)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:96)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1381)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:96)\n",
      "trial task 27 failed, exception is Yaml file '/home/debankurs/Desktop/learningspark/spark-standalone-cluster-on-docker/build/workspace/mlruns/0/meta.yaml' does not exist..\n",
      " Traceback (most recent call last):\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/hyperopt/spark.py\", line 467, in run_task_on_executor\n",
      "    result = domain.evaluate(\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/hyperopt/base.py\", line 892, in evaluate\n",
      "    rval = self.fn(pyll_rval)\n",
      "  File \"/tmp/ipykernel_229044/4099234749.py\", line 11, in train_model\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/fluent.py\", line 353, in start_run\n",
      "    active_run_obj = client.create_run(experiment_id=exp_id_for_run, tags=resolved_tags)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/client.py\", line 265, in create_run\n",
      "    return self._tracking_client.create_run(experiment_id, start_time, tags)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/_tracking_service/client.py\", line 107, in create_run\n",
      "    return self.store.create_run(\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 583, in create_run\n",
      "    experiment = self.get_experiment(experiment_id)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 430, in get_experiment\n",
      "    experiment = self._get_experiment(experiment_id)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 403, in _get_experiment\n",
      "    meta = FileStore._read_yaml(experiment_dir, FileStore.META_DATA_FILE_NAME)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 1075, in _read_yaml\n",
      "    return _read_helper(root, file_name, attempts_remaining=retries)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 1068, in _read_helper\n",
      "    result = read_yaml(root, file_name)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/utils/file_utils.py\", line 181, in read_yaml\n",
      "    raise MissingConfigException(\"Yaml file '%s' does not exist.\" % file_path)\n",
      "mlflow.exceptions.MissingConfigException: Yaml file '/home/debankurs/Desktop/learningspark/spark-standalone-cluster-on-docker/build/workspace/mlruns/0/meta.yaml' does not exist.\n",
      "\n",
      "22/09/24 01:09:56 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.$anonfun$onStageStatusActive$2(CustomListener.scala:402)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.$anonfun$onStageStatusActive$2$adapted(CustomListener.scala:385)\n",
      "\tat scala.collection.TraversableLike$WithFilter.$anonfun$foreach$1(TraversableLike.scala:877)\n",
      "\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)\n",
      "\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)\n",
      "\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:149)\n",
      "\tat scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:876)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onStageStatusActive(CustomListener.scala:385)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener$$anon$1.run(CustomListener.scala:71)\n",
      "\tat java.base/java.util.TimerThread.mainLoop(Timer.java:556)\n",
      "\tat java.base/java.util.TimerThread.run(Timer.java:506)\n",
      "22/09/24 01:09:56 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onStageStatusActive(CustomListener.scala:409)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener$$anon$1.run(CustomListener.scala:71)\n",
      "\tat java.base/java.util.TimerThread.mainLoop(Timer.java:556)\n",
      "\tat java.base/java.util.TimerThread.run(Timer.java:506)\n",
      "22/09/24 01:09:56 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onStageStatusActive(CustomListener.scala:409)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener$$anon$1.run(CustomListener.scala:71)\n",
      "\tat java.base/java.util.TimerThread.mainLoop(Timer.java:556)\n",
      "\tat java.base/java.util.TimerThread.run(Timer.java:506)\n",
      "2022/09/24 01:09:56 INFO mlflow.tracking.fluent: Autologging successfully enabled for sklearn.\n",
      "2022/09/24 01:09:56 INFO mlflow.tracking.fluent: Autologging successfully enabled for pyspark.\n",
      "2022/09/24 01:09:56 INFO mlflow.pyspark.ml: No SparkSession detected. Autologging will log pyspark.ml models contained in the default allowlist. To specify a custom allowlist, initialize a SparkSession prior to calling mlflow.pyspark.ml.autolog() and specify the path to your allowlist file via the spark.mlflow.pysparkml.autolog.logModelAllowlistFile conf.\n",
      "2022/09/24 01:09:56 INFO mlflow.tracking.fluent: Autologging successfully enabled for pyspark.ml.\n",
      "22/09/24 01:09:56 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onJobStart(CustomListener.scala:236)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:37)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:117)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:101)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:105)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:105)\n",
      "\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:100)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:96)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1381)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:96)\n",
      "22/09/24 01:09:56 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onStageSubmitted(CustomListener.scala:379)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:33)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:117)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:101)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:105)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:105)\n",
      "\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:100)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:96)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1381)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:96)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/hyperopt/spark.py\", line 467, in run_task_on_executor\n",
      "    result = domain.evaluate(\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/hyperopt/base.py\", line 892, in evaluate\n",
      "    rval = self.fn(pyll_rval)\n",
      "  File \"/tmp/ipykernel_229044/4099234749.py\", line 11, in train_model\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/fluent.py\", line 353, in start_run\n",
      "    active_run_obj = client.create_run(experiment_id=exp_id_for_run, tags=resolved_tags)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/client.py\", line 265, in create_run\n",
      "    return self._tracking_client.create_run(experiment_id, start_time, tags)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/_tracking_service/client.py\", line 107, in create_run\n",
      "    return self.store.create_run(\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 583, in create_run\n",
      "    experiment = self.get_experiment(experiment_id)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 430, in get_experiment\n",
      "    experiment = self._get_experiment(experiment_id)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 403, in _get_experiment\n",
      "    meta = FileStore._read_yaml(experiment_dir, FileStore.META_DATA_FILE_NAME)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 1075, in _read_yaml\n",
      "    return _read_helper(root, file_name, attempts_remaining=retries)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 1068, in _read_helper\n",
      "    result = read_yaml(root, file_name)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/utils/file_utils.py\", line 181, in read_yaml\n",
      "    raise MissingConfigException(\"Yaml file '%s' does not exist.\" % file_path)\n",
      "mlflow.exceptions.MissingConfigException: Yaml file '/home/debankurs/Desktop/learningspark/spark-standalone-cluster-on-docker/build/workspace/mlruns/0/meta.yaml' does not exist.\n",
      "\n",
      "22/09/24 01:09:56 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onStageCompleted(CustomListener.scala:343)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:35)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:117)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:101)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:105)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:105)\n",
      "\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:100)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:96)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1381)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:96)\n",
      "22/09/24 01:09:56 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onJobEnd(CustomListener.scala:288)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:39)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:117)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:101)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:105)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:105)\n",
      "\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:100)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:96)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1381)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:96)\n",
      "trial task 28 failed, exception is Yaml file '/home/debankurs/Desktop/learningspark/spark-standalone-cluster-on-docker/build/workspace/mlruns/0/meta.yaml' does not exist..\n",
      " Traceback (most recent call last):\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/hyperopt/spark.py\", line 467, in run_task_on_executor\n",
      "    result = domain.evaluate(\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/hyperopt/base.py\", line 892, in evaluate\n",
      "    rval = self.fn(pyll_rval)\n",
      "  File \"/tmp/ipykernel_229044/4099234749.py\", line 11, in train_model\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/fluent.py\", line 353, in start_run\n",
      "    active_run_obj = client.create_run(experiment_id=exp_id_for_run, tags=resolved_tags)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/client.py\", line 265, in create_run\n",
      "    return self._tracking_client.create_run(experiment_id, start_time, tags)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/_tracking_service/client.py\", line 107, in create_run\n",
      "    return self.store.create_run(\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 583, in create_run\n",
      "    experiment = self.get_experiment(experiment_id)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 430, in get_experiment\n",
      "    experiment = self._get_experiment(experiment_id)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 403, in _get_experiment\n",
      "    meta = FileStore._read_yaml(experiment_dir, FileStore.META_DATA_FILE_NAME)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 1075, in _read_yaml\n",
      "    return _read_helper(root, file_name, attempts_remaining=retries)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 1068, in _read_helper\n",
      "    result = read_yaml(root, file_name)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/utils/file_utils.py\", line 181, in read_yaml\n",
      "    raise MissingConfigException(\"Yaml file '%s' does not exist.\" % file_path)\n",
      "mlflow.exceptions.MissingConfigException: Yaml file '/home/debankurs/Desktop/learningspark/spark-standalone-cluster-on-docker/build/workspace/mlruns/0/meta.yaml' does not exist.\n",
      "\n",
      "22/09/24 01:09:57 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.$anonfun$onStageStatusActive$2(CustomListener.scala:402)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.$anonfun$onStageStatusActive$2$adapted(CustomListener.scala:385)\n",
      "\tat scala.collection.TraversableLike$WithFilter.$anonfun$foreach$1(TraversableLike.scala:877)\n",
      "\tat scala.collection.mutable.HashMap.$anonfun$foreach$1(HashMap.scala:149)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry(HashTable.scala:237)\n",
      "\tat scala.collection.mutable.HashTable.foreachEntry$(HashTable.scala:230)\n",
      "\tat scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:44)\n",
      "\tat scala.collection.mutable.HashMap.foreach(HashMap.scala:149)\n",
      "\tat scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:876)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onStageStatusActive(CustomListener.scala:385)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener$$anon$1.run(CustomListener.scala:71)\n",
      "\tat java.base/java.util.TimerThread.mainLoop(Timer.java:556)\n",
      "\tat java.base/java.util.TimerThread.run(Timer.java:506)\n",
      "22/09/24 01:09:57 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onStageStatusActive(CustomListener.scala:409)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener$$anon$1.run(CustomListener.scala:71)\n",
      "\tat java.base/java.util.TimerThread.mainLoop(Timer.java:556)\n",
      "\tat java.base/java.util.TimerThread.run(Timer.java:506)\n",
      "22/09/24 01:09:57 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onStageStatusActive(CustomListener.scala:409)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener$$anon$1.run(CustomListener.scala:71)\n",
      "\tat java.base/java.util.TimerThread.mainLoop(Timer.java:556)\n",
      "\tat java.base/java.util.TimerThread.run(Timer.java:506)\n",
      "2022/09/24 01:09:57 INFO mlflow.tracking.fluent: Autologging successfully enabled for sklearn.\n",
      "2022/09/24 01:09:57 INFO mlflow.tracking.fluent: Autologging successfully enabled for pyspark.\n",
      "2022/09/24 01:09:57 INFO mlflow.pyspark.ml: No SparkSession detected. Autologging will log pyspark.ml models contained in the default allowlist. To specify a custom allowlist, initialize a SparkSession prior to calling mlflow.pyspark.ml.autolog() and specify the path to your allowlist file via the spark.mlflow.pysparkml.autolog.logModelAllowlistFile conf.\n",
      "2022/09/24 01:09:57 INFO mlflow.tracking.fluent: Autologging successfully enabled for pyspark.ml.\n",
      "22/09/24 01:09:57 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onJobStart(CustomListener.scala:236)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:37)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:117)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:101)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:105)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:105)\n",
      "\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:100)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:96)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1381)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:96)\n",
      "22/09/24 01:09:57 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onStageSubmitted(CustomListener.scala:379)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:33)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:117)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:101)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:105)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:105)\n",
      "\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:100)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:96)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1381)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:96)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/hyperopt/spark.py\", line 467, in run_task_on_executor\n",
      "    result = domain.evaluate(\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/hyperopt/base.py\", line 892, in evaluate\n",
      "    rval = self.fn(pyll_rval)\n",
      "  File \"/tmp/ipykernel_229044/4099234749.py\", line 11, in train_model\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/fluent.py\", line 353, in start_run\n",
      "    active_run_obj = client.create_run(experiment_id=exp_id_for_run, tags=resolved_tags)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/client.py\", line 265, in create_run\n",
      "    return self._tracking_client.create_run(experiment_id, start_time, tags)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/_tracking_service/client.py\", line 107, in create_run\n",
      "    return self.store.create_run(\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 583, in create_run\n",
      "    experiment = self.get_experiment(experiment_id)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 430, in get_experiment\n",
      "    experiment = self._get_experiment(experiment_id)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 403, in _get_experiment\n",
      "    meta = FileStore._read_yaml(experiment_dir, FileStore.META_DATA_FILE_NAME)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 1075, in _read_yaml\n",
      "    return _read_helper(root, file_name, attempts_remaining=retries)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 1068, in _read_helper\n",
      "    result = read_yaml(root, file_name)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/utils/file_utils.py\", line 181, in read_yaml\n",
      "    raise MissingConfigException(\"Yaml file '%s' does not exist.\" % file_path)\n",
      "mlflow.exceptions.MissingConfigException: Yaml file '/home/debankurs/Desktop/learningspark/spark-standalone-cluster-on-docker/build/workspace/mlruns/0/meta.yaml' does not exist.\n",
      "\n",
      "22/09/24 01:09:57 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onStageCompleted(CustomListener.scala:343)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:35)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:117)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:101)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:105)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:105)\n",
      "\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:100)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:96)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1381)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:96)\n",
      "22/09/24 01:09:57 ERROR JupyterSparkMonitorListener: Exception sending socket message: \n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.base/java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:110)\n",
      "\tat java.base/java.net.SocketOutputStream.write(SocketOutputStream.java:150)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.writeBytes(StreamEncoder.java:233)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:303)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.implWrite(StreamEncoder.java:281)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:125)\n",
      "\tat java.base/sun.nio.cs.StreamEncoder.write(StreamEncoder.java:135)\n",
      "\tat java.base/java.io.OutputStreamWriter.write(OutputStreamWriter.java:226)\n",
      "\tat java.base/java.io.Writer.write(Writer.java:249)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.send(CustomListener.scala:53)\n",
      "\tat sparkmonitor.listener.JupyterSparkMonitorListener.onJobEnd(CustomListener.scala:288)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:39)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:117)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:101)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:105)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:105)\n",
      "\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:100)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:96)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1381)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:96)\n",
      "trial task 29 failed, exception is Yaml file '/home/debankurs/Desktop/learningspark/spark-standalone-cluster-on-docker/build/workspace/mlruns/0/meta.yaml' does not exist..\n",
      " Traceback (most recent call last):\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/hyperopt/spark.py\", line 467, in run_task_on_executor\n",
      "    result = domain.evaluate(\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/hyperopt/base.py\", line 892, in evaluate\n",
      "    rval = self.fn(pyll_rval)\n",
      "  File \"/tmp/ipykernel_229044/4099234749.py\", line 11, in train_model\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/fluent.py\", line 353, in start_run\n",
      "    active_run_obj = client.create_run(experiment_id=exp_id_for_run, tags=resolved_tags)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/client.py\", line 265, in create_run\n",
      "    return self._tracking_client.create_run(experiment_id, start_time, tags)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/tracking/_tracking_service/client.py\", line 107, in create_run\n",
      "    return self.store.create_run(\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 583, in create_run\n",
      "    experiment = self.get_experiment(experiment_id)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 430, in get_experiment\n",
      "    experiment = self._get_experiment(experiment_id)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 403, in _get_experiment\n",
      "    meta = FileStore._read_yaml(experiment_dir, FileStore.META_DATA_FILE_NAME)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 1075, in _read_yaml\n",
      "    return _read_helper(root, file_name, attempts_remaining=retries)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/store/tracking/file_store.py\", line 1068, in _read_helper\n",
      "    result = read_yaml(root, file_name)\n",
      "  File \"/home/debankurs/miniconda3/envs/pysparkenv/lib/python3.8/site-packages/mlflow/utils/file_utils.py\", line 181, in read_yaml\n",
      "    raise MissingConfigException(\"Yaml file '%s' does not exist.\" % file_path)\n",
      "mlflow.exceptions.MissingConfigException: Yaml file '/home/debankurs/Desktop/learningspark/spark-standalone-cluster-on-docker/build/workspace/mlruns/0/meta.yaml' does not exist.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the search space to explore\n",
    "search_space = {\n",
    "  'n_estimators': scope.int(hp.quniform('n_estimators', 20, 1000, 1)),\n",
    "  'learning_rate': hp.loguniform('learning_rate', -3, 0),\n",
    "  'max_depth': scope.int(hp.quniform('max_depth', 2, 5, 1)),\n",
    "}\n",
    "\n",
    "def train_model(params):\n",
    "  # Enable autologging on each worker\n",
    "  mlflow.autolog()\n",
    "  with mlflow.start_run(nested=True):\n",
    "    model_hp = sklearn.ensemble.GradientBoostingClassifier(\n",
    "      random_state=0,\n",
    "      **params\n",
    "    )\n",
    "    model_hp.fit(X_train, y_train)\n",
    "    predicted_probs = model_hp.predict_proba(X_test)\n",
    "    # Tune based on the test AUC\n",
    "    # In production settings, you could use a separate validation set instead\n",
    "    roc_auc = sklearn.metrics.roc_auc_score(y_test, predicted_probs[:,1])\n",
    "    mlflow.log_metric('test_auc', roc_auc)\n",
    "    \n",
    "    # Set the loss to -1*auc_score so fmin maximizes the auc_score\n",
    "    return {'status': STATUS_OK, 'loss': -1*roc_auc}\n",
    "\n",
    "# SparkTrials distributes the tuning using Spark workers\n",
    "# Greater parallelism speeds processing, but each hyperparameter trial has less information from other trials\n",
    "# On smaller clusters or Databricks Community Edition try setting parallelism=2\n",
    "spark_trials = SparkTrials(\n",
    "  parallelism=8\n",
    ")\n",
    "\n",
    "with mlflow.start_run(run_name='gb_hyperopt') as run:\n",
    "  # Use hyperopt to find the parameters yielding the highest AUC\n",
    "  best_params = fmin(\n",
    "    fn=train_model, \n",
    "    space=search_space, \n",
    "    algo=tpe.suggest, \n",
    "    max_evals=32,\n",
    "    trials=spark_trials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "bae7ad39-0eea-4dcd-a58a-dcfc723ba91f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Search runs to retrieve the best model\n",
    "Because all of the runs are tracked by MLflow, you can retrieve the metrics and parameters for the best run using the MLflow search runs API to find the tuning run with the highest test auc.\n",
    "\n",
    "This tuned model should perform better than the simpler models trained in Part 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "5ea75b75-0439-4fa2-9f8c-6bbd2b586719",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Sort runs by their test auc; in case of ties, use the most recent run\n",
    "best_run = mlflow.search_runs(\n",
    "  order_by=['metrics.test_auc DESC', 'start_time DESC'],\n",
    "  max_results=10,\n",
    ").iloc[0]\n",
    "print('Best Run')\n",
    "print('AUC: {}'.format(best_run[\"metrics.test_auc\"]))\n",
    "print('Num Estimators: {}'.format(best_run[\"params.n_estimators\"]))\n",
    "print('Max Depth: {}'.format(best_run[\"params.max_depth\"]))\n",
    "print('Learning Rate: {}'.format(best_run[\"params.learning_rate\"]))\n",
    "\n",
    "best_model_pyfunc = mlflow.pyfunc.load_model(\n",
    "  'runs:/{run_id}/model'.format(\n",
    "    run_id=best_run.run_id\n",
    "  )\n",
    ")\n",
    "best_model_predictions = best_model_pyfunc.predict(X_test[:5])\n",
    "print(\"Test Predictions: {}\".format(best_model_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "32be40f4-ba23-4512-817e-ab62e9a85825",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Compare multiple runs in the UI\n",
    "As in Part 1, you can view and compare the runs in the MLflow experiment details page, accessible via the external link icon at the top of the **Experiment** sidebar. \n",
    "\n",
    "On the experiment details page, click the \"+\" icon to expand the parent run, then select all runs except the parent, and click **Compare**. You can visualize the different runs using a parallel coordinates plot, which shows the impact of different parameter values on a metric. \n",
    "\n",
    "<img width=\"800\" src=\"https://docs.databricks.com/_static/images/mlflow/quickstart/parallel-plot.png\"/>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "ML Quickstart: Model Training",
   "notebookOrigID": 1254233491087509,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
